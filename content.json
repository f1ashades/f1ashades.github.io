{"meta":{"title":"f1ashades' blogs","subtitle":"","description":"once again I am a child","author":"f1ashades","url":"http://example.com","root":"/"},"pages":[{"title":"categories","date":"2022-06-05T08:58:34.000Z","updated":"2022-06-05T09:42:38.092Z","comments":false,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2022-06-05T09:03:17.000Z","updated":"2022-06-05T09:04:01.712Z","comments":false,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Spring Cloud的基本使用","slug":"spring cloud","date":"2022-05-12T07:29:48.000Z","updated":"2023-03-06T12:44:21.228Z","comments":true,"path":"2022/05/12/spring cloud/","link":"","permalink":"http://example.com/2022/05/12/spring%20cloud/","excerpt":"Eureka注册中心、Ribbon负载均衡、Nacos注册中心、Nacos配置管理、Feign远程调用、Gateway服务网关……","text":"Eureka注册中心、Ribbon负载均衡、Nacos注册中心、Nacos配置管理、Feign远程调用、Gateway服务网关…… 1. 基本概念1）单体架构单体架构：将业务的所有功能集中在一个项目中开发，打成一个包部署。 单体架构的优缺点如下： 优点： 架构简单 部署成本低 缺点： 耦合度高（维护困难、升级困难） 2）分布式架构分布式架构：根据业务功能对系统做拆分，每个业务功能模块作为独立项目开发，称为一个服务。 分布式架构的优缺点： 优点： 降低服务耦合 有利于服务升级和拓展 缺点： 服务调用关系错综复杂 分布式架构虽然降低了服务耦合，但是服务拆分时也有很多问题需要思考： 服务拆分的粒度如何界定？ 服务之间如何调用？ 服务的调用关系如何管理？ 人们需要制定一套行之有效的标准来约束分布式架构。 3）微服务 服务网关： 对流量做一个校验，过滤流量 对放进来的流量按照策略进行路由，负载均衡（用户要访问的是哪一个服务，就路由到哪一个服务 ） 注册中心 服务注册和服务发现，各个服务之间有依赖关系，需要由统一进行管理 如银行有多种业务：存取、取钱等，用户只需去前台说我需要怎样的服务，前台（注册中心）就会查询并提供相应服务 监控各个服务的状态 配置中心 各服务都有自己的配置，当需要更改配置的时候，若一个一个去修改对应配置太麻烦了，所以需要一个配置中心记录下所 有服务的配置。在服务A配置发生变化的时候，只需在配置中心进行修改，配置中心会通过通知的方式来修改对应服务A的 配置 数据库 对于微服务而言，一个服务对应一个数据库，保证了各个服务间 微服务的架构特征： 单一职责：微服务拆分粒度更小，每一个服务都对应唯一的业务能力，做到单一职责 自治：团队独立、技术独立、数据独立，独立部署和交付 面向服务：服务提供统一标准的接口，与语言和技术无关 隔离性强：服务调用做好隔离、容错、降级，避免出现级联问题 微服务的上述特性其实是在给分布式架构制定一个标准，进一步降低服务之间的耦合度，提供服务的独立性和灵活性 做到高内聚，低耦合 可以认为微服务是一种经过良好架构设计的分布式架构方案 对于java，springCloud就是一种非常优秀的微服务方案 4）SpringCloudSpringCloud是目前国内使用最广泛的微服务框架。官网地址：https://spring.io/projects/spring-cloud。 SpringCloud集成了各种微服务功能组件，并基于SpringBoot实现了这些组件的自动装配，从而提供了良好的开箱即用体验。 其中常见的组件包括： 另外，SpringCloud底层是依赖于SpringBoot的，并且有版本的兼容关系，如下： 5）微服务技术对比 Dubbo，主要用于服务远程调用 zookeeper主要用来做集群，redis主要用来做缓存，所以用他们做注册中心并不太合适 springCloud，其团队主要针对微服务的各个组件做了一些整合 Eureka使用更多，支持Feign协议 Feign是基于http协议的远程调用，所以适用restful风格 springCloudConfig作为配置中心 springCloudGateway和Zuul为服务网关，前者使用更多 springCloudAlibaba，实现了springCLoud标准，主要是将Dubbo整合进springCloud 注册中心中的Nacos组件支持Dubbo和Feign协议，所以更为灵活 远程调用Dubbo和Feign均可使用 Sentinel哨兵作为服务监控和保护 2. 服务拆分和远程调用1）拆分原则 不同微服务，不要重复开发相同业务 微服务数据独立，不要访问其它微服务的数据库 微服务可以将自己的业务暴露为接口，供其它微服务调用 具体服务拆分示例： cloud-demo：父工程，管理依赖 order-service：订单微服务，负责订单相关业务 user-service：用户微服务，负责用户相关业务 要求： 订单微服务和用户微服务都必须有各自的数据库，相互独立 订单服务和用户服务都对外暴露Restful的接口 订单服务如果需要查询用户信息，只能调用用户服务的Restful接口，不能查询用户数据库 2）远程调用在单体项目中，由于所有service都打在一个包中，所以可以直接调用。但对于微服务架构来说， 如订单service要调用配送service，但两个service部署在两个不同的服务器上，这就涉及到了远程调用。 在服务调用关系中，会有两个不同的角色： 服务提供者：一次业务中，被其它微服务调用的服务。（提供接口给其它微服务） 服务消费者：一次业务中，调用其它微服务的服务。（调用其它微服务提供的接口） 但是，服务提供者与服务消费者的角色并不是绝对的，而是相对于业务而言。 如果服务A调用了服务B，而服务B又调用了服务C，服务B的角色是什么？ 对于A调用B的业务而言：A是服务消费者，B是服务提供者 对于B调用C的业务而言：B是服务消费者，C是服务提供者 因此，服务B既可以是服务提供者，也可以是服务消费者，这是一个相对的概念。 3. Eureka注册中心1）Eureka的结构和作用假如我们的服务提供者user-service部署了多个实例，如图： 有如下几个问题 问题1：order-service在发起远程调用的时候，该如何得知user-service实例的ip地址和端口？ 问题2：有多个user-service实例地址，order-service调用时该如何选择？ 问题3：order-service如何得知某个user-service实例是否依然健康，是不是已经宕机？ 这些问题都需要利用SpringCloud中的注册中心来解决，其中最广为人知的注册中心就是Eureka 问题1：order-service如何得知user-service实例地址？ 获取地址信息的流程如下： user-service服务实例启动后，将自己的信息注册到eureka-server（Eureka服务端）。这个叫服务注册 eureka-server保存服务名称到服务实例地址列表的映射关系 order-service根据服务名称，拉取实例地址列表。这个叫服务发现或服务拉取 问题2：order-service如何从多个user-service实例中选择具体的实例？ order-service从实例列表中利用负载均衡算法（ribbon）选中一个实例地址 向该实例地址发起远程调用 问题3：order-service如何得知某个user-service实例是否依然健康，是不是已经宕机？ user-service会每隔一段时间（默认30秒）向eureka-server发起请求，报告自己状态，称为心跳 当超过一定时间没有发送心跳时，eureka-server会认为微服务实例故障，将该实例从服务列表中剔除 order-service拉取服务时，就能将故障实例排除了 注意：一个微服务，既可以是服务提供者，又可以是服务消费者，因此eureka将服务注册、服务发现等功能统一封装到了eureka-client端 2）搭建eureka-server首先大家注册中心服务端：eureka-server，这必须是一个独立的微服务 ①创建eureka-server服务在cloud-demo父工程下，创建一个子模块： 填写模块信息： 然后填写服务信息： ②引入eureka依赖引入SpringCloud为eureka提供的starter依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt;&lt;/dependency&gt; ③编写启动类给eureka-server服务编写一个启动类，一定要添加一个@EnableEurekaServer注解，开启eureka的注册中心功能： 12345678910111213package cn.itcast.eureka;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;@SpringBootApplication@EnableEurekaServerpublic class EurekaApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaApplication.class, args); &#125;&#125; ④编写配置文件编写一个application.yml文件，内容如下： 123456789server: port: 10086spring: application: name: eureka-servereureka: client: service-url: defaultZone: http://127.0.0.1:10086/eureka eureka本身也是一个服务，所以需要注册到eureka的注册中心中 ⑤启动服务启动微服务，然后在浏览器访问：http://127.0.0.1:10086 可以看到，eureka自己已经注册了。 3）服务注册下面将user-service注册到eureka-server中去。 ①引入依赖在user-service的pom文件中，引入下面的eureka-client依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt; ②配置文件在user-service中，修改application.yml文件，添加服务名称、eureka地址： 1234567spring: application: name: userservice #服务名eureka: client: service-url: defaultZone: http://127.0.0.1:10086/eureka #eureka地址 ③启动多个user-service实例为了演示一个服务有多个实例的场景，我们添加一个SpringBoot的启动配置，再启动一个user-service。 首先，复制原来的user-service启动配置： 然后，在弹出的窗口中，填写信息： 这样就是用VM命令来设置了参数，该参数的优先级高于yml文件 现在，SpringBoot窗口会出现两个user-service启动配置： 不过，第一个是8081端口，第二个是8082端口。 启动两个user-service实例： 查看eureka-server管理页面： 可以看到user-service服务已经有了两个具体事例被注册了 4）服务发现下面，我们将order-service的逻辑修改：向eureka-server拉取user-service的信息，实现服务发现。 服务发现，实际上就是消费者拉取需要的service的信息。 ①引入依赖之前说过，服务发现、服务注册统一都封装在eureka-client依赖，因此这一步与服务注册时一致。 在order-service的pom文件中，引入下面的eureka-client依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt; ②配置文件服务发现也需要知道eureka地址，因此第二步与服务注册一致，都是配置eureka信息： 在order-service中，修改application.yml文件，添加服务名称、eureka地址： 1234567spring: application: name: orderserviceeureka: client: service-url: defaultZone: http://127.0.0.1:10086/eureka 消费者本身也需要作为一个服务注册到eureka中 ③服务拉取和负载均衡最后，我们要去eureka-server中拉取user-service服务的实例列表，并且实现负载均衡。 这里只需要添加一些注解即可（此处是基于restTemplate来进行远程调用的） 在order-service的OrderApplication中，给RestTemplate这个Bean添加一个@LoadBalanced注解： 修改order-service服务中的cn.itcast.order.service包下的OrderService类中的queryOrderById方法。修改访问的url路径，用服务名代替ip、端口： spring会自动帮助我们从eureka-server端，根据userservice这个服务名称，获取实例列表，而后完成负载均衡。 4. Ribbon负载均衡1）负载均衡原理之前在eureka中添加了@LoadBalanced注解，即可实现负载均衡功能，这是什么原理呢？ SpringCloud底层其实是利用了一个名为Ribbon的组件，来实现负载均衡功能的。 那么我们发出的请求明明是http://userservice/user/1，怎么变成了http://localhost:8081的呢？ 2）源码跟踪为什么我们只输入了service名称就可以访问了呢？ip和端口是怎么获取到的呢？ 显然有人帮我们根据service名称，获取到了服务实例的ip和端口。它就是LoadBalancerInterceptor，这个类会在对RestTemplate的请求进行拦截，然后从Eureka根据服务id获取服务列表，随后利用负载均衡算法得到真实的服务地址信息，替换服务id。 我们进行源码跟踪： ①LoadBalancerIntercepor 可以看到这里的intercept方法，拦截了用户的HttpRequest请求，然后做了几件事： request.getURI()：获取请求uri，本例中就是 http://user-service/user/8 originalUri.getHost()：获取uri路径的主机名，其实就是服务id，user-service this.loadBalancer.execute()：处理服务id，和用户请求。 这里的this.loadBalancer是LoadBalancerClient类型，我们继续跟入。 ②LoadBalancerClient继续跟入execute方法： 代码是这样的： getLoadBalancer(serviceId)：根据服务id获取ILoadBalancer，而ILoadBalancer会拿着服务id去eureka中获取服务列表并保存起来，这里获取的是一个dynamicServerLoadBalancer，它其中就有serveList（服务的列表） getServer(loadBalancer)：利用内置的负载均衡算法，从服务列表中选择一个。本例中，可以看到获取了8082端口的服务 放行后，再次访问并跟踪，发现获取的是8081： 果然实现了负载均衡。 ③负载均衡策略IRule在刚才的代码中，可以看到获取服务使通过一个getServer方法来做负载均衡: 我们继续跟入： 继续跟踪源码chooseServer方法，发现这么一段代码： 我们看看这个rule是谁： 这里的rule默认值是一个RoundRobinRule，看类的介绍： 这不就是轮询的意思嘛。 到这里，整个负载均衡的流程我们就清楚了。 ④总结SpringCloudRibbon的底层采用了一个拦截器，拦截了RestTemplate发出的请求，对地址做了修改。用一幅图来总结一下： 基本流程如下： 拦截我们的RestTemplate请求http://userservice/user/1 RibbonLoadBalancerClient会从请求url中获取服务名称(id)，也就是user-service 然后RibbonLoadBalancerClient中就或取得了DynamicServerListLoadBalancer DynamicServerListLoadBalancer根据user-service到eureka拉取服务列表 eureka返回列表，localhost:8081、localhost:8082 IRule利用内置负载均衡规则，从列表中选择一个，例如localhost:8081 RibbonLoadBalancerClient修改请求地址，用localhost:8081替代userservice，得到http://localhost:8081/user/1，发起真实请求 注意： RibbonLoadBalancerClient主要工作 获取原始url中的userservice 等到DynamicServerListLoadBalancer负载均衡选择了一个url后，修改请求地址，向具体的地址发送请求 DynamicServerListLoadBalancer主要工作 从eureka中拉取服务列表：localhost:8081、localhost:8082 getServer方法来根据IRule对服务列表实现负载均衡 IRule是一个负载均衡策略接口，它有许多具体的负载均衡实现 rule的默认值是RoundRobinRule轮询 3）负载均衡策略①负载均衡策略负载均衡的规则都定义在IRule接口中，而IRule有很多不同的实现类： 不同规则的含义如下： 内置负载均衡规则类 规则描述 RoundRobinRule 简单轮询服务列表来选择服务器。它是Ribbon默认的负载均衡规则。 AvailabilityFilteringRule 对以下两种服务器进行忽略： （1）在默认情况下，这台服务器如果3次连接失败，这台服务器就会被设置为“短路”状态。短路状态将持续30秒，如果再次连接失败，短路的持续时间就会几何级地增加。 （2）并发数过高的服务器。如果一个服务器的并发连接数过高，配置了AvailabilityFilteringRule规则的客户端也会将其忽略。并发连接数的上限，可以由客户端的&lt; clientName&gt;.&lt; clientConfigNameSpace&gt;.ActiveConnectionsLimit属性进行配置 WeightedResponseTimeRule 为每一个服务器赋予一个权重值。服务器响应时间越长，这个服务器的权重就越小。这个规则会随机选择服务器，这个权重值会影响服务器的选择。 ZoneAvoidanceRule 以区域可用的服务器为基础进行服务器的选择。使用Zone对服务器进行分类，这个Zone可以理解为一个机房、一个机架等。而后再对Zone内的多个服务做轮询。 BestAvailableRule 忽略那些短路的服务器，并选择并发数较低的服务器。 RandomRule 随机选择一个可用的服务器。 RetryRule 重试机制的选择逻辑 默认的实现就是ZoneAvoidanceRule，是一种轮询方案 ②自定义负载均衡策略通过定义IRule实现可以修改负载均衡规则，有两种方式： 代码方式：在order-service中的OrderApplication类中，定义一个新的IRule： 1234@Beanpublic IRule randomRule()&#123; return new RandomRule();&#125; 用自己的bean去代替默认的IRULE的bean 配置文件方式：在order-service的application.yml文件中，添加新的配置也可以修改规则： 123userservice: # 给某个微服务配置负载均衡规则，这里是userservice服务 ribbon: NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule # 负载均衡规则 注意，一般用默认的负载均衡规则，不做修改。 4）饥饿加载 Ribbon默认是采用懒加载，即第一次访问时才会去创建LoadBalancerClient（ribbon的），请求时间会很长 因为初始的时候回去拉取服务list（创建dynamicServeLoadBalancer） 加载以后服务列表就缓存在内存中，后序请求就会很快了 而饥饿加载则会在项目启动时创建，降低第一次访问的耗时，通过下面配置开启饥饿加载： 1234ribbon: eager-load: enabled: true clients: userservice 当然第一次加载的时候还需要做其它事情，比如DispatcherServlet等，饥饿加载只是其中一个因数 5. Nacos注册中心国内公司一般都推崇阿里巴巴的技术，比如注册中心，SpringCloudAlibaba也推出了一个名为Nacos的注册中心。 1）认识和安装NacosNacos是阿里巴巴的产品，现在是SpringCloud中的一个组件。 相比Eureka功能更加丰富（注册中心、配置中心），国内受欢迎程度较高。 nacos安装：https://gitee.com/f1ashades/store/tree/master/nacos%E5%AE%89%E8%A3%85 2）服务注册到nacosNacos是SpringCloudAlibaba的组件，而SpringCloudAlibaba也遵循SpringCloud中定义的服务注册、服务发现规范。因此使用Nacos和使用Eureka对于微服务来说，并没有太大区别。 主要差异在于： 依赖不同 服务地址不同 ①引入依赖在cloud-demo父工程的pom文件中的&lt;dependencyManagement&gt;中引入SpringCloudAlibaba的依赖： 1234567&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt; &lt;version&gt;2.2.6.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt;&lt;/dependency&gt; 然后在user-service和order-service中的pom文件中引入nacos-discovery依赖： 1234&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;&lt;/dependency&gt; 注意：不要忘了注释掉eureka的依赖。 ②配置nacos地址在user-service和order-service的application.yml中添加nacos地址： 1234spring: cloud: nacos: server-addr: localhost:8848 注意：不要忘了注释掉eureka的地址 ③重启重启微服务后，登录nacos管理页面，可以看到微服务信息： 3）服务分级存储模型①基本概念一个服务可以有多个实例，例如我们的user-service，可以有: 127.0.0.1:8081 127.0.0.1:8082 127.0.0.1:8083 假如这些实例分布于全国各地的不同机房，例如： 127.0.0.1:8081，在上海机房 127.0.0.1:8082，在上海机房 127.0.0.1:8083，在杭州机房 Nacos就将同一机房内的实例 划分为一个集群。 也就是说，user-service是服务，一个服务可以包含多个集群，如杭州、上海，每个集群下可以有多个实例，形成分级模型，如图： 微服务互相访问时，应该尽可能访问同集群实例，因为本地访问速度更快。当本集群内不可用时，才访问其它集群。例如： 即：杭州机房内的order-service应该优先访问同机房的user-service。 ①给user-service配置集群修改user-service的application.yml文件，添加集群配置： 123456spring: cloud: nacos: server-addr: localhost:8848 discovery: cluster-name: HZ # 集群名称 重启两个user-service实例后，我们可以在nacos控制台看到下面结果： 我们再次复制一个user-service启动配置，添加属性： 1-Dserver.port=8083 -Dspring.cloud.nacos.discovery.cluster-name=SH 配置如图所示： 以上是在修改端口号和机房信息 启动UserApplication3后再次查看nacos控制台： ②同集群优先的负载均衡默认的ZoneAvoidanceRule并不能实现根据同集群优先来实现负载均衡。 因此Nacos中提供了一个NacosRule的实现，可以优先从同集群中挑选实例。 1）给order-service配置集群信息 修改order-service的application.yml文件，添加集群配置： 123456spring: cloud: nacos: server-addr: localhost:8848 discovery: cluster-name: HZ # 集群名称 2）修改负载均衡规则 修改order-service的application.yml文件，修改负载均衡规则： 123userservice: ribbon: NFLoadBalancerRuleClassName: com.alibaba.cloud.nacos.ribbon.NacosRule # 负载均衡规则 4）权重配置实际部署中会出现这样的场景： 服务器设备性能有差异，部分实例所在机器性能较好，另一些较差，我们希望性能好的机器承担更多的用户请求。 但默认情况下NacosRule是同集群内随机挑选，不会考虑机器的性能问题。 因此，Nacos提供了权重配置来控制访问频率，权重越大则访问频率越高。 修改： 在nacos控制台，找到user-service的实例列表，点击编辑，即可修改权重： 在弹出的编辑窗口，修改权重： 注意：如果权重修改为0，则该实例永远不会被访问 一个应用： 做升级的时候，我们可以将集群中某一个实例A的权重设置为0，那么实例A就不再接收用户请求了，然后我们对实例A进行升级，然后慢慢调整权重0.1,0.2,0.3…..，当运行中该实例A没有发生问题，那么升级成功，可以调大权重。 5）环境隔离①基本概念Nacos提供了namespace来实现环境隔离功能。 nacos中可以有多个namespace namespace下可以有group、service等 不同namespace之间相互隔离，例如不同namespace的服务互相不可见 虽然之前已经实现了低于隔离、服务隔离，但是还会有一些其它情况 生产环境、测试环境、上线环境等不同的环境之间实现隔离 ②创建namespace默认情况下，所有service、data、group都在同一个namespace，名为public： 我们可以点击页面新增按钮，添加一个namespace： 然后，填写表单： 就能在页面看到一个新的namespace： ③给微服务配置namespace给微服务配置namespace只能通过修改配置来实现。 例如，修改order-service的application.yml文件： 1234567spring: cloud: nacos: server-addr: localhost:8848 discovery: cluster-name: HZ namespace: 492a7d5d-237b-46a1-a99a-fa8e98e4b0f9 # 命名空间，填ID 重启order-service后，访问控制台，可以看到下面的结果： 此时访问order-service，因为namespace不同，会导致找不到userservice，控制台会报错： 6）实例类型Nacos的服务实例分为两种类型： 临时实例：如果实例宕机超过一定时间，会从服务列表剔除，默认的类型。 非临时实例：如果实例宕机，不会从服务列表剔除，也可以叫永久实例。 配置一个服务实例为永久实例： 12345spring: cloud: nacos: discovery: ephemeral: false # 设置为非临时实例 6）Nacos与Eureka的区别Nacos和Eureka整体结构类似，服务注册、服务拉取、心跳等待，但是也存在一些差异： Nacos与eureka的共同点 都支持服务注册和服务拉取 都支持服务提供者心跳方式做健康检测 Nacos与Eureka的区别 Nacos支持服务端主动检测提供者状态（基于临时实例和非临时实例）： 临时实例采用心跳模式：临时实例心跳不正常会被剔除 非临时实例采用主动检测模式：非临时实例则不会被剔除，只是会标记为不健康，nacos会等待其恢复健康 Nacos支持服务列表变更的消息推送模式，服务列表更新更及时 Eureka会定时pull，而nacos会pull+push 当nacos发现有服务提供者挂掉，会立即push给服务消费者，服务列表更新更及时 集群方式 Nacos集群默认采用AP方式，当集群中存在非临时实例时，采用CP模式 Eureka采用AP方式，不能修改成CP模式 6. Nacos配置管理Nacos除了可以做注册中心，同样可以做配置管理来使用。 1）统一配置管理当微服务部署的实例越来越多，达到数十、数百时，逐个修改微服务配置就会让人抓狂，而且很容易出错。我们需要一种统一配置管理方案，可以集中管理所有实例的配置。 Nacos一方面可以将配置集中管理，另一方可以在配置变更时，及时通知微服务，实现配置的热更新。 ①在nacos中添加配置文件如何在nacos中管理配置呢？ 然后在弹出的表单中，填写配置信息： 配置文件ID有三要素：[服务名称]-[profile].[后缀名] profile就是缓解：如dev生产环境，test测试环境等 注意： 项目的核心配置，需要热更新的配置才有放到nacos管理的必要。基本不会变更的一些配置还是保存在微服务本地比较好。 ②从微服务拉取配置微服务要拉取nacos中管理的配置，并且与本地的application.yml配置合并，才能完成项目启动。 但如果尚未读取application.yml，又如何得知nacos地址呢？ 因此spring引入了一种新的配置文件：bootstrap.yaml文件（更早被读取），会在application.yml之前被读取，流程如下： 1）引入nacos-config依赖 首先，在user-service服务中，引入nacos-config的客户端依赖： 12345&lt;!--nacos配置管理依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt;&lt;/dependency&gt; 2）添加bootstrap.yaml 然后，在user-service中添加一个bootstrap.yaml文件（resource下和application.yaml同级），内容如下： 12345678910spring: application: name: userservice # 服务名称 profiles: active: dev #开发环境，这里是dev cloud: nacos: server-addr: localhost:8848 # Nacos地址 config: file-extension: yaml # 文件后缀名 这里会根据spring.cloud.nacos.server-addr获取nacos地址，再根据 $&#123;spring.application.name&#125;-$&#123;spring.profiles.active&#125;.$&#123;spring.cloud.nacos.config.file-extension&#125;作为文件id，来读取配置。（其实就是①中定义的文件名三要数） 本例中，就是去读取userservice-dev.yaml： 3）读取nacos配置 在user-service中的UserController中添加业务逻辑，读取pattern.dateformat配置： 完整代码： 1234567891011121314151617@Slf4j@RestController@RequestMapping(&quot;/user&quot;)public class UserController &#123; @Autowired private UserService userService; @Value(&quot;$&#123;pattern.dateformat&#125;&quot;) //这里就可以获得统一配置中的值了 private String dateformat; @GetMapping(&quot;now&quot;) public String now()&#123; return LocalDateTime.now().format(DateTimeFormatter.ofPattern(dateformat)); &#125; // ...略&#125; 在页面访问，可以看到效果： 2）配置热更新我们最终的目的，是修改nacos中的配置后，微服务中无需重启即可让配置生效，也就是配置热更新。 要实现配置热更新，可以使用两种方式： ①方式一在@Value注入的变量所在类上添加注解@RefreshScope注解： ②方式二使用@ConfigurationProperties注解代替@Value注解，这种方式不需要@RefreshScope 在user-service服务中，添加一个类，读取patterrn.dateformat属性： 1234567891011package cn.itcast.user.config;import lombok.Data;import org.springframework.boot.context.properties.ConfigurationProperties;import org.springframework.stereotype.Component;@Component@Data@ConfigurationProperties(prefix = &quot;pattern&quot;)public class PatternProperties &#123; private String dateformat;&#125; 在UserController中使用这个类代替@Value： 完整代码： 123456789101112131415161718192021222324252627282930313233package cn.itcast.user.web;import cn.itcast.user.config.PatternProperties;import cn.itcast.user.pojo.User;import cn.itcast.user.service.UserService;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;import java.time.LocalDateTime;import java.time.format.DateTimeFormatter;@Slf4j@RestController@RequestMapping(&quot;/user&quot;)public class UserController &#123; @Autowired private UserService userService; @Autowired private PatternProperties patternProperties; //注入该bean @GetMapping(&quot;now&quot;) public String now()&#123; return LocalDateTime.now().format(DateTimeFormatter.ofPattern(patternProperties.getDateformat())); &#125; // 略&#125; 3）配置共享其实微服务启动时，会去nacos读取多个配置文件，例如： [spring.application.name]-[spring.profiles.active].yaml，例如：userservice-dev.yaml [spring.application.name].yaml，例如：userservice.yaml 而[spring.application.name].yaml不包含环境，因此可以被多个环境共享。 下面我们通过案例来测试配置共享 ①添加一个环境共享配置我们在nacos中添加一个userservice.yaml文件： ②在user-service中读取共享配置在user-service服务中，修改PatternProperties类，读取新添加的属性： 在user-service服务中，修改UserController，添加一个方法： ③运行两个UserApplication，使用不同的profile修改UserApplication2这个启动项，改变其profile值： 这样可以在不修改代码的情况下，设置profiles 这样，UserApplication(8081)使用的profile是dev，UserApplication2(8082)使用的profile是test。 启动UserApplication和UserApplication2 访问http://localhost:8081/user/prop，结果： 访问http://localhost:8082/user/prop，结果： 可以看出来，不管是dev，还是test环境，都读取到了envSharedValue这个属性的值。 4）配置共享的优先级当nacos、服务本地同时出现相同属性时，优先级有高低之分： 远程配置&gt;本地配置 远程特定配置&gt;profile共享配置 5）搭建Nacos集群具体：https://gitee.com/f1ashades/store/blob/master/nacos%E9%9B%86%E7%BE%A4/nacos%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA.md 7. Feign远程调用1）Feign作用先来看我们以前利用RestTemplate发起远程调用的代码： 存在下面的问题： •代码可读性差，编程体验不统一 •参数复杂URL难以维护 Feign是一个声明式的http客户端，官方地址：https://github.com/OpenFeign/feign 其作用就是帮助我们优雅的实现http请求的发送，解决上面提到的问题。 Feign能让我们以一种优雅的方式完成远程调用。 2）Feign替代RestTemplateFegin的使用步骤如下： ①引入依赖我们在order-service服务的pom文件中引入feign的依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt; ②添加注解在order-service的启动类添加注解开启Feign的功能： ③编写Feign的客户端在order-service中新建一个接口，内容如下： 123456789101112package cn.itcast.order.client;import cn.itcast.order.pojo.User;import org.springframework.cloud.openfeign.FeignClient;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;@FeignClient(&quot;userservice&quot;)public interface UserClient &#123; @GetMapping(&quot;/user/&#123;id&#125;&quot;) User findById(@PathVariable(&quot;id&quot;) Long id);&#125; 定义的是一个接口interface 接口上添加注解@FeignClient(&quot;userservice&quot;) 这个客户端主要是基于SpringMVC的注解来声明远程调用的信息，比如： 服务名称：userservice 请求方式：GET 请求路径：&#x2F;user&#x2F;{id} 请求参数：Long id 返回值类型：User 这样，Feign就可以帮助我们发送http请求，无需自己使用RestTemplate来发送了。 ④测试修改order-service中的OrderService类中的queryOrderById方法，使用Feign客户端代替RestTemplate： 可以看到，我们就是用方法调用的方式来进行远程调用了 ⑤总结使用Feign的步骤： ① 引入依赖 ② 添加@EnableFeignClients注解 ③ 编写FeignClient接口 ④ 使用FeignClient中定义的方法代替RestTemplate 3）自定义配置Feign可以支持很多的自定义配置，如下表所示： 类型 作用 说明 feign.Logger.Level 修改日志级别 包含四种不同的级别：NONE、BASIC、HEADERS、FULL feign.codec.Decoder 响应结果的解析器 http远程调用的结果做解析，例如解析json字符串为java对象 feign.codec.Encoder 请求参数编码 将请求参数编码，便于通过http请求发送 feign. Contract 支持的注解格式 默认是SpringMVC的注解 feign. Retryer 失败重试机制 请求失败的重试机制，默认是没有，不过会使用Ribbon的重试 一般情况下，默认值就能满足我们使用，如果要自定义时，只需要创建自定义的@Bean覆盖默认Bean即可。 下面以日志为例来演示如何自定义配置。 ①配置文件方式基于配置文件修改feign的日志级别可以针对单个服务： 12345feign: client: config: userservice: # 针对某个微服务的配置 loggerLevel: FULL # 日志级别 也可以针对所有服务： 12345feign: client: config: default: # 这里用default就是全局配置，如果是写服务名称，则是针对某个微服务的配置 loggerLevel: FULL # 日志级别 而日志的级别分为四种： NONE：不记录任何日志信息，这是默认值。 BASIC：仅记录请求的方法，URL以及响应状态码和执行时间 HEADERS：在BASIC的基础上，额外记录了请求和响应的头信息 FULL：记录所有请求和响应的明细，包括头信息、请求体、元数据。 注意：过多的日志会影响Feign的效率，最好是NONE或BASIC ②Java代码方式也可以基于Java代码来修改日志级别，先声明一个类，然后声明一个Logger.Level的对象： 123456public class DefaultFeignConfiguration &#123; @Bean public Logger.Level feignLogLevel()&#123; return Logger.Level.BASIC; // 日志级别为BASIC &#125;&#125; 1）如果是局部生效，则把它放到对应的@FeignClient这个注解中： 1@FeignClient(value = &quot;userservice&quot;, configuration = DefaultFeignConfiguration .class) 2）如果要全局生效，将其放到启动类的@EnableFeignClients这个注解中： 1@EnableFeignClients(defaultConfiguration = DefaultFeignConfiguration.class) 4）Feign使用优化①连接池Feign底层发起http请求，依赖于其它的框架。其底层客户端实现包括： URLConnection：默认实现，不支持连接池 Apache HttpClient ：支持连接池 OKHttp：支持连接池 因此提高Feign的性能主要手段就是使用连接池代替默认的URLConnection。 这里我们用Apache的HttpClient来演示。 1）引入依赖 在order-service的pom文件中引入Apache的HttpClient依赖： 12345&lt;!--httpClient的依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;io.github.openfeign&lt;/groupId&gt; &lt;artifactId&gt;feign-httpclient&lt;/artifactId&gt;&lt;/dependency&gt; 2）配置连接池 在order-service的application.yml中添加配置： 123456789feign: client: config: default: # default全局的配置 loggerLevel: BASIC # 日志级别，BASIC就是基本的请求和响应信息 httpclient: enabled: true # 开启feign对HttpClient的支持 max-connections: 200 # 最大的连接数 max-connections-per-route: 50 # 每个路径的最大连接数 接下来，在FeignClientFactoryBean中的loadBalance方法中打断点： Debug方式启动order-service服务，可以看到这里的client，底层就是Apache HttpClient： ②日志优化日志的级别分为四种： NONE：不记录任何日志信息，这是默认值。 BASIC：仅记录请求的方法，URL以及响应状态码和执行时间 HEADERS：在BASIC的基础上，额外记录了请求和响应的头信息 FULL：记录所有请求和响应的明细，包括头信息、请求体、元数据。 注意：过多的日志会影响Feign的效率，最好是NONE或BASIC ③总结Feign的优化： 日志级别尽量用basic 使用HttpClient或OKHttp代替URLConnection 引入feign-httpClient依赖 配置文件开启httpClient功能，设置连接池参数 5）最佳实践所谓最近实践，就是使用过程中总结的经验，最好的一种使用方式。 自习观察可以发现，Feign的客户端与服务提供者的controller代码非常相似： feign客户端： UserController： 有没有一种办法简化这种重复的代码编写呢？ ①继承方式一样的代码可以通过继承来共享： 1）定义一个API接口，利用定义方法，并基于SpringMVC注解做声明。 2）Feign客户端和Controller都集成改接口 优点： 简单 实现了代码共享 缺点： 服务提供方、服务消费方紧耦合 参数列表中的注解映射并不会继承，因此Controller中必须再次声明方法、参数列表、注解 ②抽取方式将Feign的Client抽取为独立模块，并且把接口有关的POJO、默认的Feign配置都放到这个模块中，提供给所有消费者使用。 例如，将UserClient、User、Feign的默认配置都抽取到一个feign-api包中，所有微服务引用该依赖包，即可直接使用。 ③实现基于抽取的最佳实践a）抽取首先创建一个module，命名为feign-api： 项目结构： 在feign-api中然后引入feign的starter依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt; 然后，order-service中编写的UserClient、User、DefaultFeignConfiguration都复制到feign-api项目中 b）在order-service中使用feign-api首先，删除order-service中的UserClient、User、DefaultFeignConfiguration等类或接口。 在order-service的pom文件中中引入feign-api的依赖： 12345&lt;dependency&gt; &lt;groupId&gt;cn.itcast.demo&lt;/groupId&gt; &lt;artifactId&gt;feign-api&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt;&lt;/dependency&gt; 修改order-service中的所有与上述三个组件有关的导包部分，改成导入feign-api中的包 c）重启测试重启后，发现服务报错了： 这是因为UserClient现在在cn.itcast.feign.clients包下，这是我们自己导入的jar包， 当前项目order-service的引导类中加了@EnableFeignClients，它会扫描自己所在的包，即cn.itcast.order， 而导入的jar包是在cn.itcast.feign.clients下，所以虽然加上了@EnableFeignClients，UserClient也不会被扫描到，也就不会被加载到spring的容器中了 d）解决扫描包问题方式一： 指定Feign应该扫描的包： 1@EnableFeignClients(basePackages = &quot;cn.itcast.feign.clients&quot;) 这种方式就是把所有clients都扫描到，userClients、stockClients…. 方式二： 指定需要加载的Client接口： 1@EnableFeignClients(clients = &#123;UserClient.class&#125;) 这种方式就是精准定位，需要userClients就加入userClients，该方法更推荐 8. Gateway服务网关1）网关作用Gateway网关是我们服务的守门神，所有微服务的统一入口。 网关的核心功能特性： 请求路由 如将orderservice的请求路由到对应的orderservice为服务中 权限控制 对请求进行权限管理 限流 1000个请求先放500个进，等500个处理的差不多了，再放剩下的500个 架构图： 路由和负载均衡：一切请求都必须先经过gateway，但网关不处理业务，而是根据某种规则，把请求转发到某个微服务，这个过程叫做路由。当然路由的目标服务有多个时，还需要做负载均衡。 权限控制：网关作为微服务入口，需要校验用户是是否有请求资格，如果没有则进行拦截。 限流：当请求流量过高时，在网关中按照下流的微服务能够接受的速度来放行请求，避免服务压力过大。 在SpringCloud中网关的实现包括两种： gateway zuul Zuul是基于Servlet的实现，属于阻塞式编程。 而SpringCloudGateway则是基于Spring5中提供的WebFlux，属于响应式编程的实现，具备更好的性能： Spring Cloud Gateway 是 Spring Cloud 的一个全新项目，该项目是基于 Spring 5.0，Spring Boot 2.0 和 Project Reactor 等响应式编程和事件流技术开发的网关，它旨在为微服务架构提供一种简单有效的统一的 API 路由管理方式。 2）gateway快速入门下面，我们就演示下网关的基本路由功能。基本步骤如下： 创建SpringBoot工程gateway，引入网关依赖 编写启动类 编写基础配置和路由规则 启动网关服务进行测试 ①创建gateway服务，引入依赖创建服务： 引入依赖： 12345678910&lt;!--网关--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-gateway&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!--nacos服务发现依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;&lt;/dependency&gt; ②编写启动类123456789101112package cn.itcast.gateway;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class GatewayApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(GatewayApplication.class, args); &#125;&#125; ③编写基础配置和路由规则创建application.yml文件，内容如下： 123456789101112131415server: port: 10010 # 网关端口spring: application: name: gateway # 服务名称 cloud: nacos: server-addr: localhost:8848 # nacos地址 gateway: routes: # 网关路由配置 - id: user-service # 路由id，自定义，只要唯一即可 # uri: http://127.0.0.1:8081 # 路由的目标地址 http就是固定地址 uri: lb://userservice # 路由的目标地址 lb就是负载均衡，后面跟服务名称 predicates: # 路由断言，也就是判断请求是否符合路由规则的条件 - Path=/user/** # 这个是按照路径匹配，只要以/user/开头就符合要求 gateway也是一种服务，也需要注册到nacos中 uri有两种写法：固定地址和服务名称 固定地址：uri: http://127.0.0.1:8081 服务名称：uri: lb://userservice 我们将符合Path 规则的一切请求，都代理到 uri参数指定的地址。 本例中，我们将 /user/**开头的请求，代理到lb://userservice，lb是负载均衡，根据服务名拉取服务列表，实现负载均衡。 ④重启测试重启网关，访问http://localhost:10010/user/1时，符合`/user/**`规则，请求转发到uri：http://userservice/user/1，得到了结果： ⑤路由的流程图整个访问的流程如下： ⑥总结网关搭建步骤： 创建项目，引入nacos服务发现和gateway依赖 配置application.yml，包括服务基本信息、nacos地址、路由 路由配置包括： 路由id：路由的唯一标示 路由目标（uri）：路由的目标地址，http代表固定地址，lb代表根据服务名负载均衡 路由断言（predicates）：判断路由的规则， 路由过滤器（filters）：对请求或响应做处理 3）断言工厂我们在配置文件中写的断言规则只是字符串，这些字符串会被Predicate Factory读取并处理，转变为路由判断的条件 例如Path&#x3D;&#x2F;user&#x2F;**是按照路径匹配，这个规则是由 org.springframework.cloud.gateway.handler.predicate.PathRoutePredicateFactory类来 处理的，像这样的断言工厂在SpringCloudGateway还有十几个: 名称 说明 示例 After 是某个时间点后的请求 - After&#x3D;2037-01-20T17:42:47.789-07:00[America&#x2F;Denver] Before 是某个时间点之前的请求 - Before&#x3D;2031-04-13T15:14:47.433+08:00[Asia&#x2F;Shanghai] Between 是某两个时间点之前的请求 - Between&#x3D;2037-01-20T17:42:47.789-07:00[America&#x2F;Denver], 2037-01-21T17:42:47.789-07:00[America&#x2F;Denver] Cookie 请求必须包含某些cookie - Cookie&#x3D;chocolate, ch.p Header 请求必须包含某些header - Header&#x3D;X-Request-Id, \\d+ Host 请求必须是访问某个host（域名） - Host&#x3D;.somehost.org,.anotherhost.org Method 请求方式必须是指定方式 - Method&#x3D;GET,POST Path 请求路径必须符合指定规则 - Path&#x3D;&#x2F;red&#x2F;{segment},&#x2F;blue&#x2F;** Query 请求参数必须包含指定参数 - Query&#x3D;name, Jack或者- Query&#x3D;name RemoteAddr 请求者的ip必须是指定范围 - RemoteAddr&#x3D;192.168.1.1&#x2F;24 Weight 权重处理 4）过滤器工厂GatewayFilter是网关中提供的一种过滤器，可以对进入网关的请求和微服务返回的响应做处理： ①路由过滤器的种类Spring提供了31种不同的路由过滤器工厂。例如： 名称 说明 AddRequestHeader 给当前请求添加一个请求头 RemoveRequestHeader 移除请求中的一个请求头 AddResponseHeader 给响应结果中添加一个响应头 RemoveResponseHeader 从响应结果中移除有一个响应头 RequestRateLimiter 限制请求的流量 …… ②请求头过滤器下面我们以AddRequestHeader 为例来讲解。 需求：给所有进入userservice的请求添加一个请求头：Truth&#x3D;itcast is freaking awesome! 只需要修改gateway服务的application.yml文件，添加路由过滤即可： 12345678910spring: cloud: gateway: routes: - id: user-service uri: lb://userservice predicates: - Path=/user/** filters: # 过滤器 - AddRequestHeader=Truth, Itcast is freaking awesome! # 添加请求头 当前过滤器写在userservice路由下，因此仅仅对访问userservice的请求有效。 ③默认过滤器如果要对所有的路由都生效，则可以将过滤器工厂写到default下。格式如下： 12345678910spring: cloud: gateway: routes: - id: user-service uri: lb://userservice predicates: - Path=/user/** default-filters: # 默认过滤项 - AddRequestHeader=Truth, Itcast is freaking awesome! routes和default-filters平级，是全局的，并不在某个服务的具体route下 ④总结过滤器的作用： 对路由的请求或响应做加工处理，比如添加请求头 配置在路由下的过滤器只对当前路由的请求生效 defaultFilters的作用： 对所有路由都生效的过滤器 5）全局过滤器过滤器工厂网关提供了31种，但每一种过滤器的作用都是固定的。 如果我们希望拦截请求，做自己的业务逻辑则没办法实现，这时候需要就需要全局过滤器了。 ①全局过滤器作用全局过滤器的作用也是处理一切进入网关的请求和微服务响应，与GatewayFilter的作用一样。区别在于GatewayFilter通过配置定义，处理逻辑是固定的；而GlobalFilter的逻辑需要自己写代码实现。 定义方式是实现GlobalFilter接口。 12345678910public interface GlobalFilter &#123; /** * 处理当前请求，有必要的话通过&#123;@link GatewayFilterChain&#125;将请求交给下一个过滤器处理 * * @param exchange 请求上下文，里面可以获取Request、Response等信息 * @param chain 用来把请求委托给下一个过滤器 * @return &#123;@code Mono&lt;Void&gt;&#125; 返回标示当前过滤器业务结束 */ Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain);&#125; 在filter中编写自定义逻辑，可以实现下列功能： 登录状态判断 权限校验 请求限流等 ②自定义全局过滤器需求：定义全局过滤器，拦截请求，判断请求的参数是否满足下面条件： 参数中是否有authorization， authorization参数值是否为admin 如果同时满足则放行，否则拦截 实现： 在gateway中定义一个过滤器： 1234567891011121314151617181920212223242526272829303132package cn.itcast.gateway.filters;import org.springframework.cloud.gateway.filter.GatewayFilterChain;import org.springframework.cloud.gateway.filter.GlobalFilter;import org.springframework.core.annotation.Order;import org.springframework.http.HttpStatus;import org.springframework.stereotype.Component;import org.springframework.web.server.ServerWebExchange;import reactor.core.publisher.Mono;@Order(-1)@Componentpublic class AuthorizeFilter implements GlobalFilter &#123; @Override public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) &#123; // 1.获取请求参数 MultiValueMap&lt;String, String&gt; params = exchange.getRequest().getQueryParams(); // 2.获取authorization参数，getFirst是取出第一个匹配的 String auth = params.getFirst(&quot;authorization&quot;); // 3.校验 if (&quot;admin&quot;.equals(auth)) &#123; // 放行 //其实就是从filters链中找出下一个filter，传递exchange，这也就是放行了 return chain.filter(exchange); &#125; // 4.拦截 // 4.1.禁止访问，设置状态码 exchange.getResponse().setStatusCode(HttpStatus.FORBIDDEN); // 4.2.结束处理 return exchange.getResponse().setComplete(); &#125;&#125; @Order(-1)：定义过滤器优先级 或者implement Ordered接口 ④过滤器执行顺序请求进入网关会碰到三类过滤器：当前路由的过滤器、DefaultFilter、GlobalFilter 请求路由后，会将当前路由过滤器和DefaultFilter、GlobalFilter，合并到一个过滤器链（集合）中，排序后依次执行每个过滤器： 排序的规则是什么呢？ 每一个过滤器都必须指定一个int类型的order值，order值越小，优先级越高，执行顺序越靠前。 GlobalFilter通过实现Ordered接口，或者添加@Order注解来指定order值，由我们自己指定 路由过滤器和defaultFilter的order由Spring指定，默认是按照声明顺序从1递增。 当过滤器的order值一样时，会按照 defaultFilter &gt; 路由过滤器 &gt; GlobalFilter的顺序执行。 为什么可以合并成一个过滤器链？ ①对于filter和default filter org.springframework.cloud.gateway.route.RouteDefinitionRouteLocator#getFilters()方法是先加载defaultFilters，然后再加载某个route的filters，然后合并。 这两种本质上都是GatewayFilter，很容易就能合并成filter链 ②对于global filter org.springframework.cloud.gateway.handler.FilteringWebHandler#handle()方法会加载全局过滤器，与前面的过滤器合并后根据order排序，组织过滤器链 虽然全局过滤器类型是GlobalFilter，但是网关中有GlobalFilter的适配器，将其适配成一个GatewayFilter 因此global filter也就可以变成一个加入到GatewayFilter加入到filter链中了 6）跨域问题①什么是跨域问题跨域：域名不一致就是跨域，主要包括： 域名不同： www.taobao.com 和 www.taobao.org 和 www.jd.com 和 miaosha.jd.com 域名相同，端口不同：localhost:8080和localhost8081 跨域问题：浏览器禁止请求的发起者与服务端发生跨域ajax请求，请求被浏览器拦截的问题 当我们在localhost:8081向localhost:8082发送 跨域ajax请求，浏览器会禁止这个操作 解决方案：CORS-https://www.ruanyifeng.com/blog/2016/04/cors.html 简单来说，CORS就是浏览器在发生跨域请求的时候，会去询问服务端，看服务端是否同意该域名的跨域请求， 所以我们需要在服务端进行跨域请求的配置，配置哪些域名能对本服务器进行跨域访问 ②模拟跨域问题页面文件： 放入tomcat或者nginx这样的web服务器中，启动并访问。 可以在浏览器控制台看到下面的错误： 从localhost:8090访问localhost:10010，端口不同，显然是跨域的请求。 ③解决跨域问题在gateway服务的application.yml文件中，添加下面的配置： 123456789101112131415161718spring: cloud: gateway: globalcors: # 全局的跨域处理 add-to-simple-url-handler-mapping: true # 解决options请求被拦截问题 corsConfigurations: &#x27;[/**]&#x27;: allowedOrigins: # 允许哪些网站的跨域请求 - &quot;http://localhost:8090&quot; allowedMethods: # 允许的跨域ajax的请求方式 - &quot;GET&quot; - &quot;POST&quot; - &quot;DELETE&quot; - &quot;PUT&quot; - &quot;OPTIONS&quot; allowedHeaders: &quot;*&quot; # 允许在请求中携带的头信息 allowCredentials: true # 是否允许携带cookie maxAge: 360000 # 这次跨域检测的有效期","categories":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://example.com/categories/Spring-Cloud/"}],"tags":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://example.com/tags/Spring-Cloud/"}]},{"title":"Redis相关","slug":"1redis - 副本","date":"2022-04-15T08:05:12.000Z","updated":"2022-08-17T03:45:57.038Z","comments":true,"path":"2022/04/15/1redis - 副本/","link":"","permalink":"http://example.com/2022/04/15/1redis%20-%20%E5%89%AF%E6%9C%AC/","excerpt":"基本指令使用：基本数据类型和命令、SpringData封装 redis缓存：缓存更新、缓存穿透、缓存雪崩、缓存击穿 分布式缓存：redis持久化、redis主从、redis哨兵sentinel、redis分片集群 redis实践：键值设计（BigKey）、批处理（pipeline）、服务端优化（持久化、慢查询、内存划分、集群选择） redis原理： ​ redis底层数据结构：SDS、intSet、ZipList、QuickList、SkipList、Dict、RedisObject ​ redis基本数据类型：String、List、Set、ZSet、Hash ​ redis网络模型：五种IO、redis—IO多路复用+事件派发 ​ redis通信协议：RESP ​ redis过期策略：懒惰删除、周期删除（slow、fast） ​ 内存淘汰策略：8种淘汰策略、基本流程","text":"基本指令使用：基本数据类型和命令、SpringData封装 redis缓存：缓存更新、缓存穿透、缓存雪崩、缓存击穿 分布式缓存：redis持久化、redis主从、redis哨兵sentinel、redis分片集群 redis实践：键值设计（BigKey）、批处理（pipeline）、服务端优化（持久化、慢查询、内存划分、集群选择） redis原理： ​ redis底层数据结构：SDS、intSet、ZipList、QuickList、SkipList、Dict、RedisObject ​ redis基本数据类型：String、List、Set、ZSet、Hash ​ redis网络模型：五种IO、redis—IO多路复用+事件派发 ​ redis通信协议：RESP ​ redis过期策略：懒惰删除、周期删除（slow、fast） ​ 内存淘汰策略：8种淘汰策略、基本流程 一、基本使用1.数据类型和命令1）通用命令 通用指令是部分数据类型的，都可以使用的指令，常见的有如下表格所示 指令 描述 KEYS 查看符合模板的所有key，不建议在生产环境设备上使用 DEL 删除一个指定的key EXISTS 判断key是否存在 EXPIRE 给一个key设置有效期，有效期到期时该key会被自动删除 TTL 查看一个KEY的剩余有效期 可以通过help [command] 可以查看一个命令的具体用法！ 2）String类型①基本使用 String类型，也就是字符串类型，是Redis中最简单的存储类型。 其value是字符串，不过根据字符串的格式不同，又可以分为3类： string：普通字符串 int：整数类型，可以做自增、自减操作 float：浮点类型，可以做自增、自减操作 不管是哪种格式，底层都是字节数组形式存储，只不过是编码方式不同。字符串类型的最大空间不能超过512m. KEY VALUE msg hello world num 10 score 92.5 当value是数字，redis会将其转换成二进制的方式去存储，这样一个字节（8bit）就能表示很大的数字‘ 当value是字符串，redis只能将其转换成字节码来进行存储 String的常见命令有如下表格所示 命令 描述 SET 添加或者修改已经存在的一个String类型的键值对 GET 根据key获取String类型的value MSET 批量添加多个String类型的键值对 MGET 根据多个key获取多个String类型的value INCR 让一个整型的key自增1 INCRBY 让一个整型的key自增并指定步长，例如：incrby num 2 让num值自增2，（负数就代表自减&#x3D;DECR命令） INCRBYFLOAT 让一个浮点类型的数字自增并指定步长 SETNX 添加一个String类型的键值对，前提是这个key不存在，否则不执行，类似set k v -nx SETEX 添加一个String类型的键值对，并且指定有效期，类似set k v -ex 5 ②key的层级命名Redis的key允许有多个单词形成层级结构，多个单词之间用” ：“隔开，格式如下： 1项目名:业务名:类型:id 这个格式并非固定，也可以根据自己的需求来删除或添加词条。 例如我们的项目名称叫 fla，有user和product两种不同类型的数据，我们可以这样定义key： user相关的key：fla:user:1 product相关的key：fla:product:1 如果Value是一个Java对象，例如一个User对象，则可以将对象序列化为JSON字符串后存储 KEY VALUE heima:user:1 {“id”:1, “name”: “Jack”, “age”: 21} heima:product:1 {“id”:1, “name”: “小米11”, “price”: 4999} 3）Hash类型 Hash类型，也叫散列，其value是一个无序字典，类似于Java中的HashMap结构。 Hash结构可以将对象中的每个字段独立存储，可以针对单个字段做CRUD Hash的常见命令有： 命令 描述 HSET key field value 添加或者修改hash类型key的field的值 HGET key field 获取一个hash类型key的field的值 HMSET hmset 和 hset 效果相同 ，4.0之后hmset可以弃用了 HMGET 批量获取多个hash类型key的field的值 HGETALL 获取一个hash类型的key中的所有的field和value HKEYS 获取一个hash类型的key中的所有的field HVALS 获取一个hash类型的key中的所有的value HINCRBY 让一个hash类型key的字段值自增并指定步长 HSETNX 添加一个hash类型的key的field值，前提是这个field不存在，否则不执行 4）List类型 Redis中的List类型与Java中的LinkedList类似，可以看做是一个双向链表结构。既可以支持正向检索和也可以支持反向检索。 特征也与LinkedList类似： 有序 元素可以重复 插入和删除快 查询速度一般 常用来存储一个有序数据，例如：朋友圈点赞列表，评论列表等. List的常见命令有 命令 描述 LPUSH key element … 向列表左侧插入一个或多个元素 LPOP key 移除并返回列表左侧的第一个元素，没有则返回nil RPUSH key element … 向列表右侧插入一个或多个元素 RPOP key 移除并返回列表右侧的第一个元素 LRANGE key star end 返回一段角标范围内的所有元素 BLPOP和BRPOP 与LPOP和RPOP类似，只不过在没有元素时等待指定时间，而不是直接返回nil BLPOP和BRPOP在没有元素的时候会阻塞，直到有元素或者等待时间到了为止 思考问题 如何利用List结构模拟一个栈? 先进后出，入口和出口在同一边 如何利用List结构模拟一个队列? 先进先出，入口和出口在不同边 如何利用List结构模拟一个阻塞队列? 入口和出口在不同边 出队时采用BLPOP或BRPOP 5）Set类型 Redis的Set结构与Java中的HashSet类似，可以看做是一个value为null的HashMap。因为也是一个hash表，因此具备与HashSet类似的特征 无序 每一个元素通过hash算法分配到指定位置，所以不保证顺序 元素不可重复 查找快 支持交集、并集、差集等功能 Set的常见命令有 命令 描述 SADD key member … 向set中添加一个或多个元素 SREM key member … 移除set中的指定元素 SCARD key 返回set中元素的个数 SISMEMBER key member 判断一个元素是否存在于set中 SMEMBERS key 获取set中的所有元素 SINTER key1 key2 … 求key1与key2的交集 SDIFF key1 key2 … 求key1与key2的差集 SUNION key1 key2 .. 求key1和key2的并集 注意：SDIFF key1 key2 求出来的结果是key1中有，但是key2没有的 123456127.0.0.1:6379&gt; sadd yk 1 2 3(integer) 3127.0.0.1:6379&gt; sadd yd 1 2 4(integer) 3127.0.0.1:6379&gt; SDIFF yk yd1) &quot;3&quot; //注意结果是yk中有的，yd中没有的 ​ 6）SortedSet类型 Redis的SortedSet是一个可排序的set集合，与Java中的TreeSet有些类似，但底层数据结构却差别很大。SortedSet中的每一个元素都带有一个score属性，可以基于score属性对元素排序，底层的实现是一个跳表（SkipList）加 hash表。 SortedSet具备下列特性： 可排序 元素不重复 查询速度快 因为SortedSet的可排序特性，经常被用来实现排行榜这样的功能。 SortedSet的常见命令有 命令 描述 ZADD key score member 添加一个或多个元素到sorted set ，如果已经存在则更新其score值 ZREM key member 删除sorted set中的一个指定元素 ZSCORE key member 获取sorted set中的指定元素的score值 ZRANK key member 获取sorted set 中的指定元素的排名（按score升序排） ZREVRANK key member 和ZRANK类似，但这是按score降序排后的排名 ZCARD key 获取sorted set中的元素个数 ZCOUNT key min max 统计score值在给定范围内的所有元素的个数 ZINCRBY key increment member 让sorted set中的指定元素自增，步长为指定的increment值 ZRANGE key min max 按照score升序排序后，获取指定排名范围内的元素，传入的是排名 ZREVRANGE key min max 按照score降序排序后，获取指定排名范围内的元素，传入的是排名 ZRANGEBYSCORE key min max 按照score排序后，获取指定score范围内的元素 ZDIFF、ZINTER、ZUNION 求差集、交集、并集 默认的排名方式都是按score升序，加上REV代表是按score降序排 rank查某一member的排名，count查score在某一范围的数量，range查某一排名中的具体members 7）bitMap Redis中是利用String类型数据结构实现BitMap，因此最大上限是512MB，转换为bit则是 2^32个bit位。 BitMap的操作命令有： SETBIT：向指定位置（offset）存入一个0或1 GETBIT ：获取指定位置（offset）的bit值 BITCOUNT ：统计BitMap中值为1的bit位的数量 BITFIELD ：操作（查询、修改、自增）BitMap中bit数组中的指定位置（offset）的值 BITFIELD sign:5:202203 GET u14 0 u代表是无符号位 https://cloud.tencent.com/developer/section/1374165 BITFIELD_RO ：获取BitMap中bit数组，并以十进制形式返回 BITOP ：将多个BitMap的结果做位运算（与 、或、异或） BITPOS ：查找bit数组中指定范围内第一个0或1出现的位置 如果我们使用java客户端接收，接收到的是这个bitMap按照askII转换过的一个字符串，而不是这个 8）GeoGEO就是Geolocation的简写形式，代表地理坐标。Redis在3.2版本中加入了对GEO的支持，允许存储地理坐标信息，帮助我们根据经纬度来检索数据。常见的命令有： GEOADD：添加一个地理空间信息，包含：经度（longitude）、纬度（latitude）、值（member） GEODIST：计算指定的两个点之间的距离并返回 GEOHASH：将指定member的坐标转为hash字符串形式并返回 GEOPOS：返回指定member的坐标 GEORADIUS：指定圆心、半径，找到该圆内包含的所有member，并按照与圆心之间的距离排序后返回。6.以后已废弃 GEOSEARCH：在指定范围内搜索member，并按照与指定点之间的距离排序后返回。范围可以是圆形或矩形。6.2.新功能 GEOSEARCHSTORE：与GEOSEARCH功能一致，不过可以把结果存储到一个指定的key。 6.2.新功能 2.Spring Data 封装​ redis在不同的客户端有不同的操作指令，jedis的指令和命令行指令相同，但是lettuce中就对于不同操作自己定义了一套指令。spring对redis客户端的实现做了整合，提供了一套完整的api来提供一致的接口，统一了不同客户端的操作方式。 1）准备工作依赖： 123456789101112131415161718&lt;!--连接池依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!--jackson转换--&gt;&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!--redis的starter--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 自定义redisTemplate中用到jason的序列化需要导入jason依赖 如果不配置commons-pool2，连接池是不会生效的 yml配置： 123456789101112spring: redis: port: 6379 host: 127.0.0.1 #连接地址 password: 1234 #连接密码 lettuce: lettuce: pool: max-active: 8 #最大连接数 max-idle: 8 #最大空闲数 min-idle: 0 #最小空闲数 max-wait: 100ms #连接等待时间 2）RedisTemplate①默认序列化测试类： 12345678910public class RedisTest &#123; @Autowired private RedisTemplate redisTemplate; @Test public void t1()&#123; ValueOperations ops = redisTemplate.opsForValue(); ops.set(&quot;name&quot;, &quot;moxioxi&quot;);// ops.set(&quot;stu1&quot;,new Stu(&quot;yk&quot;,31)); 如果我们执行这一句话会直接报错，因为没有针对user进行序列化 &#125; 结果： 原因： 当我们查看源码可以看到 1234567891011121314151617public class RedisTemplate&lt;K, V&gt; extends RedisAccessor implements RedisOperations&lt;K, V&gt;, BeanClassLoaderAware &#123; private boolean enableTransactionSupport = false; private boolean exposeConnection = false; private boolean initialized = false; private boolean enableDefaultSerializer = true; @Nullable private RedisSerializer&lt;?&gt; defaultSerializer; @Nullable private ClassLoader classLoader; @Nullable private RedisSerializer keySerializer = null;//1. @Nullable private RedisSerializer valueSerializer = null;//2. @Nullable private RedisSerializer hashKeySerializer = null;//3. @Nullable. private RedisSerializer hashValueSerializer = null;//4. ​ 实际上RedisTemplate是具有泛型的，并且在内部定义了4种序列化的格式，分别针对key，value，hashKey，hashValue，当我们没有相应的serializer的时候，默认的就是jdk的序列化方法。redisTemplate首先会根据设置的序列化器将数据序列化，然后将序列化的值存入redis中。 ​ 为了解决这个问题我们有两种方式 配置RedisTemplate，在RedisTemplate中提供serializer 手动序列化 ​ ②配置RedisTemplate注意我们这里由jason的处理，一定要有jason的依赖： 12345&lt;!--jackson转换--&gt;&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;&lt;/dependency&gt; 测试： 1234567@Test public void t1()&#123; ValueOperations ops = redisTemplate.opsForValue(); ops.set(&quot;stu1&quot;,new Stu(&quot;yk&quot;,31)); Stu stu1= (Stu)ops.get(&quot;stu1&quot;); System.out.println(stu1); &#125; RedisTemplate配置： 12345678910111213141516171819@Configurationpublic class redisConfig &#123; @Bean public RedisTemplate&lt;String, Object&gt; redisTemplate(RedisConnectionFactory redisConnectionFactory) &#123; // 1.创建Template RedisTemplate&lt;String, Object&gt; redisTemplate = new RedisTemplate&lt;&gt;(); // 2.设置连接工厂 redisTemplate.setConnectionFactory(redisConnectionFactory); // 3.设置序列化工具 GenericJackson2JsonRedisSerializer jsonRedisSerializer = new GenericJackson2JsonRedisSerializer(); // 4.key和 hashKey采用 string序列化 redisTemplate.setKeySerializer(RedisSerializer.string()); redisTemplate.setHashKeySerializer(RedisSerializer.string()); // 5.value和 hashValue采用 JSON序列化 redisTemplate.setValueSerializer(jsonRedisSerializer); redisTemplate.setHashValueSerializer(jsonRedisSerializer); return redisTemplate; &#125;&#125; boot自动装载了一个RedisTemplate的bean（id是redisTemplate），但是其中所有的serializer都是jdk的serializer，我们需要自己重写一个 一个基本思路：key都是String格式，value都是json格式 结果 可以看到key是String，value也是jason格式，并且通过打印结果我们看到get的String可以直接反序列化 12控制台: Stu(name=yk, age=31) 原因很简单，当我们在RedisTemplate自定义了json的serializer后，它会帮我们加上序列化的全路径，这样就可以直接反序列化 12345&#123; &quot;@class&quot;: &quot;com.fla.domain.pojo.Stu&quot;, &quot;name&quot;: &quot;yk&quot;, &quot;age&quot;: 31&#125; 但这样为我们的jason加上了额外的数据，增加了额外负担，因此我们应该手动进行序列化和反序列化、 ③StringRedisTemplate（推荐）​ StringRedisTemplate其实就是泛型都是String，key和value都是String，然后手动序列化和反序列化 测试类： 123456789101112131415161718@Autowiredprivate StringRedisTemplate stringRedisTemplate;// JSON工具private static final ObjectMapper mapper = new ObjectMapper();@Testpublic void t1() throws JsonProcessingException &#123; // 准备对象 Stu stu = new Stu(&quot;yc&quot;,32); // 手动序列化 String json = mapper.writeValueAsString(stu); // 写入一条数据到redis stringRedisTemplate.opsForValue().set(&quot;stu2&quot;,json); // 读取数据 String stu2 = stringRedisTemplate.opsForValue().get(&quot;stu2&quot;); // 反序列化 System.out.println(mapper.readValue(stu2, Stu.class));&#125; 在存入之前手动序列化数据 在接收到数据后手动反序列化数据 这里用的序列化工具是ObjectMapper，也可以使用其它的工具如fastJason 这是更推荐的方式，因为不会在redis中增加额外的数据 二、实际使用1.Redis实现session共享1）原始session共享的问题 通过短信登录的案例来理解，我们在进行短信登录时首先会在发送验证码并进行校验，在校验成功后会根据手机号在数据库中查询是否有对应用户，若没有则进行注册。最后将该用户的部分信息(主要是用户名等非敏感信息，密码等敏感信息不能存，可以专门封装成一个UserDTO)存入session中。 ​ 这样我们配置一个拦截器，拦截有需要验证登录状态的请求，从session中取出这个userDTO，并通过ThreadLocal的方式绑定在该线程（这么做是为了后序的controller中能拿到这个userDTO）。 这样做会有很多问题： 我们一次会话中的请求可能被负载到不同的tomcat中，多台tomcat之间不能直接共享session数据，需要进行session备份。 这样并不是一个好的解决办法，因为多台tomcat备份同样的数据，造成了内存空间的浪费 并且拷贝是需要时间的，若在这段时间有请求，则会发生数据延迟的现象 我们需要一个替代品 数据共享，多台tomcat都能访问 内存存储，需要较好的性能 key-value格式，类似session 通过以上信息，redis是一个完美的解决方案 2）redis解决①验证码存redis 123456789101112131415161718@Overridepublic Result sendCode(String phone, HttpSession session) &#123; // 1.校验手机号 if (RegexUtils.isPhoneInvalid(phone)) &#123; // 2.如果不符合，返回错误信息 return Result.fail(&quot;手机号格式错误！&quot;); &#125; // 3.符合，生成验证码 String code = RandomUtil.randomNumbers(6); // 4.保存验证码到 session stringRedisTemplate.opsForValue().set(LOGIN_CODE_KEY + phone, code, LOGIN_CODE_TTL, TimeUnit.MINUTES); // 5.发送验证码 SMSUtils.send(&quot;发送短信验证码成功，验证码：&#123;&#125;&quot;, code); // 返回ok return Result.ok();&#125; 将发送的验证码存入redis中 注意redis中存入的key：前缀（login:code:）+phoneNum ②用户状态存入redis 部分UserService中的log代码： 1234567891011121314151617// 7.保存用户信息到 redis中// 7.1.随机生成token，作为登录令牌String token = UUID.randomUUID().toString(true);// 7.2.将User对象转为HashMap存储UserDTO userDTO = BeanUtil.copyProperties(user, UserDTO.class);Map&lt;String, Object&gt; userMap = BeanUtil.beanToMap(userDTO, new HashMap&lt;&gt;(), CopyOptions.create() .setIgnoreNullValue(true) .setFieldValueEditor((fieldName, fieldValue) -&gt; fieldValue.toString()));// 7.3.存储String tokenKey = LOGIN_USER_KEY + token;stringRedisTemplate.opsForHash().putAll(tokenKey, userMap);// 7.4.设置token有效期stringRedisTemplate.expire(tokenKey, LOGIN_USER_TTL, TimeUnit.MINUTES);// 8.返回tokenreturn Result.ok(token); 将随机生成的UUID作为redis的key的一部分，redis的key&#x3D;前缀token（login:token:）+ UUID 将前缀的token作为数据返回回去 存入的时候value是HashMap，为了避免使用put多次与redis交互，这里使用putAll一次性存完 注意这里在userDTO中的id是long值，stringRedisTemplate要求key和value都要是String值，所以这里需要处理一下，否则报错 注意和session一样，我们需要添加有效期 但我我们还需要做到用户若一直在请求那么就要一直更新这个有效期，这个可以用拦截器实现 ③拦截器校验登录状态 拦截器： 1234567891011121314151617181920212223242526272829303132333435public class LoginInterceptor implements HandlerInterceptor &#123; private StringRedisTemplate stringRedisTemplate; public LoginInterceptor(StringRedisTemplate stringRedisTemplate) &#123; this.stringRedisTemplate = stringRedisTemplate; &#125; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; //1.获取请求头中的token String token = request.getHeader(&quot;authorization&quot;) ; if (StrUtil.isBlank(token)) &#123; //不存在，拦截，返回401状态码response.setStatus (401); // return false; &#125; // 2.基于TOKEN获取redis中的用户 String key = RedisConstants.LOGIN_USER_KEY + token; Map&lt;Object,Object &gt; userMap = stringRedisTemplate.opsForHash().entries(key); // 3. 判断用户是否存在 if (userMap.isEmpty()) &#123; //4.不存在，拦截，返回401状态码response.setstatus(401); return false; &#125; // 5.将查询到的Hash数据转为userDTO对象 UserDTO userDTO = BeanUtil.fillBeanthMap(userMap,new UserDTO(),false); // 6.存在，保存用户信息到ThreadLocal UserHolder.saveUser(userDTO); // 7.刷新token有效期 stringRedisTemplate.expire(key, RedisConstants.LOGIN_USER_TTL, TimeUnit.MINUTES); // 8.放行 return true; &#125;&#125; 注意：拦截器中不能autowired需要的stringRedisTemplate，因为我们在springMvcConfig中配置的拦截器是直接new()出来的，并没有交给spring管理，自然也不能获得spring容器中的其它对象，所以在这里我们通过构造方法的方式获得了，详细结合下方sringMvcConfig配置看 根据UUID在redis中查询到了对应的userDTO后需要绑定在该线程的threadLocal中，并更新token有效期 springMvcConfig： 1234567891011121314151617181920212223242526@Configurationpublic class MvcConfig implements WebMvcConfigurer &#123; @Resource private StringRedisTemplate stringRedisTemplate; @Override public void addInterceptors(InterceptorRegistry registry) &#123; // 登录拦截器 registry.addInterceptor(new LoginInterceptor(stringRedisTemplate)) //构造器注入 .excludePathPatterns( &quot;/shop/**&quot;, &quot;/voucher/**&quot;, &quot;/shop-type/**&quot;, &quot;/upload/**&quot;, &quot;/blog/hot&quot;, &quot;/user/code&quot;, &quot;/user/login&quot; ).order(1); // token刷新的拦截器 registry.addInterceptor(new RefreshTokenInterceptor(stringRedisTemplate)). addPathPatterns(&quot;/**&quot;). order(0); &#125;&#125; 拦截器需要stringRedisTemplate，但是拦截器并没有交给spring管理，无法autowired，所以首先在配置中获取，然后再通过构造方法注入（这是mvc配置，不是拦截器，所以可以@AutoWired或@Resource） ④一个小问题​ 因为我们目前设置的拦截器仅仅拦截了部分必须要登录用户状态（如主页）的操作，但我们在访问一些不需要登录信息的资源的时候（如排行榜）时我们也应该更新token的有效期，所以我们可以再设置一个拦截器完成该操作 更新token有效期的拦截器： 1234567891011121314151617181920212223242526272829303132public class RefreshTokenInterceptor implements HandlerInterceptor &#123; private StringRedisTemplate stringRedisTemplate; public RefreshTokenInterceptor(StringRedisTemplate stringRedisTemplate) &#123; this.stringRedisTemplate = stringRedisTemplate; &#125; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; // 1.获取请求头中的token String token = request.getHeader(&quot;authorization&quot;); if (StrUtil.isBlank(token)) &#123; return true; &#125; // 2.基于TOKEN获取redis中的用户 String key = LOGIN_USER_KEY + token; Map&lt;Object, Object&gt; userMap = stringRedisTemplate.opsForHash().entries(key); // 3.判断用户是否存在 if (userMap.isEmpty()) &#123; return true; &#125; // 5.将查询到的hash数据转为UserDTO UserDTO userDTO = BeanUtil.fillBeanWithMap(userMap, new UserDTO(), false); // 6.存在，保存用户信息到 ThreadLocal UserHolder.saveUser(userDTO); // 7.刷新token有效期 stringRedisTemplate.expire(key, LOGIN_USER_TTL, TimeUnit.MINUTES); // 8.放行 return true; &#125;&#125; 在这里做了获取用户信息并存入ThreadLocal中，并更新有效期 如果redis没有用户信息，那么会直接到下一个拦截器，不会存入ThreadLocal并更新token有效期 需要登录状态信息的模块的拦截器： 1234567891011121314public class LoginInterceptor implements HandlerInterceptor &#123; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; // 1.判断是否需要拦截（ThreadLocal中是否有用户） if (UserHolder.getUser() == null) &#123; // 没有，需要拦截，设置状态码 response.setStatus(401); // 拦截 return false; &#125; // 有用户，则放行 return true; &#125; 这里只需要判断ThreadLocal中是否有用户 SpringMvcConfig： 拦截器顺序的配置 1234567891011121314151617181920212223242526@Configurationpublic class MvcConfig implements WebMvcConfigurer &#123; @Resource private StringRedisTemplate stringRedisTemplate; @Override public void addInterceptors(InterceptorRegistry registry) &#123; // token刷新的拦截器 registry.addInterceptor(new RefreshTokenInterceptor(stringRedisTemplate)). addPathPatterns(&quot;/**&quot;).order(0); // 登录拦截器 registry.addInterceptor(new LoginInterceptor()) .excludePathPatterns( &quot;/shop/**&quot;, &quot;/voucher/**&quot;, &quot;/shop-type/**&quot;, &quot;/upload/**&quot;, &quot;/blog/hot&quot;, &quot;/user/code&quot;, &quot;/user/login&quot; ).order(1); // token刷新的拦截器 &#125;&#125; 每个拦截器中都有一个默认order&#x3D;0，如果没有手动修改，默认的顺序就是拦截器的配置顺序 可以手动修该order来配置拦截器顺序（从0开始，0优先级最高） 2.Redis缓存1）基本概述 redis作为一种高读写速度的数据库，非常适合做缓存 基本流程 首先查询redis，如果命中直接返回 未命中查询数据库，返回数据的同时还要将数据写入redis 缓存的好处： 降低后端负载 提高读写效率，降低响应时间 缓存的成本 数据一致性成本：保证缓存中数据和实际数据一致性 增加维护成本 运维成本 ​ 2）添加缓存 123456789101112131415161718192021@overridepublic Result queryById1(Long id)&#123; // 1.从redis查询商铺缓存 String key = &quot;cache : shop: &quot; + id; String shopJson = stringRedisTemplate.opsForValue().get(key);//2.判断是否存在 if (StrUtil.isNotBlank(shopJson)) &#123; // 3.存在，直接返回 Shop shop = JSONUtil.toBean(shopJson,Shop.class); return Result.ok(shop); &#125; // 4.不存在，根据id查询数据库 Shop shop = getById(id); // 5.不存在，返回错误 if (shop == null)&#123; return Result.fail(&quot;店铺不存在!&quot;); &#125; //6.存在，写入redis stringRedisTemplate.opsForValue().set(key,JSONUtil.toJsonStr(shop)); //7.返回 return Result.ok(shop);&#125; 首先查缓存，存在直接反序列化返回 没有则查数据库，不存在直接返回错误；存在序列化写入缓存并返回该shop 3）缓存更新①缓存更新策略 内存淘汰：redis本身的淘汰机制，在内存不足时自动淘汰 无法控制，且维护一致性很差 超时剔除：给缓存在redis的数据添加有效期，到期自动删除。删除后下次查询更新缓存 控制性一般 主动更新：编写逻辑代码，修改数据库的同时更新缓存 综上，主动更新策略是最有效的，我们主要选择主动更新，超时剔除作为辅助 ②主动更新方案 Cache Aside Pattern 人为编码，在更新数据库同时更新缓存 Read&#x2F;Write Through Patther 维护一个服务，我们在操作数据的时候并不知道是缓存数据还是数据库数据，由这个服务去维护一致性 Write Behind Caching Pattern 我们只操作缓存，由其它线程异步将缓存数据持久化到数据库 综上，后两者都有一定难度，对于我们来说选择自己编码的方式 ③实施细节操作缓存和数据库时有三个问题需要考虑： 1）删除缓存还是更新缓存？ 更新缓存：每次更新数据库都更新缓存，无效写操作较多，查询100次就会更新100次，但其实最后一次才是有效的 删除缓存：更新数据库时让缓存失效，查询时再更新缓存 2）如何保证缓存与数据库的操作的同时成功或失败？ 单体系统，将缓存与数据库操作放在一个事务 分布式系统，利用TCC等分布式事务方案 3）先操作缓存还是先操作数据库 先删除缓存，再操作数据库 先操作数据库，再删除缓存 先删除缓存，再操作数据库 线程1先删除缓存A&#x3D;10，线程2此时查询缓存未命中，查询数据库A&#x3D;10并写入缓存A&#x3D;10，线程1更新数据库A&#x3D;20 先操作数据库，再删除缓存 缓存由于一些问题没有了，线程1查询缓存未命中查询数据库A&#x3D;10，线程2此时更新数据库A&#x3D;20（缓存已经没了），删除缓存，并写入缓存， 两种方式都有问题，第一种线程2查询数据库和写缓存操作时间较少，很容易完成；但第二种线程2要更新数据库并删除缓存，这要的时间是较长的，不容易完成，所以选择第二种 并且还可以设置ttl时间来辅助 综上，选择先操作数据库再删除缓存 ④exampleShopService中的update操作： 12345678910111213@Override@Transactionalpublic Result update(Shop shop) &#123; Long id = shop.getId(); if (id == null) &#123; return Result.fail(&quot;店铺id不能为空&quot;); &#125; // 1.更新数据库 updateById(shop); // 2.删除缓存 stringRedisTemplate.delete(CACHE_SHOP_KEY + id); return Result.ok();&#125; 加入了事务保证原子性 4）缓存穿透①基本概念 缓存穿透指的是请求的数据在缓存和数据库中都不存在，这样缓存永远都不会命中，若有大量这样的请求会加剧数据库负担 主动解决 热点key限流 权限管理 设置复杂无规律id，让别人没办法猜 做好数据基础格式校验 主动屏蔽一些不可能的id如：0,-1等 被动解决 缓存空值 在请求数据库没有数据后，缓存空值并设置ttl 布隆过滤 布隆过滤是一种算法，会根据hash函数映射出是否存在对应数据，如果不存在直接不会请求缓存和数据库 可能误判 布隆说没有，一定没有 布隆说有，不一定有 ②缓存空值实现 shopService根据id查询店铺方法： 12345678910111213141516171819202122232425262728public Result queryById (Long id) &#123; // 1.从redis查询商铺缓存 String key = CACHE_SHOP_KEY + id; String shopJson = stringRedisTemplate.opsForValue().get(key); // 2.判断是否存在，blank：&quot;&quot;,null,&quot; &quot; 都算 if (StrUtil.isNotBlank(shopJson)) &#123; // 3.存在，直接返回 Shop shop = JSONUtil.toBean(shopJson, Shop.class); return Result.ok(shop); &#125; // 4.判断命中的是否是空值，如果不是null那就是&quot;&quot;空值 if (shopJson != null) &#123; //返回一个错误信息 return Result.fail(&quot;店铺信息不存在!&quot;); &#125; // 5.不存在，根据id查询数据库 Shop shop = getById(id); // 6.不存在，返回错误 if (shop == null) &#123; //将空值写入redis stringRedisTemplate.opsForValue().set(key, &quot;&quot;, CACHE_NULL_TTL, TimeUnit.MINUTES); //返回错误信息 return Result.fail(&quot;店铺不存在!&quot;); &#125; //7.存在，写入redis stringRedisTemplate.opsForValue().set(key, JSONUtil.toJsonStr(shop), CACHE_SHOP_TTL, TimeUnit.MINUTES); return Result.ok(shop);&#125; 查询缓存若是&quot;&quot;空值直接返回 查询数据库没数据写&quot;&quot;到缓存 ③布隆过滤5）缓存雪崩 缓存雪崩由两个原因造成： ttl同时失效，一瞬间大量缓存消失 redis宕机 这样会造成大量请求到达数据库，加剧数据库压力 解决办法： 给不同的Key的TTL添加随机值 可以通过在TTL后面加random值来实现 利用Redis集群提高服务的可用性 搭建redis集群，多台redis服务器，其中一台宕机，其余还可以工作 主机宕机，哨兵机制会从从机中选择一个继续工作 给缓存业务添加降级限流策略 微服务知识：如果流量过大，快速失败，减轻负担 给业务添加多级缓存 多级缓存：nginx缓存-&gt;redis缓存 这样就是两级缓存 redis这一级宕机了，还有其它级缓存 6）缓存击穿 ​ 缓存雪崩是大量key失效，缓存击穿是个别key失效（TTL到期），这些key是高并发访问并且缓存重建业务较复杂的热点key，这样会造成大量请求冲击在数据库上。 如上图： 线程1查询未命中，说明该热点key已经失效，那么需要查询数据库重新构建缓存 其它线程会重复线程1的操作，大量请求访问数据库，加剧数据库压力并且重复了重建缓存的操作，一直到线程1结束重建并写入缓存为止（红线），其它线程都会重复这些操作 解决办法： 互斥锁： 线程1缓存未命中后会获取互斥锁，然后开始重建缓存操作直到写入缓存，释放锁 线程2在未命中后也会尝试获取互斥锁，因为互斥锁已被线程1获取，这时候线程2就会重复：休眠等待，查询缓存，未命中尝试获取互斥锁这个操作，直到线程1写入缓存释放了锁，这时候线程2缓存命中 特点： 没有额外的内存消耗（不用存逻辑过期） 若重建缓存时间较长，这段时间其它线程都会被阻塞 牺牲了服务性，维护了数据一致性 逻辑过期 缓存穿透其实就是热点key的ttl失效问题，那我们就不为它设置ttl，而是设置一个逻辑过期时间 热点key的value额外会存储一个逻辑过期时间，逻辑过期时间过期说明该数据是旧数据，需要更新 线程1发现逻辑过期时间过期，会尝试获取互斥锁 获取到互斥锁后线程1不会自己来重建，而是开启一个新线程2来执行重建操作 线程2执行重建操作，在写入缓存后会释放锁 线程3查询缓存发现逻辑过期时间过期，尝试获取互斥锁，发现互斥锁已经被获取了，这时候线程3不会重复：休眠等待，查询缓存，未命中尝试获取互斥锁这个操作，而是直接返回缓存中旧的数据（逻辑过期的数据）。 特点： 因为存了逻辑过期时间，有了额外的内存消耗 服务性较好（返回旧数据），但存在数据一致性 ①互斥锁利用redis.se1tnx来实现互斥锁： 因为只有第一个setnx的线程能够成功返回1，这就相当于获得了锁 后来的线程再setnx返回的就是0，这就相当于没获得锁 锁操作的方法： 12345678private boolean tryLock(String key) &#123; Boolean flag = stringRedisTemplate.opsForValue().setIfAbsent(key, &quot;1&quot;, 10, TimeUnit.SECONDS); return BooleanUtil.isTrue(flag);&#125;private void unlock(String key) &#123; stringRedisTemplate.delete(key);&#125; 可以看到锁操作其实很简单： 上锁操作就是为传入的key值setnx 如果返回是true（这里spring data进行了封装，redis中是返回1），说明是第一个，相当于获得了锁 如果返回是false（这里spring data进行了封装，redis中是返回2），说明已经有人获取锁了 解锁操作就是将传入的key值delete，这样下一次setnx就能成功了 互斥锁实现方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public &lt;R, ID&gt; R queryWithMutex( String keyPrefix, ID id, Class&lt;R&gt; type, Function&lt;ID, R&gt; dbFallback, Long time, TimeUnit unit) &#123; String key = keyPrefix + id; // 1.从redis查询商铺缓存 String shopJson = stringRedisTemplate.opsForValue().get(key); // 2.判断是否存在，blank：&quot;&quot;,null,&quot; &quot;都算 if (StrUtil.isNotBlank(shopJson)) &#123; // 3.存在，直接返回 return JSONUtil.toBean(shopJson, type); &#125; // 判断命中的是否是空值，如果不是null那就是&quot;&quot;空值 if (shopJson != null) &#123; // 返回一个错误信息 return null; &#125; // 4.实现缓存重建 // 4.1.获取互斥锁 String lockKey = LOCK_SHOP_KEY + id; R r = null; try &#123; boolean isLock = tryLock(lockKey); // 4.2.判断是否获取成功 if (!isLock) &#123; // 4.3.获取锁失败，休眠并重试 Thread.sleep(50); return queryWithMutex(keyPrefix, id, type, dbFallback, time, unit); &#125; // 4.4.获取锁成功，根据id查询数据库 r = dbFallback.apply(id); // 5.不存在，返回错误 if (r == null) &#123; // 将空值写入redis stringRedisTemplate.opsForValue().set(key, &quot;&quot;, CACHE_NULL_TTL, TimeUnit.MINUTES); // 返回错误信息 return null; &#125; // 6.存在，写入redis this.set(key, r, time, unit); &#125; catch (InterruptedException e) &#123; throw new RuntimeException(e); &#125;finally &#123; // 7.释放锁 unlock(lockKey); &#125; // 8.返回 return r; &#125; 当从redis中获得的是null，说明没有该key了，此时需要重建缓存：boolean isLock &#x3D; tryLock(lockKey); false：说明已经有人获取过互斥锁了，重复 休眠一段时间+重查缓存+获取锁的 过程，这里是通过递归的方式进行的重复的 123456// 4.2.判断是否获取成功if (!isLock) &#123; // 4.3.获取锁失败，休眠并重试 Thread.sleep(50); return queryWithMutex(keyPrefix, id, type, dbFallback, time, unit);&#125; true：说明是第一个线程，成功获取互斥锁，下一步就是该线程重建缓存 上面的代码在数据库没查到的时候存入的””（存空值，防止缓存穿透） ②逻辑过期 存入redis添加逻辑过期： 逻辑过期实现方法： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758//定义的线程池private static final ExecutorService CACHE_REBUILD_EXECUTOR = Executors.newFixedThreadPool(10);public Shop querywithLogicalExpire(Long id) &#123; String key = CACHE_SHOP_KEY + id; // 1.从redis查询商铺缓存 String shopJson = stringRedisTemplate.opsForValue().get(key); //2.判断是否存在 if (StrUtil.isBlank(shopJson)) &#123; // 3.存在，直接返回 return null; &#125; // 4.命中，需要先把json反序列化为对象 RedisData redisData = JSONUtil.toBean(shopJson, RedisData.class); //因为redis中的data是Object，从redis查到后需要先强转(JSONObject)，然后再传入对应的.class反序列化 Shop shop = JSONUtil.toBean((JSONObject) redisData.getData(), Shop.class); LocalDateTime expireTime = redisData.getExpireTime(); // 5.判断是否过期 if (expireTime.isAfter(LocalDateTime.now())) &#123; // 5.1.未过期，直接返回店铺信总 return shop; &#125; // 5.2.已过期，需要缓存重建 //6.缓存重建 // 6.1.获取互斥锁 String lockKey =LOCK_SHOP_KEY + id ;boolean isLock = tryLock(lockKey) ; // 6.2.判断是否获取锁成功 if (isLock)&#123; //这里其实应该再次检查现在的数据是否过期 // 6.3.成功，从线程池中获取并开启独立线程，实现缓存重建 CACHE_REBUILD_EXECUTOR.submit(() -&gt; &#123; try&#123; //调用下面的方法，重建缓存 saveShop2Redis(id,expireSeconds,20L); &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125;finally&#123; //释放锁 this.unlock(lockKey) ; &#125; &#125;); &#125; // 6.4.返回过期的商铺信息 return shop;&#125; public void saveShop2Redis(Long id, Long expireSeconds) &#123; //1.从数据库中查询店铺数据 Shop shop = getById(id); //2.封装逻辑过期时间 RedisData redisData = new RedisData(); //data就是存入redis的数据 redisData.setData(shop); //加上逻辑过期时间 redisData.setExpireTime(LocalDateTime.now().plusSeconds(expireSeconds)); //3.写入Redis，手动序列化 stringRedisTemplate.opsForValue().set(CACHE_SHOP_KEY + id, JSONUtil.toJsonStr(redisData)); &#125; 线程1查询缓存发现过期，获取互斥锁成功，从线程池中获取一个线程并开启，实现重建缓存，线程1返回的也是旧数据（过期） 当获取锁成功后应该再次检查一下，目前的redis中的数据是否过期 因为可能出现刚构建完缓存释放锁，线程3这时候刚好查询完缓存发现过期并获取锁的情况 这时候cache中的数据已经重建了，但是线程2还是会执行线程重建操作 线程2查询缓存发现过期，获取互斥锁失败，直接返回旧的数据 7）封装缓存工具用到的工具类pom： 123456&lt;dependency&gt; &lt;groupId&gt;cn.hutool&lt;/groupId&gt; &lt;artifactId&gt;hutool-all&lt;/artifactId&gt; &lt;version&gt;5.7.17&lt;/version&gt;&lt;/dependency&gt; cacheClient工具类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273import cn.hutool.core.util.BooleanUtil;import cn.hutool.core.util.StrUtil;import cn.hutool.json.JSONObject;import cn.hutool.json.JSONUtil;import lombok.extern.slf4j.Slf4j;import org.springframework.data.redis.core.StringRedisTemplate;import org.springframework.stereotype.Component;import java.time.LocalDateTime;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.TimeUnit;import java.util.function.Function;@Slf4j@Componentpublic class CacheClient &#123; //缓存控制“”过期时间 public static final Long CACHE_NULL_TTL = 2L; //互斥锁的key的prefix public static final String LOCK_Mutex_KEY = &quot;lockMutex:&quot;; //操作redis，通过构造函数传入 private final StringRedisTemplate stringRedisTemplate; //自定义线程池，逻辑过期处理缓存击穿使用 private static final ExecutorService CACHE_REBUILD_EXECUTOR = Executors.newFixedThreadPool(10); //构造函数传入redis的操作对象 public CacheClient(StringRedisTemplate stringRedisTemplate) &#123; this.stringRedisTemplate = stringRedisTemplate; &#125; /** * 往redis存值，存储结构是key-value，并设置真实过期时间 * @param key 需要存入redis的key * @param value 需要存入redis的value * @param time 设置过期时间 * @param unit 时间单位 */ public void setWithRealExpire(String key, Object value, Long time, TimeUnit unit) &#123; stringRedisTemplate.opsForValue().set(key, JSONUtil.toJsonStr(value), time, unit); &#125; /** * 往redis存值，存储结构是key-value，并设置逻辑过期时间 * @param key 需要存入redis的key * @param value 需要存入redis的value * @param time 设置逻辑过期时间 * @param unit 时间单位 */ public void setWithLogicalExpire(String key, Object value, Long time, TimeUnit unit) &#123; // 将存入的数据进行封装，添加上逻辑过期时间: redisData=原本的数据 + 逻辑过期时间 RedisData redisData = new RedisData(); redisData.setData(value); redisData.setExpireTime(LocalDateTime.now().plusSeconds(unit.toSeconds(time))); // 手动序列话redisData并存入redis stringRedisTemplate.opsForValue().set(key, JSONUtil.toJsonStr(redisData)); &#125; /** * 获取互斥锁，锁就是redis中某特定key * @param key 互斥锁key * @return 是否获取成功，true:成功，false:失败 */ private boolean tryLock(String key) &#123; //通过setnx来为特定id设置，只有第1个能成功，返回1，spring data封装成了true //第2,3,4....来设置会失败，返回0，spring data封装成了false Boolean flag = stringRedisTemplate.opsForValue().setIfAbsent(key, &quot;1&quot;, 10, TimeUnit.SECONDS); return BooleanUtil.isTrue(flag); &#125; /** * 释放互斥锁，其实就是删除对应key数据，这样下一个新线程来修改就能setnx成功获得锁 * @param key 互斥锁key */ private void unlock(String key) &#123; stringRedisTemplate.delete(key); &#125; /** * 缓存&quot;&quot;空值——解决缓存穿透问题 * @param keyPrefix 缓存redis数据的前缀 * @param id 要查询的ID * @param type Result.class，用于反序列化 * @param dbFallback 函数式编程，在缓存查询失败后，根据id查询DB的方法:参数泛型是ID，返回值泛型是Result * @param time 过期时间 * @param unit 时间单位 * @param &lt;Result&gt; 返回值的泛型 * @param &lt;ID&gt; 要查询的ID的泛型：因为可能是int型id，也有可能是String型 * @return null：数据库和缓存中没有数据，错误查询 result：返回查询到的具体数据 */ public &lt;Result,ID&gt; Result queryWithPassThrough( String keyPrefix, ID id, Class&lt;Result&gt; type, Function&lt;ID, Result&gt; dbFallback, Long time, TimeUnit unit)&#123; // 1.拼接缓存的key String key = keyPrefix + id; // 2.根据拼接的key从redis查询商铺缓存 String json = stringRedisTemplate.opsForValue().get(key); // 3.判断是否存在，blank包括：null，“”，“ ” 这三种情况 if (StrUtil.isNotBlank(json)) &#123; // 3.1缓存命中且有真实数据，根据传入的Result.class反序列化，直接返回 return JSONUtil.toBean(json, type); &#125; // 3.2判断命中的是否是空值：不是null，说明是&quot;&quot;,&quot; &quot;这样的空值 if (json != null) &#123; // 返回空值说明没有数据，返回null（错误信息） return null; &#125; // 4.不存在，根据id查询数据库：函数式编程，传入了一段逻辑函数 Result result = dbFallback.apply(id); // 5.不存在，返回错误 if (result == null) &#123; // 将空值写入redis并设置空值的过期时间 stringRedisTemplate.opsForValue().set(key, &quot;&quot;, CACHE_NULL_TTL, TimeUnit.MINUTES); // 返回错误信息 return null; &#125; // 6.存在，写入redis并设置真实过期时间 this.setWithRealExpire(key, result, time, unit); return result; &#125; /** * 添加逻辑过期时间——解决缓存击穿问题（热点key） * @param keyPrefix 缓存redis数据的前缀 * @param id 要查询的ID * @param resultType Result.class，用于反序列化 * @param dbFallback 函数式编程，在缓存查询失败后，根据id查询DB的方法:参数泛型是ID，返回值泛型是Result * @param time 逻辑过期时间 * @param unit 时间单位 * @param &lt;Result&gt; 返回值的泛型 * @param &lt;ID&gt; 要查询的ID的泛型：因为可能是int型id，也有可能是String型 * @return null：数据库和缓存中没有数据，错误查询 result：返回查询到的具体数据 */ public &lt;Result, ID&gt; Result queryWithLogicalExpire( String keyPrefix, ID id, Class&lt;Result&gt; resultType, Function&lt;ID, Result&gt; dbFallback, Long time, TimeUnit unit) &#123; // 1.拼接缓存的key String key = keyPrefix + id; // 2.根据拼接的key从redis查询商铺缓存 String json = stringRedisTemplate.opsForValue().get(key); // 3.判断是否存在，blank包括：null，“”，“ ” 这三种情况 if (StrUtil.isBlank(json)) &#123; //3.1没有相关数据，直接返回不查数据库 return null; &#125; // 3.2命中，需要先把json反序列化为对象 //因为RedisData中封装的data是Object,从redis中查询出的是data是JSONObject，需要先强转 //强转成JSONObject后再根据传入的Result.class反序列化 RedisData redisData = JSONUtil.toBean(json, RedisData.class); Result result = JSONUtil.toBean((JSONObject) redisData.getData(), resultType); LocalDateTime expireTime = redisData.getExpireTime(); // 4.判断是否过期 if(expireTime.isAfter(LocalDateTime.now())) &#123; // 4.1.未过期，直接返回店铺信息 return result; &#125; // 4.2.已过期，需要缓存重建 // 5.缓存重建 // 5.1.根据拼接的id获取互斥锁 String lockKey = LOCK_Mutex_KEY +keyPrefix+ id; boolean isLock = tryLock(lockKey); // 5.2.判断是否获取锁成功，因为可能这时候已经有现成重建完毕并返回锁了，cache已经是最新数据了 if (isLock)&#123; //5.3这里还要重查一下redis中数据是否过期 String jsonCheck = stringRedisTemplate.opsForValue().get(key); if (StrUtil.isBlank(json)) &#123; //3.1没有相关数据，直接返回不查数据库 return null; &#125; // 3.2命中，需要先把json反序列化为对象 //因为RedisData中封装的data是Object,从redis中查询出的是data是JSONObject，需要先强转 //强转成JSONObject后再根据传入的Result.class反序列化 RedisData redisDataCheck = JSONUtil.toBean(json, RedisData.class); Result resultCheck = JSONUtil.toBean((JSONObject) redisData.getData(), resultType); LocalDateTime expireTimeCheck = redisData.getExpireTime(); // 4.判断是否过期 if(expireTime.isAfter(LocalDateTime.now())) &#123; // 4.1.未过期，直接返回店铺信息 return result; &#125; // 5.4.成功，开启独立线程，实现缓存重建 CACHE_REBUILD_EXECUTOR.submit(() -&gt; &#123; try &#123; // 查询数据库 Result newResult = dbFallback.apply(id); // 重建缓存,并设置逻辑过期时间 this.setWithLogicalExpire(key, newResult, time, unit); &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125;finally &#123; // 无论重建成功或失败都释放锁 unlock(lockKey); &#125; &#125;); &#125; // 6.4.无论获取成功还是失败，都返回过期的商铺信息 return result; &#125; /** * 添加逻辑过期时间——解决缓存击穿问题（热点key） * @param keyPrefix 缓存redis数据的前缀 * @param id 要查询的ID * @param resultType Result.class，用于反序列化 * @param dbFallback 函数式编程，在缓存查询失败后，根据id查询DB的方法:参数泛型是ID，返回值泛型是Result * @param time 真实过期时间 * @param unit 时间单位 * @param &lt;Result&gt; 返回值的泛型 * @param &lt;ID&gt; 要查询的ID的泛型：因为可能是int型id，也有可能是String型 * @return null：数据库和缓存中没有数据，错误查询 result：返回查询到的具体数据 */ public &lt;Result, ID&gt; Result queryWithMutex( String keyPrefix, ID id, Class&lt;Result&gt; resultType, Function&lt;ID, Result&gt; dbFallback, Long time, TimeUnit unit) &#123; // 1.拼接缓存的key String key = keyPrefix + id; // 2.根据拼接的key从redis查询商铺缓存 String json = stringRedisTemplate.opsForValue().get(key); // 3.判断是否存在，blank包括：null，“”，“ ” 这三种情况 if (StrUtil.isNotBlank(json)) &#123; // 3.1缓存命中且有真实数据，根据传入的Result.class反序列化，直接返回 return JSONUtil.toBean(json, resultType); &#125; // 3.2判断命中的是否是空值：不是null，说明是&quot;&quot;,&quot; &quot;这样的空值 if (json != null) &#123; // 返回空值说明没有数据，返回null（错误信息） return null; &#125; // 4.实现缓存重建 // 4.1.获取互斥锁 String lockKey = LOCK_Mutex_KEY + keyPrefix + id; Result result = null; try &#123; boolean isLock = tryLock(lockKey); // 4.2.判断是否获取成功 if (!isLock) &#123; // 4.3.获取锁失败，休眠并重试，递归方式重复 Thread.sleep(50); return queryWithMutex(keyPrefix, id, resultType, dbFallback, time, unit); &#125; // 4.4.获取锁成功，根据id查询数据库 result = dbFallback.apply(id); // 5.不存在，返回错误 if (result == null) &#123; // 将空值写入redis stringRedisTemplate.opsForValue().set(key, &quot;&quot;, CACHE_NULL_TTL, TimeUnit.MINUTES); // 返回错误信息 return null; &#125; // 6.存在，写入redis this.setWithRealExpire(key, result, time, unit); &#125; catch (InterruptedException e) &#123; throw new RuntimeException(e); &#125;finally &#123; // 7.释放锁 unlock(lockKey); &#125; // 8.重建过后返回新数据 return result; &#125;&#125; 三、分布式缓存1.Redis持久化1）RDBRDB全称Redis Database Backup file（Redis数据备份文件），也被叫做Redis数据快照。简单来说就是把内存中的所有数据都记录到磁盘中。当Redis实例故障重启后，从磁盘读取快照文件，恢复数据。快照文件称为RDB文件，默认是保存在当前运行目录。 ①执行时机RDB持久化在四种情况下会执行： 执行save命令 执行bgsave命令 Redis停机时 触发RDB条件时 1）save命令 执行下面的命令，可以立即执行一次RDB： save命令会导致主进程执行RDB，这个过程中其它所有命令都会被阻塞。只有在数据迁移时可能用到。 2）bgsave命令 下面的命令可以异步执行RDB： 这个命令执行后会开启独立进程完成RDB，主进程可以持续处理用户请求，不受影响。 3）停机时 Redis停机时会执行一次save命令，实现RDB持久化 因为决定要停机了，那么主线程被阻塞了也没关系 4）触发RDB条件 Redis内部有触发RDB的机制，可以在redis.conf文件中找到，格式如下： 1234# 900秒内，如果至少有1个key被修改，则执行bgsave ， 如果是save &quot;&quot; 则表示禁用RDBsave 900 1 save 300 10 save 60 10000 注意满足条件以后，是执行bgsave RDB的其它配置也可以在redis.conf文件中设置： 12345678# 是否压缩 ,建议不开启，压缩也会消耗cpu，磁盘的话不值钱rdbcompression yes# RDB文件名称dbfilename dump.rdb # 文件保存的路径目录dir ./ ②RDB原理bgsave开始时会fork主进程得到子进程，子进程共享主进程的内存数据。完成fork后读取内存数据并写入 RDB 文件。 fork采用的是copy-on-write技术： 当主进程执行读操作时，访问共享内存； 当主进程执行写操作时，则会拷贝一份数据，执行写操作 主进程的页表记录进程中的一页与内存中页框的对应关系，也就是具体的数据的位置 fork就是子进程直接复制了该页表，然后可以通过页表直接找到内存中数据的对应位置 fork采用的是copy-on-write技术 fork会将共享内存设置为read-only只读模式，这就意味着所有线程都只能读不能写 当主线程接收到了写的请求，那么它会将要写的数据进行复制，对复制的数据做修改 而当主线程接收到了读请求，也会读取这个复制的数据 考虑一个极端情况，如果子进程进行RDB的时间过长，这段时间有大量的写请求 那么所有数据都会进行复制，这样redis就会占用2倍内存，原来是4g，现在就是8g了 因此服务器需要预留一些内存，32g不能都交给redis，否则RDB的时候就可能发生内存溢出 RDB结束后会替换掉旧的RDB文件 ③RDB间隔 不能过长，否则出现宕机，丢失的数据就太多了 不能太短，比如1s，1次修改就进行RDB，这样重写RDB的时间都不够 ④缺点 RDB执行间隔时间长，两次RDB之间写入数据有丢失的风险 期间宕机了，会丢失大量数据 fork子进程、压缩、写出RDB文件都比较耗时 2）AOF①AOF原理AOF全称为Append Only File（追加文件）。Redis处理的每一个写命令都会记录在AOF文件，可以看做是命令日志文件。 ②AOF配置AOF默认是关闭的，需要修改redis.conf配置文件来开启AOF： 1234# 是否开启AOF功能，默认是noappendonly yes# AOF文件的名称appendfilename &quot;appendonly.aof&quot; AOF的命令记录的频率也可以通过redis.conf文件来配： 123456# 表示每执行一次写命令，立即记录到AOF文件appendfsync always # 写命令执行完先放入AOF缓冲区，然后表示每隔1秒通过另外的线程将缓冲区数据写到AOF文件，是默认方案appendfsync everysec # 写命令执行完先放入AOF缓冲区，由操作系统决定何时将缓冲区内容写回磁盘appendfsync no 三种策略对比： ③AOF文件重写​ 因为是记录命令，AOF文件会比RDB文件大的多。而且AOF会记录对同一个key的多次写操作，但只有最后一次写操作才有意义。通过执行bgrewriteaof命令，可以让AOF文件执行重写功能（后台另开一个线程），用最少的命令达到相同效果。 ​ ​ ​ 如图，AOF原本有三个命令，但是set num 123 和 set num 666都是对num的操作，第二次会覆盖第一次的值，因此第一个命令记录下来没有意义。 所以重写命令后，AOF文件内容就是：mset name jack num 666 Redis也会在触发阈值时自动去重写AOF文件。阈值也可以在redis.conf中配置： 1234# AOF文件比上次文件 增长超过多少百分比则触发重写，默认也就是说增长1倍的时候重写AOFauto-aof-rewrite-percentage 100# AOF文件体积最小多大以上才触发重写 auto-aof-rewrite-min-size 64mb 3）RDB与AOF对比RDB和AOF各有自己的优缺点，如果对数据安全性要求较高，在实际开发中往往会结合两者来使用。 RDB： 只存数据，并且支持压缩，所以文件大小较小 只存数据，所以开机恢复速度很快 要进行fork操作，另开子线程，要浪费CPU和内存 两次备份间隔数据容易丢失，数据完整性不如AOF RDB优先级低于AOF AOF： 存储的是指令，文件大小较大 存储的是指令，开机恢复的耗时更长 主要是IO操作，但是重写要另开线程执行，也会消耗CPU和内存 数据完整性取决于刷盘策略，完整性较RDB更高 2.Redis主从1）基本概念单节点Redis的并发能力是有上限的，要进一步提高Redis的并发能力，就需要搭建主从集群，实现读写分离： Redis大部分都是读多写少，所以主从可以做以下分工 主节点负责写请求 从节点负责读请求 从节点通过 slaveof [ip] [port] 来连接主节点 2）全量同步①基本原理 主从同步涉及以下两个概念： Replication Id：简称replid，是数据集的标记，id一致则说明是同一数据集。每一个master都有唯一的replid，slave则会继承master节点的replid offset：偏移量，随着记录在repl_baklog中的数据增多而逐渐增大。slave完成同步时也会记录当前同步的offset。如果slave的offset小于master的offset，说明slave数据落后于master，需要更新。 slave做数据同步，必须向master声明自己的replication id 和offset，master才可以判断到底需要同步哪些数据。 ​ ​ 因为slave原本也是一个master，有自己的replid和offset，当第一次变成slave，与master建立连接时，发送的replid和offset是自己的replid和offset。master判断发现slave发送来的replid与自己的不一致，说明这是一个全新的slave，就知道要做全量同步了。 master会将自己的replid和offset都发送给这个slave，slave保存这些信息。以后slave的replid就与master一致了。 ​ 因此，master判断一个节点是否是第一次同步的依据，就是看replid是否一致。 ②完整流程第一阶段： 从节点执行replicof命令向主节点发送自己的repl_id和offset尝试增量同步 主节点发现发送来的repl_id和自己的不一样，说明该从节点是第一次做同步，所以不能做增量同步，要做全量同步 主节点向从节点发送自己的repl_id和offset，从节点保存版本信息 第二阶段： 主节点执行bgsave，开启单独的线程来生成RDB，并发送给从节点 在主节点RDB的过程中，还会接收写请求，这部分数据以指令的形式存在repl_baklog缓存区中（类似AOF记录命令） 从节点接收主节点的RDB，清空本地数据，加载RDB数据 因为还有rep_baklog的数据，所以目前同步的只是大部分数据 第三阶段： 主节点向从节点发送rep_baklog，从节点执行rep_baklog的命令，完成同步 3）增量同步①完整流程​ 全量同步需要先做RDB，然后将RDB文件通过网络传输个slave，成本太高了。因此除了第一次做全量同步，其它大多数时候slave与master都是做增量同步，就是只更新slave与master存在差异的部分数据。如图： 第一阶段： slave发送repl_id和offset请求增量同步 master发现repl_id相同，说明不是第一次同步，那么继续 第二阶段： master根据slave发送来的offset，发送在rep_baklog中offset后面的命令 slave执行命令 ②repl_backlog原理master怎么知道slave与自己的数据差异在哪里呢? 这就要说到全量同步时的repl_baklog文件了。 这个文件是一个固定大小的数组，只不过数组是环形，也就是说角标到达数组末尾后，会再次从0开始读写，这样数组头部的数据就会被覆盖，类似一个环形链表。 repl_baklog中会记录Redis处理过的命令日志及offset，包括master当前的offset，和slave已经拷贝到的offset： slave与master的offset之间的差异，就是salve需要增量拷贝的数据了。 随着不断有数据写入，master的offset逐渐变大，slave也不断的拷贝，追赶master的offset： 直到数组被填满： 此时，如果有新的数据写入，就会覆盖数组中的旧数据。不过，旧的数据只要是绿色的，说明是已经被同步到slave的数据，即便被覆盖了也没什么影响。因为未同步的仅仅是红色部分。 但还有特殊情况： 如果slave出现网络阻塞，导致master的offset远远超过了slave的offset： 如果master继续写入新数据，其offset就会覆盖旧的数据，直到将slave现在的offset也覆盖： ​ 这种情况就是master已经超过slave的offset已经一圈以上了 ​ 棕色框中的红色部分，就是尚未同步，但是却已经被覆盖的数据。此时如果slave恢复，需要同步，却发现自己的offset都没有了，无法完成增量同步了。只能做全量同步。 4）主从同步优化主从从架构图： 主从同步可以保证主从数据的一致性，非常重要。 可以从以下几个方面来优化Redis主从就集群： 在master中配置repl-diskless-sync yes启用无磁盘复制，避免全量同步时的磁盘IO。 省去将RDB写入磁盘过程，直接通过网络进行传输给从节点 可能导致网络拥塞，在网络带宽大、网络情况好的情况下使用，默认是关闭的 Redis单节点上的内存占用不要太大，减少RDB导致的过多磁盘IO 8g内存不要让redis占用全部，使Redis整体的数据不要太大，减少RDB大小 适当提高repl_baklog的大小，发现slave宕机时尽快实现故障恢复，尽可能避免全量同步 这样能够减小master超过slave的offset一圈的情况 限制一个master上的slave节点数量，如果实在是太多slave，则可以采用主-从-从链式结构，减少master压力 主节点给2个slave做同步了后，2个slave再继续给另外的slave同步 3.Redis哨兵1）集群结构和作用 哨兵的作用如下： 监控：Sentinel 会不断检查您的master和slave是否按预期工作 自动故障恢复：如果master故障，Sentinel会将一个slave提升为master。当故障实例恢复后也以新的master为主 通知：Sentinel充当Redis客户端的服务发现来源，当集群发生故障转移时，会将最新信息推送给Redis的客户端 ①集群监控原理Sentinel基于心跳机制监测服务状态，每隔1秒向集群的每个实例发送ping命令： 主观下线：如果某sentinel节点发现某实例未在规定时间响应，则认为该实例主观下线。 客观下线：若超过指定数量（quorum）的sentinel都认为该实例主观下线，则该实例客观下线。quorum值最好超过Sentinel实例数量的一半。 ②集群故障恢复原理一旦发现master故障，sentinel需要在salve中选择一个作为新的master，选择依据是这样的： 首先会判断slave节点与master节点断开时间长短，如果超过指定值（down-after-milliseconds * 10）则会排除该slave节点 slave和master长时间断开，那么数据也不是新的，且slave也可能宕机了 然后判断slave节点的slave-priority值，越小优先级越高，如果是0则永不参与选举 默认值都是1，也就是priority是相同的，这个可以配置 如果slave-prority一样，则判断slave节点的offset值，越大说明数据越新，优先级越高 选择出一个与master节点差距最小的节点作为新节点 最后是判断slave节点的运行id大小，越小优先级越高。 run_id就是slave启动的时的id，到这一步其实就是随便选一个了 当选出一个新的master后，该如何实现切换呢？ 注意：sentinel之间也会选一个老大，让这个老大去执行通知和切换主节点的操作，就是最先发现问题的sentinel 流程如下： sentinel给备选的slave1节点发送slaveof no one命令，让该节点成为master（加入该节点是7002） sentinel给所有其它slave发送slaveof 192.168.150.101 7002 命令，让这些slave成为新master的从节点，开始从新的master上同步数据。 最后，sentinel将故障节点标记为slave，当故障节点恢复后会自动成为新的master的slave节点（7001） ③小结Sentinel的三个作用是什么？ 监控 故障转移 通知 Sentinel如何判断一个redis实例是否健康？ 每隔1秒发送一次ping命令，如果超过一定时间没有相向则认为是主观下线 如果大多数sentinel都认为实例主观下线，则判定服务下线（一般是超过半数） 故障转移步骤有哪些？ 首先选定一个slave作为新的master，执行slaveof no one 然后让所有节点都执行slaveof 新master 修改故障节点配置，添加slaveof 新master 2）RedisTemplate配置①引入依赖在项目的pom文件中引入依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; ②配置Redis地址然后在配置文件application.yml中指定redis的sentinel相关信息： 12345678spring: redis: sentinel: master: mymaster #这是给master取名字 nodes: - 192.168.150.101:27001 - 192.168.150.101:27002 - 192.168.150.101:27003 其实就是将所有的sentinel节点都配置上 java客户端其实是不知道具体的redis节点在哪儿，而是通过sentinel来获得的 ③配置读写分离在项目的启动类中，添加一个新的bean： 1234@Beanpublic LettuceClientConfigurationBuilderCustomizer clientConfigurationBuilderCustomizer()&#123; return clientConfigurationBuilder -&gt; clientConfigurationBuilder.readFrom(ReadFrom.REPLICA_PREFERRED);&#125; 这个bean中配置的就是读写策略，包括四种： MASTER：从主节点读取 MASTER_PREFERRED：优先从master节点读取，master不可用才读取replica REPLICA：从slave（replica）节点读取 REPLICA _PREFERRED：优先从slave（replica）节点读取，所有的slave都不可用才读取master 这是最推荐的 4.Redis分片集群1）搭建分片集群主从和哨兵可以解决高可用、高并发读的问题。但是依然有两个问题没有解决： 海量数据存储问题 高并发写的问题 使用分片集群可以解决上述问题，如图: ‘ 分片集群特征： 集群中有多个master，每个master保存不同数据 也就是说m1掌控0-50个slots的资源，m2掌控51-100个slots的资源 每个master都可以有多个slave节点 m1接收50个slots的资源，slaves就负责这50个slots的读操作 master之间通过ping监测彼此健康状态 这也就是实现了sentienl的机制 客户端请求可以访问集群任意节点，最终都会被转发到正确节点 访问到每个结点，每个结点都会根据这个key的有效值来计算，重新去找应该有的节点 2）散列插槽①基本概念Redis会把每一个master节点映射到0~16383共16384个插槽（hash slot）上，查看集群信息时就能看到： 数据key不是与节点绑定，而是与插槽绑定。redis会根据key的有效部分计算插槽值，分两种情况： key中包含”{}”，且“{}”中至少包含1个字符，“{}”中的部分是有效部分 key中不包含“{}”，整个key都是有效部分 ②example​ key是num，那么就根据num计算，如果是{itcast}num，则根据itcast计算。计算方式是利用CRC16算法得到一个hash值，然后对16384取余，得到的结果就是slot值。 ​ 如图，在7001这个节点执行set a 1时，对a做hash运算，对16384取余，得到的结果是15495，因此要存储到7003节点。 到了7003后，执行get num时，对num做hash运算，对16384取余，得到的结果是2765，因此需要切换到7001节点 ③小结 Redis如何判断某个key应该在哪个实例？ 将16384个插槽分配到不同的实例 根据key的有效部分计算哈希值，对16384取余 余数作为插槽，寻找插槽所在实例即可 如何将同一类数据固定的保存在同一个Redis实例？ 这一类数据使用相同的有效部分，例如key都以{typeId}为前缀 为什么key不和主节点绑定而是要和slots绑定 redis可能出现主节点宕机，集群伸缩时删除节点的情况，这时候如果key和节点绑定，那么上面的数据就丢失了 如果和slots绑定，我们就可以将这些slots转移到新的节点上 数据跟着slots走，我们可以很容易地进行数据转移，找到对应数据的位置 3）集群伸缩①相关命令 redis-cli –cluster提供了很多操作集群的命令，可以通过下面方式查看： 注意一定要–cluster，这样才是集群操作 比如，添加节点的命令： ②需求分析需求：向集群中添加一个新的master节点，并向其中存储 num &#x3D; 10 启动一个新的redis实例，端口为7004 添加7004到之前的集群，并作为一个master节点 给7004节点分配插槽，使得num这个key可以存储到7004实例 这里需要两个新的功能： 添加一个节点到集群中 将部分插槽分配到新插槽 将num这个key对应的slots分配到7004实例 ③创建新的redis实例创建一个文件夹： 1mkdir 7004 拷贝配置文件： 1cp redis.conf /7004 修改配置文件： 1sed /s/6379/7004/g 7004/redis.conf 启动 1redis-server 7004/redis.conf ④添加新节点到redis添加节点的语法如下： 执行命令： 1redis-cli --cluster add-node 192.168.150.101:7004 192.168.150.101:7001 注意要加上–cluster 通过命令查看集群状态： 1redis-cli -p 7001 cluster nodes 如图，7004加入了集群，并且默认是一个master节点： 但是，可以看到7004节点的插槽数量为0，因此没有任何数据可以存储到7004上 slots还是分配在7001,7002,7003三个节点上，我们需要重新分配key&#x3D;num对应的slots到7004 ⑤转移插槽我们要将num存储到7004节点，因此需要先看看num的插槽是多少： 如上图所示，num的插槽为2765. 我们可以将0~3000的插槽从7001转移到7004，命令格式如下： 具体流程如下： 建立连接： 随便连接一个主节点，因为集群操作，操作哪一个都是一样的 得到下面的反馈： 询问要移动多少个slots 因为key&#x3D;num对应的是2700左右，我们计划是移动7001的前3000个： 新的问题来了： 那个node来接收这些插槽？？ 显然是7004，7004节点的id可以如图： 复制这个id，然后拷贝到刚才的控制台后： 这里询问，你的插槽是从哪里移动过来的？ all：代表全部，也就是三个节点各转移一部分 具体的id：目标节点的id done：没有了 这里我们要从7001获取，因此填写7001的id： 填完后，点击done，这样插槽转移就准备好了： 确认要转移吗？输入yes： 然后，通过命令查看结果： 可以看到： 完成了需求 4）故障转移①自动故障转移当集群中有一个master宕机会发生什么呢？ 直接停止一个redis实例，例如7002： 1redis-cli -p 7002 shutdown 1）首先是该实例与其它实例失去连接 2）然后是疑似宕机： 3）最后是确定下线，自动提升一个slave为新的master： 4）当7002再次启动，就会变为一个slave节点了： ②手动故障转移利用cluster failover命令可以手动让集群中的某个master宕机，切换到执行cluster failover命令的这个slave节点，实现无感知的数据迁移。其流程如下： 这种failover命令可以指定三种模式： 缺省：默认的流程，如图1~6歩 force：省略了对offset的一致性校验 takeover：直接执行第5歩，忽略数据一致性、忽略master状态和其它master的意见 ③example案例需求： 在7002这个slave节点执行手动故障转移，重新夺回master地位 步骤如下： 1）利用redis-cli连接7002这个节点，执行cluster failover命令 ​ 2）结果 5）RedisTemplate访问分片集群RedisTemplate底层同样基于lettuce实现了分片集群的支持，而使用的步骤与哨兵模式基本一致： ①引入redis的starter依赖在项目的pom文件中引入依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; ②配置分片集群地址与哨兵模式相比，其中只有分片集群的配置方式略有差异，如下： 12345678910spring: redis: cluster: nodes: - 192.168.150.101:7001 - 192.168.150.101:7002 - 192.168.150.101:7003 - 192.168.150.101:8001 - 192.168.150.101:8002 - 192.168.150.101:8003 注意是配置cluster的所有节点，主从都要配置上 以下是哨兵模式： 在配置文件application.yml中指定redis的sentinel相关信息： 12345678spring: redis: sentinel: master: mymaster #这是给master取名字 nodes: - 192.168.150.101:27001 - 192.168.150.101:27002 - 192.168.150.101:27003 其实就是将所有的sentinel节点都配置上 java客户端其实是不知道具体的redis节点在哪儿，而是通过sentinel来获得的 ③配置读写分离在项目的启动类中，添加一个新的bean： 1234@Beanpublic LettuceClientConfigurationBuilderCustomizer clientConfigurationBuilderCustomizer()&#123; return clientConfigurationBuilder -&gt; clientConfigurationBuilder.readFrom(ReadFrom.REPLICA_PREFERRED);&#125; 这个bean中配置的就是读写策略，包括四种： MASTER：从主节点读取 MASTER_PREFERRED：优先从master节点读取，master不可用才读取replica REPLICA：从slave（replica）节点读取 REPLICA _PREFERRED：优先从slave（replica）节点读取，所有的slave都不可用才读取master 这是最推荐的 四、多级缓存1.基本概念传统的缓存策略一般是请求到达Tomcat后，先查询Redis，如果未命中则查询数据库，如图： 存在下面的问题： 请求要经过Tomcat处理，Tomcat的性能成为整个系统的瓶颈 Redis缓存失效时，会对数据库产生冲击 多级缓存就是充分利用请求处理的每个环节，分别添加缓存，减轻Tomcat压力，提升服务性能： 浏览器访问静态资源时，优先读取浏览器本地缓存 游览器会存储大部分静态资源，访问的 访问非静态资源（ajax查询数据）时，访问服务端 请求到达Nginx后，优先读取Nginx本地缓存 如果Nginx本地缓存未命中，则去直接查询Redis（不经过Tomcat） 基于nginx，使用lua语言去查询redis 如果Redis查询未命中，则查询Tomcat 请求进入Tomcat后，优先查询JVM进程缓存 caffeine 如果JVM进程缓存未命中，则查询数据库 在多级缓存架构中，Nginx内部需要编写本地缓存查询、Redis查询、Tomcat查询的业务逻辑，因此这样的nginx服务不再是一个反向代理服务器，而是一个编写业务的Web服务器了（等同于Tomact，使用lua语言操作redis）。 因此这样的业务Nginx服务也需要搭建集群来提高并发，再有专门的nginx服务来做反向代理，如图： 另外，我们的Tomcat服务将来也会部署为集群模式： 可见，多级缓存的关键有两个： 一个是在nginx中编写业务，实现nginx本地缓存、Redis、Tomcat的查询 另一个就是在Tomcat中实现JVM进程缓存 其中Nginx编程则会用到OpenResty框架结合Lua这样的语言。 2.JVM进程缓存1）Caffeine①基本概念缓存在日常开发中启动至关重要的作用，由于是存储在内存中，数据的读取速度是非常快的，能大量减少对数据库的访问，减少数据库的压力。我们把缓存分为两类： 分布式缓存，例如Redis： 优点：存储容量更大、可靠性更好、可以在集群间共享 缺点：访问缓存有网络开销 场景：缓存数据量较大、可靠性要求较高、需要在集群间共享 进程本地缓存，例如HashMap、GuavaCache： 优点：读取本地内存，没有网络开销，速度更快 缺点：存储容量有限、可靠性较低、无法共享 场景：性能要求较高，缓存数据量较小 利用Caffeine框架来实现JVM进程缓存。 Caffeine是一个基于Java8开发的，提供了近乎最佳命中率的高性能的本地缓存库。目前Spring内部的缓存使用的就是Caffeine。GitHub地址：https://github.com/ben-manes/caffeine Caffeine的性能非常好，下图是官方给出的性能对比： 可以看到Caffeine的性能遥遥领先！ ②API及使用缓存使用的基本API： 12345678910111213141516171819202122@Testvoid testBasicOps() &#123; // 构建cache对象 Cache&lt;String, String&gt; cache = Caffeine.newBuilder().build(); // 存数据 cache.put(&quot;gf&quot;, &quot;迪丽热巴&quot;); // 取数据 String gf = cache.getIfPresent(&quot;gf&quot;); System.out.println(&quot;gf = &quot; + gf); // 取数据，包含两个参数： // 参数一：缓存的key // 参数二：Lambda表达式，表达式参数就是缓存的key，方法体是查询数据库的逻辑 // 优先根据key查询JVM缓存，如果未命中，则执行参数二的Lambda表达式 String defaultGF = cache.get(&quot;defaultGF&quot;, key -&gt; &#123; // 根据key去数据库查询数据 return &quot;柳岩&quot;; &#125;); System.out.println(&quot;defaultGF = &quot; + defaultGF);&#125; Caffeine既然是缓存的一种，肯定需要有缓存的清除策略，不然的话内存总会有耗尽的时候。 Caffeine提供了三种缓存驱逐策略： 基于容量：设置缓存的数量上限 1234// 创建缓存对象Cache&lt;String, String&gt; cache = Caffeine.newBuilder() .maximumSize(1) // 设置缓存大小上限为 1 .build(); 基于时间：设置缓存的有效时间 123456// 创建缓存对象Cache&lt;String, String&gt; cache = Caffeine.newBuilder() // 设置缓存有效期为 10 秒，从最后一次写入开始计时 .expireAfterWrite(Duration.ofSeconds(10)) .build(); 基于引用：设置缓存为软引用或弱引用，利用GC来回收缓存数据。性能较差，不建议使用。 注意：在默认情况下，当一个缓存元素过期的时候，Caffeine不会自动立即将其清理和驱逐。而是在一次读或写操作后，或者在空闲时间完成对失效数据的驱逐。 2）实现JVM进程缓存①需求利用Caffeine实现下列需求： 给根据id查询商品的业务添加缓存，缓存未命中时查询数据库 给根据id查询商品库存的业务添加缓存，缓存未命中时查询数据库 缓存初始大小为100 缓存上限为10000 ②实现首先，我们需要定义两个Caffeine的缓存对象，分别保存商品、库存的缓存数据。 在item-service的com.heima.item.config包下定义CaffeineConfig类： 12345678910111213141516171819202122232425262728package com.heima.item.config;import com.github.benmanes.caffeine.cache.Cache;import com.github.benmanes.caffeine.cache.Caffeine;import com.heima.item.pojo.Item;import com.heima.item.pojo.ItemStock;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;@Configurationpublic class CaffeineConfig &#123; @Bean public Cache&lt;Long, Item&gt; itemCache()&#123;//查询商品信息 return Caffeine.newBuilder() .initialCapacity(100)//初始大小 .maximumSize(10_000)//最大值 .build(); &#125; @Bean public Cache&lt;Long, ItemStock&gt; stockCache()&#123;//查询商品库存 return Caffeine.newBuilder() .initialCapacity(100) .maximumSize(10_000) .build(); &#125;&#125; 添加缓存逻辑： 123456789101112131415161718192021222324252627282930@RestController@RequestMapping(&quot;item&quot;)public class ItemController &#123; @Autowired private IItemService itemService; @Autowired private IItemStockService stockService; @Autowired private Cache&lt;Long, Item&gt; itemCache; @Autowired private Cache&lt;Long, ItemStock&gt; stockCache; // ...其它略 @GetMapping(&quot;/&#123;id&#125;&quot;) public Item findById(@PathVariable(&quot;id&quot;) Long id) &#123; //第一个参数是缓存的key，第二个参数是没有查到缓存后要执行的function return itemCache.get(id, key -&gt; itemService.query() .ne(&quot;status&quot;, 3).eq(&quot;id&quot;, key) .one() ); &#125; @GetMapping(&quot;/stock/&#123;id&#125;&quot;) public ItemStock findStockById(@PathVariable(&quot;id&quot;) Long id) &#123; return stockCache.get(id, key -&gt; stockService.getById(key)); &#125;&#125; 3.Redis预热Redis缓存会面临冷启动问题： 冷启动：服务刚刚启动时，Redis中并没有缓存，如果所有商品数据都在第一次查询时添加缓存，可能会给数据库带来较大压力。 缓存预热：在实际开发中，我们可以利用大数据统计用户访问的热点数据，在项目启动时将这些热点数据提前查询并保存到Redis中。 我们数据量较少，并且没有数据统计相关功能，目前可以在启动时将所有数据都放入缓存中。 1）利用Docker安装Redis 1docker run --name redis -p 6379:6379 -d redis redis-server --appendonly yes 2）在item-service服务中引入Redis依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 3）配置Redis地址 123spring: redis: host: 192.168.150.101 4）编写初始化类 缓存预热需要在项目启动时完成，并且必须是拿到RedisTemplate之后。 这里我们利用InitializingBean接口来实现，因为InitializingBean可以在对象被Spring创建并且成员变量全部注入后执行。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package com.heima.item.config;import com.fasterxml.jackson.core.JsonProcessingException;import com.fasterxml.jackson.databind.ObjectMapper;import com.heima.item.pojo.Item;import com.heima.item.pojo.ItemStock;import com.heima.item.service.IItemService;import com.heima.item.service.IItemStockService;import org.springframework.beans.factory.InitializingBean;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.data.redis.core.StringRedisTemplate;import org.springframework.stereotype.Component;import java.util.List;@Componentpublic class RedisHandler implements InitializingBean &#123; @Autowired private StringRedisTemplate redisTemplate; @Autowired private IItemService itemService; @Autowired private IItemStockService stockService; private static final ObjectMapper MAPPER = new ObjectMapper(); @Override public void afterPropertiesSet() throws Exception &#123; // 初始化缓存 // 1.从mysql中查询商品信息 List&lt;Item&gt; itemList = itemService.list(); // 2.放入缓存 for (Item item : itemList) &#123; // 2.1.item序列化为JSON String json = MAPPER.writeValueAsString(item); // 2.2.存入redis redisTemplate.opsForValue().set(&quot;item:id:&quot; + item.getId(), json); &#125; // 3.从mysql中查询商品库存信息 List&lt;ItemStock&gt; stockList = stockService.list(); // 4.放入缓存 for (ItemStock stock : stockList) &#123; // 2.1.item序列化为JSON String json = MAPPER.writeValueAsString(stock); // 2.2.存入redis redisTemplate.opsForValue().set(&quot;item:stock:id:&quot; + stock.getId(), json); &#125; &#125;&#125; 4.缓存同步1）基本策略缓存数据同步的常见方式有三种： 设置有效期：给缓存设置有效期，到期后自动删除。再次查询时更新 优势：简单、方便 缺点：时效性差，缓存过期之前可能不一致 场景：更新频率较低，时效性要求低的业务 同步双写：在修改数据库的同时，直接修改缓存 优势：时效性强，缓存与数据库强一致 缺点：有代码侵入，耦合度高； 场景：对一致性、时效性要求较高的缓存数据 异步通知：修改数据库时发送事件通知，相关服务监听到通知后修改缓存数据 优势：低耦合，可以同时通知多个缓存服务 缺点：时效性一般，可能存在中间不一致状态 场景：时效性要求一般，有多个服务需要同步 而异步实现又可以基于MQ或者Canal来实现： ①基于MQ的异步通知： 解读： 商品服务完成对数据的修改后，只需要发送一条消息到MQ中。 缓存服务监听MQ消息，然后完成对缓存的更新 依然有少量的代码侵入 ②基于Canal的通知 解读： 商品服务完成商品修改后，业务直接结束，没有任何代码侵入 Canal监听MySQL变化，当发现变化后，立即通知缓存服务 缓存服务接收到canal通知，更新缓存 代码零侵入 2）认识Canal**Canal [kə’næl]**，译意为水道&#x2F;管道&#x2F;沟渠，canal是阿里巴巴旗下的一款开源项目，基于Java开发。基于数据库增量日志解析，提供增量数据订阅&amp;消费。GitHub的地址：https://github.com/alibaba/canal Canal是基于mysql的主从同步来实现的，MySQL主从同步的原理如下： 1）MySQL master 将数据变更写入二进制日志( binary log），其中记录的数据叫做binary log events 2）MySQL slave 将 master 的 binary log events拷贝到它的中继日志(relay log) 3）MySQL slave 重放 relay log 中事件，将数据变更反映它自己的数据 而Canal就是把自己伪装成MySQL的一个slave节点，从而监听master的binary log变化。再把得到的变化信息通知给Canal的客户端，进而完成对其它数据库的同步。 我们可以利用Canal提供的Java客户端，监听Canal通知消息。当收到变化的消息时，完成对缓存的更新。 这里使用GitHub上的第三方开源的canal-starter客户端。地址：https://github.com/NormanGyllenhaal/canal-client 与SpringBoot完美整合，自动装配，比官方客户端要简单好用很多。 3）具体流程①引入依赖12345&lt;dependency&gt; &lt;groupId&gt;top.javatool&lt;/groupId&gt; &lt;artifactId&gt;canal-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.2.1-RELEASE&lt;/version&gt;&lt;/dependency&gt; ②编写配置123canal: destination: heima # canal的集群名字，要与安装canal时设置的名称一致 server: 192.168.150.101:11111 # canal服务地址 ③修改Item实体类通过@Id、@Column、等注解完成Item与数据库表字段的映射： 12345678910111213141516171819202122232425262728293031323334353637package com.heima.item.pojo;import com.baomidou.mybatisplus.annotation.IdType;import com.baomidou.mybatisplus.annotation.TableField;import com.baomidou.mybatisplus.annotation.TableId;import com.baomidou.mybatisplus.annotation.TableName;import lombok.Data;import org.springframework.data.annotation.Id;import org.springframework.data.annotation.Transient;import javax.persistence.Column;import java.util.Date;@Data@TableName(&quot;tb_item&quot;)public class Item &#123; @TableId(type = IdType.AUTO) @Id private Long id;//商品id @Column(name = &quot;name&quot;) private String name;//商品名称 private String title;//商品标题 private Long price;//价格（分） private String image;//商品图片 private String category;//分类名称 private String brand;//品牌名称 private String spec;//规格 private Integer status;//商品状态 1-正常，2-下架 private Date createTime;//创建时间 private Date updateTime;//更新时间 @TableField(exist = false) @Transient private Integer stock; @TableField(exist = false) @Transient private Integer sold;&#125; ④编写监听器通过实现EntryHandler&lt;T&gt;接口编写监听器，监听Canal消息。注意两点： 实现类通过@CanalTable(&quot;tb_item&quot;)指定监听的表信息 EntryHandler的泛型是与表对应的实体类 12345678910111213141516171819202122232425262728293031323334353637383940414243package com.heima.item.canal;import com.github.benmanes.caffeine.cache.Cache;import com.heima.item.config.RedisHandler;import com.heima.item.pojo.Item;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;import top.javatool.canal.client.annotation.CanalTable;import top.javatool.canal.client.handler.EntryHandler;@CanalTable(&quot;tb_item&quot;)@Componentpublic class ItemHandler implements EntryHandler&lt;Item&gt; &#123; @Autowired private RedisHandler redisHandler; @Autowired private Cache&lt;Long, Item&gt; itemCache; @Override public void insert(Item item) &#123; // 写数据到JVM进程缓存 itemCache.put(item.getId(), item); // 写数据到redis redisHandler.saveItem(item); &#125; @Override public void update(Item before, Item after) &#123; // 写数据到JVM进程缓存 itemCache.put(after.getId(), after); // 写数据到redis redisHandler.saveItem(after); &#125; @Override public void delete(Item item) &#123; // 删除数据到JVM进程缓存 itemCache.invalidate(item.getId()); // 删除数据到redis redisHandler.deleteItemById(item.getId()); &#125;&#125; 在这里对Redis的操作都封装到了RedisHandler这个对象中，是我们之前做缓存预热时编写的一个类，内容如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package com.heima.item.config;import com.fasterxml.jackson.core.JsonProcessingException;import com.fasterxml.jackson.databind.ObjectMapper;import com.heima.item.pojo.Item;import com.heima.item.pojo.ItemStock;import com.heima.item.service.IItemService;import com.heima.item.service.IItemStockService;import org.springframework.beans.factory.InitializingBean;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.data.redis.core.StringRedisTemplate;import org.springframework.stereotype.Component;import java.util.List;@Componentpublic class RedisHandler implements InitializingBean &#123; @Autowired private StringRedisTemplate redisTemplate; @Autowired private IItemService itemService; @Autowired private IItemStockService stockService; private static final ObjectMapper MAPPER = new ObjectMapper(); @Override public void afterPropertiesSet() throws Exception &#123; // 初始化缓存 // 1.查询商品信息 List&lt;Item&gt; itemList = itemService.list(); // 2.放入缓存 for (Item item : itemList) &#123; // 2.1.item序列化为JSON String json = MAPPER.writeValueAsString(item); // 2.2.存入redis redisTemplate.opsForValue().set(&quot;item:id:&quot; + item.getId(), json); &#125; // 3.查询商品库存信息 List&lt;ItemStock&gt; stockList = stockService.list(); // 4.放入缓存 for (ItemStock stock : stockList) &#123; // 2.1.item序列化为JSON String json = MAPPER.writeValueAsString(stock); // 2.2.存入redis redisTemplate.opsForValue().set(&quot;item:stock:id:&quot; + stock.getId(), json); &#125; &#125; public void saveItem(Item item) &#123; try &#123; String json = MAPPER.writeValueAsString(item); redisTemplate.opsForValue().set(&quot;item:id:&quot; + item.getId(), json); &#125; catch (JsonProcessingException e) &#123; throw new RuntimeException(e); &#125; &#125; public void deleteItemById(Long id) &#123; redisTemplate.delete(&quot;item:id:&quot; + id); &#125;&#125; 五、Redis实践1.Redis键值设计1）优雅的key结构Redis的Key虽然可以自定义，但最好遵循下面的几个最佳实践约定： 遵循基本格式：[业务名称]:[数据名]:[id] 长度不超过44字节 不包含特殊字符 例如：我们的登录业务，保存用户信息，其key可以设计成如下格式： 这样设计的好处： 可读性强 避免key冲突 可能多个业务都会用到id，所以需要id 方便管理 更节省内存： key是string类型，底层编码包含intSet、embstr和raw三种。embstr在小于44字节使用，采用连续内存空间，内存占用更小。当字节数大于44字节时，会转为raw模式存储，在raw模式下，内存空间不是连续的，而是采用一个指针指向了另外一段内存空间，在这段空间里存储SDS内容，这样空间不连续，访问的时候性能也就会收到影响，还有可能产生内存碎片 2）BigKey①基本概念BigKey指的是一条记录很大，和这一条的value有关，并只是说key很大！ BigKey通常以Key的大小和Key中成员的数量来综合判定，例如： Key本身的数据量过大：一个String类型的Key，它的value为 5MB Key中的成员数过多：一个ZSET类型的Key，它的成员数量为10,000个 Key中成员的数据量过大：一个Hash类型的Key，它的成员数量虽然只有1,000个但这些成员的Value（值）总大小为100 MB 那么如何判断元素的大小呢？redis也给我们提供了命令 推荐值： 单个key的value小于10KB 对于集合类型的key，建议元素数量小于1000 ②BigKey的危害 网络阻塞 对BigKey执行读请求时，少量的QPS就可能导致带宽使用率被占满，导致Redis实例，乃至所在物理机变慢 数据倾斜 BigKey所在的Redis实例内存使用率远超其他实例，无法使数据分片的内存资源达到均衡 分片集群，每个集群的slots都是均分的，但是Bigkey所在的Redis的内存使用率更远超其它的实例 Redis阻塞 对元素较多的hash、list、zset等做运算会耗时较长，使主线程被阻塞 比如做diff等操作都会耗时较长 CPU压力 对BigKey的数据序列化和反序列化会导致CPU的使用率飙升，影响Redis实例和本机其它应用 ③如何发现BigKeya）redis-cli –bigkeys利用redis-cli提供的–bigkeys参数，可以遍历分析所有key，并返回Key的整体统计信息与每个数据的Top1的big key 命令：redis-cli -a 密码 --bigkeys b）scan扫描自己编程，利用scan扫描Redis中的所有key，利用strlen、hlen等命令判断key的长度（此处不建议使用MEMORY USAGE） scan 命令调用完后每次会返回2个元素，第一个是下一次迭代的光标，第一次光标会设置为0，当最后一次scan 返回的光标等于0时，表示整个scan遍历结束了，第二个返回的是List，一个匹配的key的数组 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586import com.heima.jedis.util.JedisConnectionFactory;import org.junit.jupiter.api.AfterEach;import org.junit.jupiter.api.BeforeEach;import org.junit.jupiter.api.Test;import redis.clients.jedis.Jedis;import redis.clients.jedis.ScanResult;import java.util.HashMap;import java.util.List;import java.util.Map;public class JedisTest &#123; private Jedis jedis; @BeforeEach void setUp() &#123; // 1.建立连接 // jedis = new Jedis(&quot;192.168.150.101&quot;, 6379); jedis = JedisConnectionFactory.getJedis(); // 2.设置密码 jedis.auth(&quot;123321&quot;); // 3.选择库 jedis.select(0); &#125; final static int STR_MAX_LEN = 10 * 1024; final static int HASH_MAX_LEN = 500; @Test void testScan() &#123; int maxLen = 0; long len = 0; String cursor = &quot;0&quot;; do &#123; // 扫描并获取一部分key ScanResult&lt;String&gt; result = jedis.scan(cursor); // 记录cursor cursor = result.getCursor(); List&lt;String&gt; list = result.getResult(); if (list == null || list.isEmpty()) &#123; break; &#125; // 遍历 for (String key : list) &#123; // 判断key的类型 String type = jedis.type(key); switch (type) &#123; case &quot;string&quot;: len = jedis.strlen(key); maxLen = STR_MAX_LEN; break; case &quot;hash&quot;: len = jedis.hlen(key); maxLen = HASH_MAX_LEN; break; case &quot;list&quot;: len = jedis.llen(key); maxLen = HASH_MAX_LEN; break; case &quot;set&quot;: len = jedis.scard(key); maxLen = HASH_MAX_LEN; break; case &quot;zset&quot;: len = jedis.zcard(key); maxLen = HASH_MAX_LEN; break; default: break; &#125; if (len &gt;= maxLen) &#123; System.out.printf(&quot;Found big key : %s, type: %s, length or size: %d %n&quot;, key, type, len); &#125; &#125; &#125; while (!cursor.equals(&quot;0&quot;)); &#125; @AfterEach void tearDown() &#123; if (jedis != null) &#123; jedis.close(); &#125; &#125;&#125; c）第三方工具 利用第三方工具，如 Redis-Rdb-Tools 分析RDB快照文件，全面分析内存使用情况 https://github.com/sripathikrishnan/redis-rdb-tools d）网络监控 自定义工具，监控进出Redis的网络数据，超出预警值时主动告警 一般阿里云搭建的云服务器就有相关监控页面 ④如何删除BigKeyBigKey内存占用较多，即便是删除这样的key也需要耗费很长时间，导致Redis主线程阻塞，引发一系列问题。 redis 3.0 及以下版本 如果是集合类型，则遍历BigKey的元素，先逐个删除子元素，最后删除BigKey Redis在4.0后提供了异步删除的命令：unlink 3）恰当的数据类型①存储User对象a）方式一：json字符串 user:1 {“name”: “Jack”, “age”: 21} 优点：实现简单粗暴 缺点：数据耦合，不够灵活，不能修改user的某一个field b）方式二：字段打散 user:1:name Jack user:1:age 21 优点：可以灵活访问对象任意字段 缺点：占用空间大、没办法做统一控制，不能对一个user的整体数据做操作 c）方式三：hash（推荐） user:1 name jack age 21 优点：底层使用ziplist，空间占用小，可以灵活访问对象的任意字段 缺点：代码相对复杂，有工具类可以实现 ②优化超大hash假如有hash类型的key，其中有100万对field和value，field是自增id，这个key存在什么问题？如何优化？ key field value someKey id:0 value0 ..... ..... id:999999 value999999 存在的问题： hash的entry数量超过500时，会使用哈希表Dict而不是ZipList，内存占用较多 a）方案一拆分为string类型 key value id:0 value0 ..... ..... id:999999 value999999 存在的问题： string结构底层没有太多内存优化，内存占用较多 想要批量获取这些数据比较麻烦 b）方案二拆分为小的hash，将 id &#x2F; 100 作为key， 将id % 100 作为field，这样每100个元素为一个Hash key field value key:0 id:00 value0 ..... ..... id:99 value99 key:1 id:00 value100 ..... ..... id:99 value199 .... key:9999 id:00 value999900 ..... ..... id:99 value999999 key&#x3D;1中的数据就相当于key+field&#x3D;（1+0）（1+99）&#x3D;101199条数据，value则是对应的值 c）test123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class JedisTest &#123; private Jedis jedis; @BeforeEach void setUp() &#123; // 1.建立连接 // jedis = new Jedis(&quot;192.168.150.101&quot;, 6379); jedis = JedisConnectionFactory.getJedis(); // 2.设置密码 jedis.auth(&quot;123321&quot;); // 3.选择库 jedis.select(0); &#125; @Test void testSetBigKey() &#123; //BigKey Map&lt;String, String&gt; map = new HashMap&lt;&gt;(); for (int i = 1; i &lt;= 650; i++) &#123; map.put(&quot;hello_&quot; + i, &quot;world!&quot;); &#125; jedis.hmset(&quot;m2&quot;, map); &#125; @Test void testBigHash() &#123; //超大hash Map&lt;String, String&gt; map = new HashMap&lt;&gt;(); for (int i = 1; i &lt;= 100000; i++) &#123; map.put(&quot;key_&quot; + i, &quot;value_&quot; + i); &#125; jedis.hmset(&quot;test:big:hash&quot;, map); &#125; @Test void testBigString() &#123; //打散存入str中 for (int i = 1; i &lt;= 100000; i++) &#123; jedis.set(&quot;test:str:key_&quot; + i, &quot;value_&quot; + i); &#125; &#125; @Test void testSmallHash() &#123; //打散存入小的hash中 int hashSize = 100; Map&lt;String, String&gt; map = new HashMap&lt;&gt;(hashSize); for (int i = 1; i &lt;= 100000; i++) &#123; int k = (i - 1) / hashSize; int v = i % hashSize; map.put(&quot;key_&quot; + v, &quot;value_&quot; + v); if (v == 0) &#123; jedis.hmset(&quot;test:small:hash_&quot; + k, map); &#125; &#125; &#125; @AfterEach void tearDown() &#123; if (jedis != null) &#123; jedis.close(); &#125; &#125;&#125; 4）小结 Key的最佳实践 固定格式：[业务名]:[数据名]:[id] 足够简短：不超过44字节 不包含特殊字符 Value的最佳实践： 合理的拆分数据，拒绝BigKey 选择合适数据结构 Hash结构的entry数量不要超过1000 设置合理的超时时间 2.批处理优化1）客户端与redis交互①单个命令的执行流程 ②N条命令的执行流程 redis执行命令的速度是非常快的，所以大部分时间都花在了网络传输，我们可以想办法减少网络传输时间 ③批处理 批处理的一次指令中包含了n个指令，但批处理指令是通过一次网络传输 虽然批处理可以减少网络传输时间，但要注意一次批处理指令的大小 如共mset了100,000次插入数据，虽然可以一次传输过去，但是这样可能会大量占用某时刻带宽，造成网络拥塞 我们可以分批，如循环100次批处理命令，一次批处理1000条 这样虽然多了一些网络传输耗时，但是减轻了带宽负担 2）mset①exampleRedis提供了很多Mxxx这样的命令，可以实现批量插入数据，例如： mset hmset 利用mset批量插入10万条数据 12345678910111213141516@Testvoid testMxx() &#123; String[] arr = new String[2000]; int j; long b = System.currentTimeMillis(); for (int i = 1; i &lt;= 100000; i++) &#123; j = (i % 1000) &lt;&lt; 1; //最开始的1-1000个就是2,4,6,...,0 arr[j] = &quot;test:key_&quot; + i;//2,4,6,....,0 arr[j + 1] = &quot;value_&quot; + i;//3,5,7,.....,1 if (j == 0) &#123; jedis.mset(arr); &#125; &#125; long e = System.currentTimeMillis(); System.out.println(&quot;time: &quot; + (e - b));&#125; 注意通过String数组来传String类型的key-value的时候需要array是[key1],[value],[key2],[value]这样挨着的 mxxx作为redis内置命令，是可以保证原子性的 ②缺点 String有mset，hash有hmset 但hmset也只是能改变同一个key的field，不能传不同的key LPUSH key element….，也只是能传一个key的不同node_element，并不能传不同点key 以上缺点，考虑Pipeline来解决 3）单机PipelineMSET虽然可以批处理，但是却只能操作部分数据类型，因此如果有对复杂数据类型的批处理需要，建议使用Pipeline 12345678910111213141516@Testvoid testPipeline() &#123; // 创建管道 Pipeline pipeline = jedis.pipelined();//通过jedis获得的Pipeline long b = System.currentTimeMillis(); for (int i = 1; i &lt;= 100000; i++) &#123; // 放入命令到管道 pipeline.set(&quot;test:key_&quot; + i, &quot;value_&quot; + i); if (i % 1000 == 0) &#123; // 将管道中的1000条命令，一次性发送到redis然后批量执行 pipeline.sync(); &#125; &#125; long e = System.currentTimeMillis(); System.out.println(&quot;time: &quot; + (e - b));&#125; Pipeline管道和操作系统类似，客户端设置了一个缓冲区接收一组指令，Redis实例设置了一个缓冲区来接收这一组指令并执行 除了pipeline.set，还可以pipeline.hset() 注意：管道是不能保证原子性的 将1000条指令通过管道放入redis的缓冲区以后，这1000条指令会在redis的执行队列排队，并不能保证这1000条指令中插入其它的指令 4）集群下的批处理①基本策略​ 如MSET或Pipeline这样的批处理需要在一次请求中携带多条命令，而此时如果Redis是一个集群，那批处理命令的多个key必须落在一个插槽中，否则就会导致执行失败。大家可以想一想这样的要求其实很难实现，因为我们在批处理时，可能一次要插入很多条数据，这些数据很有可能不会都落在相同的节点上（不同slot），这就会导致报错了 这个时候，我们可以找到4种解决方案： 第一种方案 串行执行，所以这种方式没有什么意义，当然，执行起来就很简单了，缺点就是耗时过久。 其实就相当于没有批处理 第二种方案 串行slot，简单来说，就是执行前，客户端先计算一下对应的key的slot，一样slot的key就放到一个组里边，不同的，就放到不同的组里边，然后对每个组执行pipeline的批处理，他就能串行执行各个组的命令，这种做法比第一种方法耗时要少，但是缺点呢，相对来说复杂一点，所以这种方案还需要优化一下 第三种方案 并行slot，相较于第二种方案，在分组完成后串行执行，第三种方案，就变成了多线程并行执行各个命令，所以他的耗时就非常短，但是实现呢，也更加复杂。 第四种方案 hash_tag，redis计算key的slot的时候，其实是根据key的有效部分来计算的，通过这种方式就能一次处理所有的key，这种方式耗时最短，实现也简单，但是如果通过操作key的有效部分，那么就会导致所有的key都落在一个节点上，产生数据倾斜的问题，所以我们推荐使用第三种方式。 如&#123;a&#125;name yy，&#123;a&#125;age 11，这样通过key计算slot的时候是根据a计算的 ②串行化执行代码实践1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class JedisClusterTest &#123; private JedisCluster jedisCluster; @BeforeEach void setUp() &#123; // 配置连接池 JedisPoolConfig poolConfig = new JedisPoolConfig(); poolConfig.setMaxTotal(8); poolConfig.setMaxIdle(8); poolConfig.setMinIdle(0); poolConfig.setMaxWaitMillis(1000); HashSet&lt;HostAndPort&gt; nodes = new HashSet&lt;&gt;(); nodes.add(new HostAndPort(&quot;192.168.150.101&quot;, 7001)); nodes.add(new HostAndPort(&quot;192.168.150.101&quot;, 7002)); nodes.add(new HostAndPort(&quot;192.168.150.101&quot;, 7003)); nodes.add(new HostAndPort(&quot;192.168.150.101&quot;, 8001)); nodes.add(new HostAndPort(&quot;192.168.150.101&quot;, 8002)); nodes.add(new HostAndPort(&quot;192.168.150.101&quot;, 8003)); jedisCluster = new JedisCluster(nodes, poolConfig); &#125; @Test void testMSet() &#123; jedisCluster.mset(&quot;name&quot;, &quot;Jack&quot;, &quot;age&quot;, &quot;21&quot;, &quot;sex&quot;, &quot;male&quot;); &#125; @Test void testMSet2() &#123; Map&lt;String, String&gt; map = new HashMap&lt;&gt;(3); map.put(&quot;name&quot;, &quot;Jack&quot;); map.put(&quot;age&quot;, &quot;21&quot;); map.put(&quot;sex&quot;, &quot;Male&quot;); //对Map数据进行分组，根据entry.getKey来计算slot，相同slot放在一个分组 //key就是slot，value就是一个组 Map&lt;Integer, List&lt;Map.Entry&lt;String, String&gt;&gt;&gt; result = map.entrySet() .stream() .collect(Collectors.groupingBy( entry -&gt; ClusterSlotHashUtil.calculateSlot(entry.getKey())) ); //串行的去执行mset的逻辑 for (List&lt;Map.Entry&lt;String, String&gt;&gt; list : result.values()) &#123; String[] arr = new String[list.size() * 2]; int j = 0; for (int i = 0; i &lt; list.size(); i++) &#123; j = i&lt;&lt;2; Map.Entry&lt;String, String&gt; e = list.get(0); arr[j] = e.getKey(); arr[j + 1] = e.getValue(); &#125; jedisCluster.mset(arr); &#125; &#125; @AfterEach void tearDown() &#123; if (jedisCluster != null) &#123; jedisCluster.close(); &#125; &#125;&#125; ②Spring中lettuce处理集群环境下批处理代码 12345678910111213@Test void testMSetInCluster() &#123; Map&lt;String, String&gt; map = new HashMap&lt;&gt;(3); map.put(&quot;name&quot;, &quot;Rose&quot;); map.put(&quot;age&quot;, &quot;21&quot;); map.put(&quot;sex&quot;, &quot;Female&quot;); stringRedisTemplate.opsForValue().multiSet(map); List&lt;String&gt; strings = stringRedisTemplate.opsForValue().multiGet(Arrays.asList(&quot;name&quot;, &quot;age&quot;, &quot;sex&quot;)); strings.forEach(System.out::println); &#125; 原理分析： 整体就是方案基本策略中方案3的思想 在RedisAdvancedClusterAsyncCommandsImpl 类中 首先根据slotHash算出来一个partitioned的map，map中的key就是slot，而他的value就是对应的对应相同slot的key对应的数据 通过 RedisFuture&lt; String &gt; mset &#x3D; super.mset(op);进行异步的消息发送 12345678910111213141516171819202122@Overridepublic RedisFuture&lt;String&gt; mset(Map&lt;K, V&gt; map) &#123; Map&lt;Integer, List&lt;K&gt;&gt; partitioned = SlotHash.partition(codec, map.keySet());//计算slot分组 if (partitioned.size() &lt; 2) &#123; return super.mset(map); &#125; Map&lt;Integer, RedisFuture&lt;String&gt;&gt; executions = new HashMap&lt;&gt;(); for (Map.Entry&lt;Integer, List&lt;K&gt;&gt; entry : partitioned.entrySet()) &#123; Map&lt;K, V&gt; op = new HashMap&lt;&gt;(); entry.getValue().forEach(k -&gt; op.put(k, map.get(k))); RedisFuture&lt;String&gt; mset = super.mset(op);//可以看到是异步执行的 executions.put(entry.getKey(), mset); &#125; return MultiNodeExecution.firstOfAsync(executions);&#125; 3.服务器端优化1）持久化配置Redis的持久化虽然可以保证数据安全，但也会带来很多额外的开销，因此持久化请遵循下列建议： 用来做缓存的Redis实例尽量不要开启持久化功能 因为缓存并不需要高安全性，禁用持久化可以提升性能 建议关闭RDB持久化功能，使用AOF持久化 RDB采用fork和copy-on-write，会消耗大量内存和cpu 利用脚本定期在slave节点做RDB，实现数据备份 RDB关闭后我们可以手动的在从节点上做数据备份 设置合理的rewrite阈值，避免频繁的bgrewrite bgrewrite会进行内存读写、消耗CPU资源(计算哪些指令可以合并或删除) 配置no-appendfsync-on-rewrite &#x3D; yes，禁止在rewrite期间做aof刷盘，避免因AOF引起的阻塞 fsync就是刷盘时间，如果超过2s，那么redis会认为刷盘有问题，主线程阻塞不再接收新命令并等待刷盘完成 rewrite和RDB的fork会涉及到大量IO，就可能导致aof的fsync阻塞，导致redis阻塞 要想安全性，在rewrite期间做持久化，开启；要想性能，关闭 部署有关建议： Redis实例的物理机要预留足够内存，应对fork和rewrite 单个Redis实例内存上限不要太大，例如4G或8G 单个Redis内存小，意味着可以加快fork的速度、减少主从同步、数据迁移压力 不要与CPU密集型应用部署在一起 如elasticsearch全文搜索 不要与高硬盘负载应用一起部署。例如：数据库、消息队列 2）服务器端优化-慢查询优化①什么是慢查询并不是很慢的查询才是慢查询，而是：在Redis执行时耗时超过某个阈值的命令，称为慢查询，所以get、set也可能是慢查询。 慢查询的危害：由于Redis是单线程的，所以当客户端发出指令后，他们都会进入到redis底层的queue来执行，如果此时有一些慢查询的数据，就会导致大量请求阻塞在queue，从而引起报错，所以我们需要解决慢查询问题。 引起慢查询的原因 如keys * 这种命令本身性质就会引发慢查询 查询到了一个bigkey，如一个集合放了超过1000个成员 …… ②慢查询相关配置 慢查询的阈值可以通过配置指定： slowlog-log-slower-than：慢查询阈值，单位是微秒。默认是10000，建议1000 慢查询会被放入慢查询日志中，日志的长度有上限，可以通过配置指定： slowlog-max-len：慢查询日志（本质是一个队列）的长度。默认是128，建议1000 修改这两个配置可以使用：config set命令： 注意这些都是动态配置重启后就没有了，持久配置需要在config文件中配置 ③如何查看慢查询 slowlog len：查询慢查询日志长度 slowlog get [n]：读取n条慢查询日志 slowlog reset：清空慢查询列表 3）敏感命令及安全配置安全可以说是服务器端一个非常重要的话题，如果安全出现了问题，那么一旦这个漏洞被一些坏人知道了之后，并且进行攻击，那么这就会给咱们的系统带来很多的损失，所以我们这节课就来解决这个问题。 Redis会绑定在0.0.0.0:6379，这样将会将Redis服务暴露到公网上，而Redis如果没有做身份认证，会出现严重的安全漏洞.漏洞重现方式：https://cloud.tencent.com/developer/article/1039000 为什么会出现不需要密码也能够登录呢，主要是linux考虑到每次登录都比较麻烦，所以linux就有一种ssh免秘钥登录的方式，生成一对公钥和私钥，私钥放在本地，公钥放在linux端，当我们登录时服务器，再登录时候，他会去解析公钥和私钥，如果没有问题，则不需要利用linux的登录也能访问，这种做法本身也很常见，但是这里有一个前提，前提就是公钥必须保存在服务器上，才行，但是linux的漏洞在于在不登录的情况下，也能把秘钥送到linux服务器，从而产生漏洞 漏洞出现的核心的原因有以下几点： Redis未设置密码 利用了Redis的config set命令动态修改Redis配置，RDB文件的位置 使用了Root账号权限启动Redis 所以：如何解决呢？我们可以采用如下几种方案 为了避免这样的漏洞，这里给出一些建议： Redis一定要设置密码 禁止线上使用下面命令：keys、flushall、flushdb、config set等命令。可以利用rename-command禁用。 bind：限制网卡，禁止外网网卡访问 开启防火墙 不要使用Root账户启动Redis 尽量不是有默认的端口 4）Redis内存划分和内存配置①基本问题 内存占用 说明 数据内存 是Redis最主要的部分，存储Redis的键值信息。主要问题是BigKey问题、内存碎片问题 进程内存 Redis主进程本身运⾏肯定需要占⽤内存，如代码、常量池等等；这部分内存⼤约⼏兆，在⼤多数⽣产环境中与Redis数据占⽤的内存相⽐可以忽略。 缓冲区内存 一般包括客户端缓冲区、AOF缓冲区、复制缓冲区等。客户端缓冲区又包括输入缓冲区和输出缓冲区两种。这部分内存占用波动较大，不当使用BigKey，可能导致内存溢出。 当Redis内存不足时，可能导致Key频繁被删除、响应时间变长、QPS不稳定等问题。当内存使用率达到90%以上时就需要我们警惕，并快速定位到内存占用的原因。 数据内存中碎片问题分析 Redis底层分配并不是这个key有多大，他就会分配多大，而是有他自己的分配策略，比如8,16,20等等，假定当前key只需要10个字节，此时分配8肯定不够，那么他就会分配16个字节，多出来的6个字节就不能被使用，这就是我们常说的 碎片问题 进程内存问题分析： 这片内存通常是固定的，并且很小，我们可以忽略不计 缓冲区内存问题分析： 一般包括客户端缓冲区、AOF缓冲区、复制缓冲区等。客户端缓冲区又包括输入缓冲区和输出缓冲区两种。这部分内存占用波动较大，所以这片内存也是我们需要重点分析的内存问题。 ②相关命令我们通过一些命令，可以查看到Redis目前的内存分配状态： info memory：查看内存分配的情况 memory xxx：查看key的主要占用情况 ③缓冲区内存接下来我们看到了这些配置，最关键的缓存区内存如何定位和解决呢？ 内存缓冲区常见的有三种： 复制缓冲区：主从复制的repl_backlog_buf，如果太小可能导致频繁的全量复制，影响性能。通过replbacklog-size来设置，默认1mb AOF缓冲区：AOF刷盘之前的缓存区域，AOF执行rewrite的缓冲区。无法设置容量上限 客户端缓冲区：分为输入缓冲区和输出缓冲区，输入缓冲区最大1G且不能设置。输出缓冲区可以设置 以上缓冲区中 **复制缓冲区 **和 AOF缓冲区 不会有问题，最关键就是客户端缓冲区的问题 ​ 客户端缓冲区：指的就是我们发送命令时，客户端用来缓存命令的一个缓冲区，也就是我们向redis输入数据的输入端缓冲区和redis向客户端返回数据的响应缓存区，输入缓冲区最大1G且不能设置，所以这一块我们根本不用担心，如果超过了这个空间，redis会直接断开，因为本来此时此刻就代表着redis处理不过来了，我们需要担心的就是输出端缓冲区 ​ 我们在使用redis过程中，处理大量的big value，那么会导致我们的输出结果过多，如果输出缓存区过大，会导致redis直接断开，而默认配置的情况下， 其实它是没有大小的，内存可能一下子被占满，会直接导致redis断开，所以解决方案有两个 设置输出缓冲区的大小 增加我们带宽的大小，避免我们出现大量数据从而直接超过了redis的承受能力 5）集群优化-集群还是主从①分片集群的问题集群虽然具备高可用特性，能实现自动故障恢复，但是如果使用不当，也会存在一些问题： 集群完整性问题 集群带宽问题 数据倾斜问题 客户端性能问题 命令的集群兼容性问题 lua和事务问题 问题1、在Redis的默认配置中，如果发现任意一个插槽不可用，则整个集群都会停止对外服务： 大家可以设想一下，如果有几个slot不能使用，那么此时整个集群都不能用了，我们在开发中，其实最重要的是可用性，所以需要把如下配置修改成no，即有slot不能使用时，我们的redis集群还是可以对外提供服务 shutdown 了一个节点的主从，那么这部分slots就没办法用了，这也会导致集群的down，尽管set的key计算出来的slots是在其它节点上，仍然会执行失败 解决途径： 修改集群的配置文件：cluster-require-full-coverage no //原来是yes 这样就能在某一个节点down的时候，其它节点还能使用 slots在其它节点仍可以进行操作，slots在down的节点还是不能操作 问题2、集群带宽问题 集群节点之间会不断的互相Ping来确定集群中其它节点的状态。每次Ping携带的信息至少包括： 插槽信息 集群状态信息 ​ 集群中节点越多，集群状态信息数据量也越大，10个节点的相关信息可能达到1kb，此时每次集群互通需要的带宽会非常高，这样会导致集群中大量的带宽都会被ping信息所占用，这是一个非常可怕的问题，所以我们需要去解决这样的问题 解决途径： 避免大集群，集群节点数不要太多，最好少于1000，如果业务庞大，则建立多个集群。 避免在单个物理机中运行太多Redis实例 1个Redis实例ping100个，那么10个就会ping1000个 配置合适的cluster-node-timeout值，也就是集群间心跳检测主观认为down的，ping的时间就是这个值的一半 适量调高，可以减少ping的频率 如果太高会导致redis集群检测故障的能力降低 问题3、命令的集群兼容性问题 ​ 有关这个问题咱们已经探讨过了，当我们使用批处理的命令时，redis要求我们的key必须落在相同的slot上，然后大量的key同时操作时，是无法完成的，所以客户端必须要对这样的数据进行处理，推荐并行按slots分组，lettuce已经实现了。 问题4、lua和事务的问题 ​ lua和事务都是要保证原子性问题，如果你的key对应的slots不在一个节点，那么是无法保证lua的执行和事务的特性的，所以在集群模式是没有办法执行lua和事务的。 ②到底是集群还是主从？​ 单体Redis（主从Redis）已经能达到万级别的QPS，并且也具备很强的高可用特性。如果主从能满足业务需求的情况下，所以如果不是在万不得已的情况下，尽量不搭建Redis集群。 六、Redis原理1.Redis底层数据结构1）动态字符串：SDS①基本概念我们都知道Redis中保存的Key是字符串，value往往是字符串或者字符串的集合。可见字符串是Redis中最常用的一种数据结构。 不过Redis没有直接使用C语言中的字符串，因为C语言字符串存在很多问题： 获取字符串长度的需要通过运算 非二进制安全 不可修改 Redis构建了一种新的字符串结构，称为简单动态字符串（Simple Dynamic String），简称SDS。 例如，我们执行命令： 1set name yy 那么Redis将在底层创建两个SDS，其中一个是包含“name”的SDS，另一个是包含“yy”的SDS。 Redis是C语言实现的，其中SDS是一个结构体，源码如下： 因为redis中定义了不同的SDS，他们能接受的char[]数组上限也不一样，比如SDS8就代表接收2^8的char.length 例如，一个包含字符串“name”的sds结构如下： 这样可以直接通过len来获得字符串长度，而不需要遍历字符串到结束标志 ②动态扩容SDS之所以叫做动态字符串，是因为它具备动态扩容的能力，例如一个内容为“hi”的SDS： 假如我们要给SDS追加一段字符串“,Amy”，这里首先会申请新内存空间： 如果新字符串小于1M，则新空间为扩展后字符串长度的两倍+1（+1是结束标志） 如果新字符串大于1M，则新空间为扩展后字符串长度+1M+1（+1是结束标志），称为内存预分配 内存预分配就是预先多分配一些空间，这样下次扩容的时候如果预先多分配的空间足够就不用再进行扩容操作，减少了扩容的频率 这里hi,Amy共有6个字符&lt;1M，新空间就是2*6+1，但这里len是6，alloc也是12，这是因为+1的是结束标志 我们可以很轻易的获得字符串长度len&#x3D;6，而不再需要遍历 2）intSet①基本概念IntSet是Redis中set集合的一种实现方式，基于整数数组来实现，并且具备长度可变、有序等特征。结构如下： 这encodeding代表数组中int变量的编码，比如支持2^16，2^32，2^64位的数据（2,4,8字节） length就代表了该数组里面有多少个数 contents虽然是int8_t定义的，但其实redis打破了该定义，数组中每个数字的编码还是根据encodeding了来定义 这里contents仅仅是一个指针，指向了数组的其实地址 其中的encoding包含三种模式，表示存储的整数大小不同： 为了方便查找，Redis会将intset中所有的整数按照升序依次保存在contents数组中，结构如图： 现在，数组中每个数字都在int16_t的范围内，因此采用的编码方式是INTSET_ENC_INT16，每部分占用的字节大小为：encoding：2字节length：3contents：2字节 * 3 &#x3D; 6字节 我们向该其中添加一个数字：50000，这个数字超出了int16_t的范围，intset会自动升级编码方式到合适的大小。以当前案例来说流程如下： 升级编码为INTSET_ENC_INT32, 每个整数占4字节，并按照新的编码方式及元素个数扩容数组 倒序依次将数组中的元素拷贝到扩容后的正确位置 若先调整5，那么5新的长度是4字节，就把原来的10给覆盖了 所以需要从后面调整起，20的下标是2，编码是4字节，那么20的新起点是2*4&#x3D;8 将待添加的元素放入数组末尾 最后，将inset的encoding属性改为INTSET_ENC_INT32，将length属性改为4 ②源码分析 小总结： Intset可以看做是特殊的整数数组，具备一些特点： Redis会确保Intset中的元素唯一、有序 具备类型升级机制，可以节省内存空间 比如说都不超过2^16的数字，那就不用去升级空间 底层采用二分查找方式来查询新插入的元素要插入的位置 找到了说明set已经有了，那么阻止插入（唯一性） 没找到就返回应该插入的位置 3）Dict①基本概念Redis是一个键值型（Key-Value Pair）的数据库，我们可以根据键实现快速的增删改查。而键与值的映射关系正是通过Dict来实现的。Dict由三部分组成，分别是：哈希表（DictHashTable）、哈希节点（DictEntry）、字典（Dict） 这就和java中的HashMap很类似，哈希节点就相当于Map.Entry，记录了自己的key和value，如果是链表还记录了下个entry的指针 entry中union代表该entry的value是这里面的一种，可以是int也可以是字符串（指针） 为什么哈希表大小必须是2^n？ 若现在size&#x3D;4&#x3D;(100)，其掩码sizemask&#x3D;3&#x3D;（011），那么元素算出来的哈希值h &amp; sizemask，这样一直到倒数第二位，前面的值进行&amp;0都&#x3D;0，最后两位是1，那么元素hash这里的值是多少，最后就是多少。 这样的效果就是所有元素hash值计算后都会在00,01,10,11这个区间，也就是0,1,2,3，这就相当于将所有的元素对4取余了 ​ 当我们向Dict添加键值对时，Redis首先根据key计算出hash值（h），然后利用 h &amp; sizemask来计算元素应该存储到数组中的哪个索引位置。我们存储k1,v1，假设k1的哈希值h &#x3D;1，则1&amp;3 &#x3D;1，因此k1,v1要存储到数组角标1位置。 Dict由三部分组成，分别是：哈希表（DictHashTable）、哈希节点（DictEntry）、字典（Dict） 它们之间的关系如下： 可以看到ht[0]才是我们常用的 ②rehash扩容​ Dict中的HashTable就是数组结合单向链表的实现，当集合中元素较多时，必然导致哈希冲突增多，链表过长，则查询效率会大大降低。Dict在每次新增键值对时都会检查负载因子（LoadFactor &#x3D; used&#x2F;size） ，满足以下两种情况时会触发哈希表扩容： 哈希表的 LoadFactor &gt;&#x3D; 1，并且服务器没有执行 BGSAVE 或者 BGREWRITEAOF 等后台进程； 这时候CPU正在被大量使用，不要因为扩容来占用CPU资源 哈希表的 LoadFactor &gt; 5 ； 具体流程 不管是扩容还是收缩，必定会创建新的哈希表，导致哈希表的size和sizemask变化，而key的查询与sizemask有关。因此必须对哈希表中的每一个key重新计算索引，插入新的哈希表，这个过程称为rehash。过程是这样的： 计算新hash表的realeSize，值取决于当前要做的是扩容还是收缩： 如果是扩容，则新size为第一个大于等于dict.ht[0].used + 1的2^n 如果是收缩，则新size为第一个大于等于dict.ht[0].used的2^n （不得小于4） 按照新的realeSize申请内存空间，创建dictht，并赋值给dict.ht[1] 设置dict.rehashidx &#x3D; 0，标示开始rehash ×并不是：将dict.ht[1]一次性赋值给dict.ht[0]，给dict.ht[1]初始化为空哈希表，释放原来的dict.ht[0]的内存 因为都是在新增、查询、修改、删除操作时区检查是否需要扩容，如果有ht[0]百万数据，这时候如果一次性进行数据的迁移，极大可能会引起主线程操作的阻塞，这是不应该的 每次执行新增、查询、修改、删除操作时，都检查一下dict.rehashidx是否大于-1，如果是则将dict.ht[0].table[rehashidx]的entry链表rehash到dict.ht[1]，并且将rehashidx++。直至dict.ht[0]的所有数据都rehash到dict.ht[1] 这样每次迁移一点，称为渐进式rehash，不会为主线程造成压力 在rehash过程中，新增操作，则直接写入ht[1]，查询、修改和删除则会在dict.ht[0]和dict.ht[1]依次查找并执行。这样可以确保ht[0]的数据只减不增，随着rehash最终为空 将dict.ht[1]赋值给dict.ht[0]，给dict.ht[1]初始化为空哈希表，释放原来的dict.ht[0]的内存 将rehashidx赋值为-1，代表rehash结束 整个过程可以描述成： ③总结Dict的结构： 类似java的HashTable，底层是数组加链表来解决哈希冲突 Dict包含两个哈希表，ht[0]平常用，ht[1]用来rehash Dict的伸缩： 当LoadFactor大于5或者LoadFactor大于1并且没有子进程任务时，Dict扩容 当LoadFactor小于0.1时，Dict收缩 扩容大小为第一个大于等于used + 1的2^n 收缩大小为第一个大于等于used 的2^n Dict采用渐进式rehash，每次访问Dict时执行一次rehash rehash时ht[0]只减不增，新增操作只在ht[1]执行，其它操作在两个哈希表 4）ZipList①ZipList基本结构​ ZipList 是一种特殊的“双端链表” ，由一系列特殊编码的连续内存块组成。可以在任意一端进行压入&#x2F;弹出操作, 并且该操作的时间复杂度为 O(1)。 属性 类型 长度 用途 zlbytes uint32_t 4 字节 记录整个压缩列表占用的内存字节数 zltail uint32_t 4 字节 记录压缩列表表尾节点距离压缩列表的起始地址有多少字节，通过这个偏移量，可以确定表尾节点的地址。 zllen uint16_t 2 字节 记录了压缩列表包含的节点数量。 最大值为UINT16_MAX （65534），如果超过这个值，此处会记录为65535，但节点的真实数量需要遍历整个压缩列表才能计算得出。 entry 列表节点 不定 压缩列表包含的各个节点，节点的长度由节点保存的内容决定。 zlend uint8_t 1 字节 特殊值 0xFF （十进制 255 ），用于标记压缩列表的末端。 可以通过zltail找到尾巴节点，从而实现从后往前找 4+4+2+1固定值 ②ZipListEntryZipList 中的Entry并不像普通链表那样记录前后节点的指针，因为记录两个指针要占用16个字节，浪费内存。而是采用了下面的结构： previous_entry_length：前一节点的长度，占1个或5个字节。 如果前一节点的长度小于254字节，则采用1个字节来保存这个长度值 如果前一节点的长度大于254字节，则采用5个字节来保存这个长度值，第一个字节为0xfe，后四个字节才是真实长度数据 encoding：编码属性，记录content的数据类型（字符串还是整数）以及长度，占用1个、2个或5个字节 contents：负责保存节点的数据，可以是字符串或整数 ZipList中所有存储长度的数值均采用小端字节序，即低位字节在前，高位字节在后。例如：数值0x1234，采用小端字节序后实际存储值为：0x3412 注意：这样我们就可以实现从前往后搜索，也可以完成从后往前搜索了 从前往后 previous_entry_length长度固定，encoding长度固定，encoding记录了contents长度，这样三部分的长度都可以计算出来，那么就可以实现从前往后的遍历 实际上还是需要遍历previous_entry_length和encoding才能知道一个entry的长度，实际上就是跳过了content的长度 从后往前 因为每个结点都记录前一个节点的长度，那么从该节点头，很容易知道前一个节点头，所以很容易就能从后向前遍历 ③Encoding编码ZipListEntry中的encoding编码分为字符串和整数两种： 字符串：如果encoding是以“00”、“01”或者“10”开头，则证明content是字符串 编码 编码长度 字符串大小 |00pppppp| 1 bytes &lt;&#x3D; 63 bytes |01pppppp|qqqqqqqq| 2 bytes &lt;&#x3D; 16383 bytes |10000000|qqqqqqqq|rrrrrrrr|ssssssss|tttttttt| 5 bytes &lt;&#x3D; 4294967295 bytes 例如，我们要保存字符串：“ab”和 “bc” 整数：如果encoding是以“11”开始，则证明content是整数，且encoding固定只占用1个字节 编码 编码长度 整数类型 11000000 1 int16_t（2 bytes） 11010000 1 int32_t（4 bytes） 11100000 1 int64_t（8 bytes） 11110000 1 24位有符整数(3 bytes) 11111110 1 8位有符整数(1 bytes) 1111xxxx 1 直接在xxxx位置保存数值，范围从0001~1101，减1后结果为实际值 ④ZipList的连锁更新问题ZipList的每个Entry都包含previous_entry_length来记录上一个节点的大小，长度是1个或5个字节： 如果前一节点的长度小于254字节，则采用1个字节来保存这个长度值 如果前一节点的长度大于等于254字节，则采用5个字节来保存这个长度值，第一个字节为0xfe，后四个字节才是真实长度数据 假设我们有N个连续的、长度为250~253字节之间的entry，因此entry的previous_entry_length属性用1个字节即可表示，如图所示： ​ 在这N个entry前面插入一个大小&gt;&#x3D;254的entry，这样就会引起第二个entry的更新其previous_entry_length（从1改成5），而当第二个entry更新，这也就意味着第二个entry的总长度改变了，这样就导致后序这N个entry的长度改变，引起了连锁更新。 ZipList这种特殊情况下产生的连续多次空间扩展操作称之为连锁更新（Cascade Update），新增、删除都可能导致连锁更新的发生。 这要有连续的N各pre为1的entry才能发生，之间有一个pre为5的都会断掉，所以这个问题的发生几率并不高 ⑤总结ZipList特性： 压缩列表的可以看做一种连续内存空间的”双向链表” 列表的节点之间不是通过指针连接，而是记录上一节点和本节点长度来寻址，内存占用较低 如果列表数据过多，导致链表过长，可能影响查询性能 因为仍然是逐个遍历的方式，所以链表过长，还是有可能有性能问题 增或删较大数据时有可能发生连续更新问题 引入了新结构来解决，但是还没有做替换 5）QuickList①作用问题1：ZipList虽然节省内存，但申请内存必须是连续空间，如果内存占用较多，申请内存效率很低。怎么办？ ​ 答：为了缓解这个问题，我们必须限制ZipList的长度和entry大小。 问题2：但是我们要存储大量数据，超出了ZipList最佳的上限该怎么办？ ​ 答：我们可以创建多个ZipList来分片存储数据。 问题3：数据拆分后比较分散，不方便管理和查找，这多个ZipList如何建立联系？ ​ 答：Redis在3.2版本引入了新的数据结构QuickList，它是一个双端链表，只不过链表中的每个节点都是一个ZipList。 ②基本设置list-max-ziplist-size： 为了避免QuickList中的每个ZipList中entry过多，Redis提供了一个配置项：list-max-ziplist-size来限制。 如果值为正，则代表ZipList的允许的entry个数的最大值 限制entry个数不能很好地解决，因为可能出现单个entry数据很大的情况 如果值为负，则代表ZipList的最大内存大小，分5种情况： -1：每个ZipList的内存占用不能超过4kb -2：每个ZipList的内存占用不能超过8kb -3：每个ZipList的内存占用不能超过16kb -4：每个ZipList的内存占用不能超过32kb -5：每个ZipList的内存占用不能超过64kb 其默认值为 -2： list-compress-depth： 除了控制ZipList的大小，QuickList还可以对节点的ZipList做压缩。通过配置项list-compress-depth来控制。因为链表一般都是从首尾访问较多，所以首尾是不压缩的。这个参数是控制首尾不压缩的节点个数： 0：特殊值，代表不压缩 1：标示QuickList的首尾各有1个节点不压缩，中间节点压缩 2：标示QuickList的首尾各有2个节点不压缩，中间节点压缩 以此类推 默认值：0，意味着默认是不会进行压缩的 ③源码分析以下是QuickList的和QuickListNode的结构源码： 我们接下来用一段流程图来描述当前的这个结构 ④总结QuickList的特点： 是一个节点为ZipList的双端链表 节点采用ZipList，解决了传统链表的内存占用问题 控制了ZipList大小，解决连续内存空间申请效率问题 中间节点可以压缩，进一步节省了内存 6）SkipList①基本概念SkipList（跳表）首先是链表，但与传统链表相比有几点差异： 元素按照升序排列存储 节点可能包含多个指针，指针跨度不同 可以看到同一个节点可能包含多个跨度的指针 比如要查询第9，先走1-10，发现score更小，那就在下一级指正来寻找 ②源码分析 每一个节点都有一个sds存储value，score则是排序查找用的 score相同的情况下会按照字典排序 节点中包含了一个指针数组（包含下一个节点指针，索引跨度） ③总结SkipList的特点： 跳跃表是一个双向链表，每个节点都包含score和ele值 节点按照score值排序，score值一样则按照ele字典排序 每个节点都可以包含多层指针，层数是1到32之间的随机数 不同层指针到下一个节点的跨度不同，层级越高，跨度越大 增删改查效率与红黑树基本一致，实现却更简单 7）RedisObject①基本概念Redis中的任意数据类型的键和值都会被封装为一个RedisObject，也叫做Redis对象： 从Redis的使用者的角度来看，⼀个Redis节点包含多个database（非cluster模式下默认是16个，cluster模式下只能是1个），而一个database维护了从key space到object space的映射关系。这个映射关系的key是string类型，⽽value可以是多种数据类型，比如：string、list、hash、set、sorted set等。我们可以看到，key的类型固定是string，而value可能的类型是多个。⽽从Redis内部实现的⾓度来看，database内的这个映射关系是用⼀个dict来维护的。dict的key固定用⼀种数据结构来表达就够了，这就是动态字符串sds。而value则比较复杂，为了在同⼀个dict内能够存储不同类型的value，这就需要⼀个通⽤的数据结构，这个通用的数据结构就是robj，全名是redisObject。 ②Redis的编码方式Redis中会根据存储的数据类型不同，选择不同的编码方式，共包含11种不同类型： 编号 编码方式 说明 0 OBJ_ENCODING_RAW raw编码动态字符串 1 OBJ_ENCODING_INT long类型的整数的字符串 2 OBJ_ENCODING_HT hash表（字典dict） 3 OBJ_ENCODING_ZIPMAP 已废弃 4 OBJ_ENCODING_LINKEDLIST 双端链表 5 OBJ_ENCODING_ZIPLIST 压缩列表 6 OBJ_ENCODING_INTSET 整数集合 7 OBJ_ENCODING_SKIPLIST 跳表 8 OBJ_ENCODING_EMBSTR embstr的动态字符串 9 OBJ_ENCODING_QUICKLIST 快速列表 10 OBJ_ENCODING_STREAM Stream流 ③五种数据结构对应的编码Redis中会根据存储的数据类型不同，选择不同的编码方式。每种数据类型的使用的编码方式如下： 数据类型 编码方式 OBJ_STRING int、embstr、raw OBJ_LIST LinkedList和ZipList(3.2以前)、QuickList（3.2以后） OBJ_SET intset、HT OBJ_ZSET ZipList、HT、SkipList OBJ_HASH ZipList、HT 2.Redis基本数据结构1）String①基本概念String是Redis中最常见的数据存储类型： 其基本编码方式是RAW，基于简单动态字符串（SDS）实现，存储上限为512mb。 如果存储的SDS长度小于44字节，则会采用EMBSTR编码，此时object head与SDS是一段连续空间。 申请内存时只需要调用一次内存分配函数，效率更高。 String类型的value的值是数字，那么Redis内部会把它转成long类型来存储 SDS： ②RAW 可以看到redisObject和SDS是分开存储的，redisObject的指正指向了SDS 因为申请内存涉及到用户态转为内核态，比较耗费资源，而RAW的方式需要申请两次内存，分别为redisObject和SDS ③embStr 当SDS存储的value&lt;&#x3D;44时候，会采用embStr方式 这时候redisObject和SDS是连续存储的，不再是通过指针指向 这时候申请内存只会申请一次，只有一次用户态转核心态，效率更高 为什么是44，因为这和redis的内存分配有关系，redis的内存分片是以2^n来分片的 如果SDS&#x3D;44+SDS头信息+redisObject的头信息，刚好是64，正好符合一个数据分片的长度 ④int（Long范围） 如果存储的字符串是整数值，并且大小在LONG_MAX范围内，则会采用INT编码：直接将数据保存在RedisObject的ptr指针位置（刚好8字节，long的编码长度），不再需要SDS了。 这样String的数据相当于就存在了redisObject当中，更加省空间了 ⑤其它相关确切地说，String在Redis中是⽤⼀个robj来表示的： 用来表示String的robj可能编码成3种内部表⽰：OBJ_ENCODING_RAW，OBJ_ENCODING_EMBSTR，OBJ_ENCODING_INT。 其中前两种编码使⽤的是sds来存储，最后⼀种OBJ_ENCODING_INT编码直接把string存成了long型。 在对string进行incr, decr等操作的时候，如果它内部是OBJ_ENCODING_INT编码，那么可以直接行加减操作； 如果它内部是OBJ_ENCODING_RAW或OBJ_ENCODING_EMBSTR编码，那么Redis会先试图把sds存储的字符串转成long型， 如果能转成功，再进行加减操作。 对⼀个内部表示成long型的string执行append, setbit, getrange这些命令，针对的仍然是string的值（即⼗进制表示的字符串），而不是针对内部表⽰的long型进⾏操作。 比如字符串”32”，我们只是将其当做long存储起来，在bitmap使用时仍然是按照字符数组来解释，它包含两个字符，它们的ASCII码分别是0x33和0x32。当我们执行命令setbit key 7 0的时候，相当于把字符0x33变成了0x32，这样字符串的值就变成了”22”。 ⽽如果将字符串”32”按照内部的64位long型来解释，那么它是0x0000000000000020，也就是2进制的100000在这个基础上执⾏setbit位操作，结果就完全不对了。因此，在这些命令的实现中，会把long型先转成字符串再进行相应的操作。 2）ListRedis的List类型可以从首、尾操作列表中的元素： 哪一个数据结构能满足上述特征？ LinkedList ：普通链表，可以从双端访问，内存占用较高，内存碎片较多 ZipList ：压缩列表，可以从双端访问，内存占用低，存储上限低 QuickList：LinkedList + ZipList，可以从双端访问，内存占用较低，包含多个ZipList，存储上限高 Redis的List结构类似一个双端链表，可以从首、尾操作列表中的元素： 在3.2版本之前，Redis采用ZipList和LinkedList来实现List，当元素数量小于512并且元素大小小于64字节时采用ZipList编码，超过则采用LinkedList编码。 在3.2版本之后，Redis统一采用QuickList来实现List： 3）Set①基本概念Set是Redis中的单列集合，满足下列特点： 不保证有序性 保证元素唯一 求交集、并集、差集 Set是Redis中的集合，不一定确保元素有序，可以满足元素唯一、查询效率要求极高（判断元素是否存在） 可以看出，Set对查询元素的效率要求非常高，思考一下，什么样的数据结构可以满足？ SkipList也有较高的效率，但是它需要一个score来进行排序，set不需要排序 HashTable，也就是Redis中的Dict，不过Dict是双列集合（可以存键、值对），我们可以只存key 综上，Set选择了HT来作为底层数据结构 ②IntSet 当存储的所有数据都是整数，并且元素数量不超过set-max-intset-entries时，Set会采用IntSet编码，以节省内存 因为IntSet使用了二分查找，所以在数据量不大的情况下，查询的效率依然非常高 在创建Set的时候会判断是否是整数，然后再构建IntSet 当存储的所有数据都是整数 元素数量不超过set-max-intset-entries ③Dict ​ IntSet是有弊端的： 必须全是整数 整数的数量不能太多，因为毕竟使用的二分查找，如果元素太多，效率还是不能保证 所以当存储的有不是整数，并且元素数量超过set-max-intset-entries的时候，会采用Dict方式： HashTable，也就是Redis中的Dict，不过Dict是双列集合（可以存键、值对），我们可以只存key，所有的value设为null 这和java中的HashMap和HashSet很类似 每次插入新元素redis都回去检查是否符合IntSet的两个条件，如果不符合就会转换成Set集合 4）Zset①基本概念ZSet也就是SortedSet，其中每一个元素都需要指定一个score值和member值： 可以根据score值排序后 member必须唯一 可以根据member查询分数 因此，zset底层数据结构必须满足键值存储、键必须唯一、可排序这几个需求。之前学习的哪种编码结构可以满足？ SkipList：可以排序，并且可以同时存储score和ele值（member），可以通过分数排名来获取值 如果根据member查分数，这样效率并不高（遍历） member需要唯一，检查其唯一效率也不高（遍历） HT（Dict）：可以键值存储，并且可以根据key找value 但是并不能排序，不能通过分数排名来获取某个值，或者一个范围内的值 综上，redis采用两种方法结合来构建Zset，如下： 由于是两种结构结合，所以内存空间占用非常大，因此再存储的元素较少的时候会使用ZipList来进行优化 ZipList 是一种特殊的“双端链表” ，由一系列特殊编码的连续内存块组成 ②SkipList+HT Dict的entry的key是member，value是score 这样我们可以快速查询member对应的score 并且我们能快速查询出是否已经有了member了，保证member唯一性 SkipList能够帮进行排序，并行快速获取score排名的某个值，或者一个范围内的值 ③ZipList 当元素数量不多时，HT和SkipList的优势不明显，而且更耗内存。因此zset还会采用ZipList结构来节省内存，不过需要同时满足两个条件： 元素数量小于zset_max_ziplist_entries，默认值128 每个元素都小于zset_max_ziplist_value字节，默认值64 ZipList 是一种特殊的“双端链表” ，由一系列特殊编码的连续内存块组成 ziplist本身没有排序功能，而且没有键值对的概念，因此需要有zset通过编码实现： ZipList是连续内存，因此score和element是紧挨在一起的两个entry， element在前，score在后 score越小越接近队首，score越大越接近队尾，按照score值升序排列 它并不能保证顺序性，所以需要代码实现 并且在大数量的时候查询效率也不能得到保证 5）Hash①基本概念Hash结构与Redis中的Zset非常类似： 都是键值存储 都需求根据键获取值 键必须唯一 区别如下： zset的键是member，值是score；hash的键和值都是任意值 zset要根据score排序；hash则无需排序 因此，可以模仿Zset的实现，但是将其排序部分也就是skipList排除，所以可以使用zset或HT(Dict)来实现 ②HT(Dict) 注意当ZipList不满足条件的时候没转化为HT 每次新增都会检查 ③Ziplist 当Hash中数据项比较少的情况下，Hash底层才⽤压缩列表ziplist进⾏存储数据，随着数据的增加，底层的ziplist就可能会转成dict，具体配置如下： hash-max-ziplist-entries 512 hash-max-ziplist-value 64 当满足上面两个条件其中之⼀的时候，Redis就使⽤dict字典来实现hash。 Redis的hash之所以这样设计，是因为当ziplist变得很⼤的时候，它有如下几个缺点： 每次插⼊或修改引发的realloc操作会有更⼤的概率造成内存拷贝，从而降低性能。 ⼀旦发生内存拷贝，内存拷贝的成本也相应增加，因为要拷贝更⼤的⼀块数据。 当ziplist数据项过多的时候，在它上⾯查找指定的数据项就会性能变得很低，因为ziplist上的查找需要进行遍历。 总之，ziplist本来就设计为各个数据项挨在⼀起组成连续的内存空间，这种结构并不擅长做修改操作。⼀旦数据发⽣改动，就会引发内存 realloc，可能导致内存拷贝。 3.Redis网络模型1） 用户空间和内核态空间​ 计算机硬件包括，如cpu，内存，网卡等等，内核（通过寻址空间）可以操作硬件的，但是内核需要不同设备的驱动，有了这些驱动之后，内核就可以去对计算机硬件去进行 内存管理，文件系统的管理，进程的管理等等 ​ 我们想要用户的应用来访问，计算机就必须要通过对外暴露的一些接口，才能访问到，从而简单的实现对内核的操控，但是内核本身上来说也是一个应用，所以他本身也需要一些内存，cpu等设备资源，如windows一开始就会占用一部分内存，占用部分CPU资源。 ​ 用户应用本身也在消耗这些资源，如果不加任何限制，用户去操作随意的去操作我们的资源，就有可能导致一些冲突，甚至有可能导致我们的系统出现无法运行的问题，如剩余资源无法支撑内核了，因此我们需要把用户和内核隔离开。 进程的寻址空间划分成两部分：内核空间、用户空间 ​ 什么是寻址空间呢？我们的应用程序也好，还是内核空间也好，都是没有办法直接去物理内存的，而是通过分配一些虚拟内存映射到物理内存中，我们的内核和应用程序去访问虚拟内存的时候，就需要一个虚拟地址，这个地址是一个无符号的整数，比如一个32位的操作系统，他的带宽就是32，他的虚拟地址就是2的32次方，也就是说他寻址的范围就是0~2的32次方， 这片寻址空间对应的就是2的32个字节，就是4GB，这个4GB，会有3个GB分给用户空间，会有1GB给内核系统 ​ 在linux中，他们权限分成两个等级，0和3。 ​ 用户空间只能执行受限的命令（Ring3），而不能直接调用系统资源，必须通过内核提供的接口来访问内核空间可以执行特权命令（Ring0），调用一切系统资源，所以一般情况下，用户的操作是运行在用户空间，而内核运行的数据是在内核空间的，而有的情况下，一个应用程序需要去调用一些特权资源，去调用一些内核空间的操作，所以此时他俩需要在用户态和内核态之间进行切换。 比如： Linux系统为了提高IO效率，会在用户空间和内核空间都加入缓冲区： 写数据时，要把用户缓冲数据拷贝到内核缓冲区，然后写入设备 读数据时，要从设备读取数据到内核缓冲区，然后拷贝到用户缓冲区 针对这个操作：我们的用户在写读数据时，会去向内核态申请，想要读取内核的数据，而内核数据要去等待驱动程序从硬件上读取数据，当从磁盘上加载到数据之后，内核会将数据写入到内核的缓冲区中，然后再将数据拷贝到用户态的buffer中，然后再返回给应用程序，整体而言，速度慢，就是这个原因，为了加速，我们希望read也好，还是wait for data也最好都不要等待，或者时间尽量的短。 ​ 如我们现在的应用接受一个网络请求（也就是读取），需要切换到内核态，然后再内核上进行等待请求wait for data（这是第一部分消耗的时间），等到数据后会将数据读入到内核空间的buffer，然后再从内核空间的buffer读入用户空间的buffer中，切换到用户空间，用户程序才能去处理这些数据。 ​ 回应response的过程就是先从用户空间的buffer将处理好的数据写会内核空间的buffer，然后再写到网卡，返回该数据。 以上过程其实就是阻塞IO，这里主要有两个改进的地方： 内核态等待数据的时间 数据拷贝时间：read时从内核空间向用户空间复制，write时从用户空间向内核空间复制 为了提高IO效率，我们就要优化这两点。 2）阻塞IO-BIO应用程序想要去读取数据，他是无法直接去读取磁盘数据的，他需要先到内核里边去等待内核操作硬件拿到数据，这个过程就是1，是需要等待的，等到内核从磁盘上把数据加载出来之后，再把这个数据写给用户的缓存区，这个过程是2，如果是阻塞IO，那么整个过程中，用户从发起读请求开始，一直到读取到数据，都是一个阻塞状态。 顾名思义，阻塞IO就是两个阶段都必须阻塞等待： 阶段一： 用户进程尝试读取数据（比如网卡数据） 此时数据尚未到达，内核需要等待数据 此时用户进程也处于阻塞状态 阶段二： 数据到达并拷贝到内核缓冲区，代表已就绪 将内核数据拷贝到用户缓冲区 拷贝过程中，用户进程依然阻塞等待 拷贝完成，用户进程解除阻塞，处理数据 总结： 可以看到，阻塞IO模型中，用户进程在两个阶段都是阻塞状态 类似操作系统中的轮询 3）非阻塞IO-NIO 顾名思义，非阻塞IO的recvfrom操作会立即返回结果而不是阻塞用户进程。 阶段一： 用户进程尝试读取数据（比如网卡数据） 此时数据尚未到达，内核需要等待数据 返回异常给用户进程 用户进程拿到error后，再次尝试读取 循环往复，直到数据就绪 阶段二： 将内核数据拷贝到用户缓冲区 拷贝过程中，用户进程依然阻塞等待 拷贝完成，用户进程解除阻塞，处理数据 总结： 可以看到，非阻塞IO模型中，用户进程在第一个阶段是非阻塞，第二个阶段是阻塞状态。 虽然是非阻塞，但性能并没有得到提高。而且忙等机制会导致CPU空转，CPU使用率暴增。 这样看非阻塞IO并不一定比阻塞IO好，但非阻塞IO在IO多路复用中有着重要作用 4）IO多路复用①基本思路无论是阻塞IO还是非阻塞IO，用户应用在一阶段都需要调用recvfrom来获取数据，差别在于无数据时的处理方案： 如果调用recvfrom时，恰好没有数据 阻塞IO会使CPU阻塞 非阻塞IO使CPU空转，都不能充分发挥CPU的作用 如果调用recvfrom时，恰好有数据，则用户进程可以直接进入第二阶段，读取并处理数据 所以怎么看起来以上两种方式性能都不好 而在单线程情况下，只能依次处理IO事件，如果正在处理的IO事件恰好未就绪（数据不可读或不可写），线程就会被阻塞，所有IO事件都必须等待，性能自然会很差。 就比如服务员给顾客点餐，分两步： 顾客排队思考要吃什么（socket等待数据就绪），前台接收下单（前台从顾客读取数据） 前台上有订单了，后厨做开始接收订单（前台有了数据，后厨读取该数据) 要提高效率有几种办法？ 方案一：增加更多服务员（多线程） 增加多个前台服务员，虽然可以提高效率，但是前台服务员多了，管理难度和管理花销就多了 线程的切换是有开销的（上下文切换） 方案二：不排队，谁想好了吃什么（数据就绪了），服务员就给谁点餐（用户应用就去读取数据） 后厨先就给服务员说，我在等这些顾客的单子，有单子了就给我说 前台在接收到各个用户数据后直接按铃，通知后厨有单子了 那么问题来了：用户进程如何知道内核中数据是否就绪呢？ 文件描述符（File Descriptor）：简称FD，是一个从0 开始的无符号整数， 用来关联Linux中的一个文件。在Linux中，一切皆文件，例如常规文件、视频、硬件设备等，当然也包括网络套接字（Socket）。可以用FD来关联一个Socket，看其数据是否就绪 通过FD，我们的网络模型可以利用一个线程监听多个FD，并在某个FD可读、可写时得到通知，从而避免无效的等待，充分利用CPU资源。 阶段一： 用户进程调用select，指定要监听的FD集合（后厨指明等待这些顾客单子） 内核监听FD对应的多个socket（前台等待这些顾客下完单） 任意一个或多个socket数据就绪则返回readable（有单子了前台就给后厨说） 此过程中用户进程阻塞 阶段二： 用户进程找到就绪的socket 依次调用recvfrom读取数据 内核将数据拷贝到用户空间 用户进程处理数据 ​ 当用户去读取数据的时候，不再去直接调用recvfrom了，而是调用select的函数，select函数会将需要监听的数据交给内核，由内核去检查这些数据是否就绪了，如果说这个数据就绪了，就会通知应用程序数据就绪，然后来读取数据，再从内核中把数据拷贝给用户态，完成数据处理，如果N多个FD一个都没处理完，此时就进行等待。 用IO复用模式，可以确保去读数据的时候，数据是一定存在的，他的效率比原来的阻塞IO和非阻塞IO性能都要高 分类： ​ IO多路复用是利用单个线程来同时监听多个FD，并在某个FD可读、可写时得到通知，从而避免无效的等待，充分利用CPU资源。不过监听FD的方式、通知的方式又有多种实现，常见的有： select poll epoll 其中select和pool相当于是当被监听的数据准备好之后，他会把你监听的FD整个数据都发给你，你需要到整个FD中去找，哪些是处理好了的，需要通过遍历的方式，所以性能也并不是那么好 而epoll，则相当于内核准备好了之后，他会把准备好的数据，直接发给你，咱们就省去了遍历的动作。 ②selectselect是Linux最早是由的I&#x2F;O多路复用技术： ​ 简单说，就是我们把需要处理的数据封装成FD，然后在用户态时创建一个fd的集合（这个集合的大小是要监听的那个FD的最大值+1，但是大小整体是有限制的 ），这个集合的长度大小是有限制的，同时在这个集合中，标明出来我们要控制哪些数据， ​ 比如要监听的数据，是1,2,5三个数据，此时会执行select函数，然后将整个fd发给内核态，内核态会去遍历用户态传递过来的数据，如果发现这里边都数据都没有就绪，就休眠，直到有数据准备好时，就会被唤醒，唤醒之后，再次遍历一遍，看看谁准备好了，然后再处理掉没有准备好的数据（就绪的保留，未就绪的改为0，这里就是socket1就绪了），最后再将这个FD集合写回到用户态中去，此时用户态就知道了，有人准备好了，但是对于用户态而言，并不知道谁处理好了，所以用户态也需要去进行遍历，然后找到对应准备好数据的节点（仍然为1的bit），再去发起读请求，重复上述过程。 ​ 我们会发现，这种模式下他虽然比阻塞IO和非阻塞IO好，但是依然有些麻烦的事情， 比如说频繁的传递fd集合，频繁的去遍历FD等问题 nfds可以使内核态知道遍历到哪儿结束 用户空间要先将要监听的fdSet拷贝到内核空间，当内核空间有fd就绪了，还要将就绪的fdSet拷贝回去，重复该过程会有大量的用户态和内核态切换 而且内核空间并不直接知道要监听哪些fd，需要遍历fdSet，用户空间也不知道哪些fd就绪了，需要遍历fdSet，这就涉及大量的遍历操作 ③poll poll模式对select模式做了简单改进，但性能提升不明显，部分关键代码如下： IO流程： 创建pollfd数组，向其中添加关注的fd信息，数组大小自定义 调用poll函数，将pollfd数组拷贝到内核空间，转链表存储，无上限 内核遍历fd，判断是否就绪 数据就绪或超时后，拷贝pollfd数组到用户空间，返回就绪fd数量n 用户进程判断n是否大于0，大于0则遍历pollfd数组，找到就绪的fd 与select对比： select模式中的fd_set大小固定为1024，而pollfd在内核中采用链表，理论上无上限 监听FD越多，每次遍历消耗时间也越久，性能反而会下降 ④epoll 详细：https://zhuanlan.zhihu.com/p/63179839 句柄唯一标识了一个eventpoll，内核空间可能有多个eventpoll，用户态可根据句柄来确定某一个eventpoll 原来select的作用被分开了 ctl只会添加要监听的事件fd并加上一个监听函数，放入event的红黑树中 对于每一个FD，只会被放入红黑树一次，减少了大量无意义的放入 epoll_wait函数，这个函数会去校验是否有数据准备完毕（因为数据一旦准备就绪，就会被回调函数添加到list_head中） epoll_wait进行循环，每次都取检查list中是否有准备好的数据，并且epoll_wait函数中传入了一个空数组，epoll_wait会将lsit中已经准备好的节点放入这个空数组中并返回就绪的数量 如果FD没有，epoll_wait会将自己线程挂在eventpoll.wq等待队列上。如果在超时时间内，list上有FD就绪了，就会唤醒该eventpoll.wq上的等待线程，该线程就能够获取到该FD并放入数组中 这样用户空间所见即所得，不需要再去遍历数组 就绪的fd会执行一个监听函数，回调函数把对应fd从红黑树放入就绪list中， 就绪list中的数据会放入用户空间的一个events数组中 这样就避免了用户空间遍历才能得到就绪的fds，现在是所见即所得 select、poll、epoll对比： select模式存在的三个问题： 能监听的FD最大不超过1024 每次select都需要把所有要监听的FD都拷贝到内核空间 每次都要遍历所有FD来判断就绪状态 poll模式的问题： poll利用链表解决了select中监听FD上限的问题，但依然要遍历所有FD，如果监听较多，性能会下降 其实这是伪上限，因为list并不能node越多，增删改查效率都会很低 epoll模式中如何解决这些问题的？ 基于epoll实例中的红黑树保存要监听的FD，理论上无上限，而且增删改查效率都非常高 每个FD只需要执行一次epoll_ctl添加到红黑树，以后每次epol_wait无需传递任何参数，无需重复拷贝FD到内核空间 拷贝的时候只拷贝就绪的FD，用户遍历的都是就绪的FD 利用ep_poll_callback机制来监听FD状态，无需遍历所有FD，因此性能不会随监听的FD数量增多而下降 FD就绪后会通过回调函数，把自己挂在list上 ⑤epoll中的ET和LT 当FD有数据可读时，我们调用epoll_wait（或者select、poll）可以得到通知。但是事件通知的模式有两种： LevelTriggered：简称LT，也叫做水平触发。只要某个FD中有数据可读，每次调用epoll_wait都会得到通知。 EdgeTriggered：简称ET，也叫做边沿触发。只有在某个FD有状态变化时，调用epoll_wait才会被通知。 举个栗子： 假设一个客户端socket对应的FD已经注册到了epoll实例中 客户端socket发送了2kb的数据 服务端调用epoll_wait，得到通知说FD就绪，将FD放入用户空间的数组中，并断开list中的指针 服务端从FD读取了1kb数据的时候又回到步骤3（再次调用epoll_wait，形成循环） LT：内核会检查FD看是否有数据没有读取完，如果没有读取完就再次将FD放入list，再次调用epoll_wait就会又一次获得这些FD，然后读取剩余的数据 ET：只能读一次，所以内核会直接移除这些FD，epoll_wait并不会得到结果 即： 如果我们采用LT模式，因为FD中仍有1kb数据，则第⑤步依然会返回结果，并且得到通知 如果我们采用ET模式，因为第③步已经消费了FD可读事件，第⑤步FD状态没有变化，因此epoll_wait不会返回，数据无法读取，客户端响应超时。 问题1：为什么还要使用ET？ ​ 因为LT会触发惊群现象： epoll_wait回去list上尝试获取FD，如果没有获取成功会将自己瓜子对应的eventpoll.wq等待队列上，在时间范围内，如果有FD就绪（被挂在了list上）那么就会通知该等待队列上的线程，A现在等待队列上，然后被唤醒了处理FD（取出放数组），FD仍然是挂在list上，那么进程B也在该eventpoll.wq等待队列上等待资源，该FD就会可能会唤醒B进程来进行处理。 我们仔细看 ep_scan_ready_list 源码，当 ep-&gt;rdllist 不为空时，会唤醒进程。 当多个进程共享同一个 “epoll fd” 时，多个进程同时在等待资源，也就是多个进程通过 epoll_wait 将自己当前进程的等待事件挂在内核 epoll 实例 eventpoll.wq 等待队列上，换句话说，eventpoll.wq 等待队列上挂着多个进程的等待事件（操作系统），当某个事件触发时，等待队列上的进程会被唤醒。 如果是 lt 模式，epoll 在下一个 epoll_wait 执行前，fd 事件节点仍然会存在就绪队列中，不管事件是否处理完成，那么唤醒进程 A 处理事件时，如果 B 进程也在等待资源，那么同样的事件有可能将 B 进程也唤醒处理，然后 B 又是同样的逻辑唤醒 C —— 连环唤醒问题，这种情况可能是用户不愿意看到的。链接：https://www.zhihu.com/question/403893498/answer/2258283710 问题2：怎样解决ET不能读完的问题 方案1：因为已经读入了FD，需要在这一次通知中把通知读完 设计为非阻塞IO使其while读完，读完后会返回一个error 不能是阻塞IO，因为读完后没内容了会一直阻塞 方案2，手动再放入list，因为其它的进程已经读完了，开始监听其它的数据了，这时候 ⑥基于epoll的web流程 服务端创建serverSocket并得到FD，记做ssfd，然后调用epoll_ctl将该fd添加到红黑树上 调用epoll_wait检查就绪list是否有fd，等待fd就绪 如果list中有fd就绪，epoll_wait将这些fd放入用户空间的events中，让服务端知道这些fd就绪了 服务端会判断时间类型，判断是否是ssfd可读，有错就会返回错误响应 是ssfd可读，代表有客户端socket希望要建立一个连接 建立连接后又生成对应客户端的监听FD，然后epoll_ctl将该FD添加到红黑树上 不是ssfd，也就是客户端socket可读，说明这是客户端要传入了一些数据，那么服务端会读取这些数据并写出响应 5）信号驱动IO 信号驱动IO是与内核建立SIGIO的信号关联并设置回调，当内核有FD就绪时，会发出SIGIO信号通知用户，期间用户应用可以执行其它业务，无需阻塞等待。 阶段一： 用户进程调用sigaction，注册信号处理函数 内核返回成功，开始监听FD 用户进程不阻塞等待，可以执行其它业务 当内核数据就绪后，回调用户进程的SIGIO处理函数 阶段二： 收到SIGIO回调信号 调用recvfrom，读取 内核将数据拷贝到用户空间 用户进程处理数据 缺点： 当有大量IO操作时，信号较多，SIGIO处理函数不能及时处理可能导致信号队列溢出，丢失信号 内核空间与用户空间的频繁信号交互性能也较低 类似于操作系统中讲的中断方式 6）异步IO-AIO这种方式，不仅仅是用户态在试图读取数据后，不阻塞，而且当内核的数据准备完成后，也不会阻塞 他会由内核将所有数据处理完成后，由内核将数据写入到用户态中，然后才算完成，所以性能极高，不会有任何阻塞，全部都由内核完成，可以看到，异步IO模型中，用户进程在两个阶段都是非阻塞状态。 这种模式其实就是将所有工作都交给内核去做了，自己干别的事情，在高并发的情况下会给内核很大的压力 实现这样一个服务有难度 类似操作系统中的DMA和通道，相当于维护了一个服务 7）各种IO方式对比 只有最后的异步IO方式才是真正的异步 IO操作是同步还是异步，需要判断其第二阶段：从内核空间读数据到用户空间的过程是否阻塞 8）redis的单线程①Redis到底是单线程还是多线程？ 如果仅仅聊Redis的核心业务部分（命令处理），答案是单线程 如果是聊整个Redis，那么答案就是多线程 在Redis版本迭代过程中，在两个重要的时间节点上引入了多线程的支持： Redis v4.0：引入多线程异步处理一些耗时较旧的任务，例如异步删除命令unlink（bigKey） Redis v6.0：在核心网络模型中引入多线程，进一步提高对于多核CPU的利用率 因此，对于Redis的核心网络模型，在Redis 6.0之前确实都是单线程。是利用epoll（Linux系统）这样的IO多路复用技术在事件循环中不断处理客户端情况 ②为什么Redis要选择单线程？ 抛开持久化不谈，Redis是纯 内存操作，执行速度非常快，它的性能瓶颈是网络延迟而不是执行速度，因此多线程并不会带来巨大的性能提升。 多线程会导致过多的上下文切换，带来不必要的开销 引入多线程会面临线程安全问题，必然要引入线程锁这样的安全手段，实现复杂度增高，而且性能也会大打折扣 9）Redis网络模型解析①基本流程 ①服务端首先会创建epoll，类似于epoll_create，也就是在内存空间创建红黑树—FD会被放在红黑树上（这里就是aeEventLoop，在这里会等待FD就绪） 这里的EventLoop就等同于epoll中的eventPoll ②aeApiAddEvent就类似于epoll中的epoll_ctl函数，会将对应的FD注册到eventLoop中的红黑树上 ③aeApiPoll就类似于epoll_wait，他会不断地去获得已经就绪的FD，它会获得已经就绪的FDs，然后遍历FDs并调用对应的处理器 如果是serverSocket可读，那么说明这是一个用户端socket连接，那就交由连接应答处理器来处理 它会accept建立起服务端和客户端连接，并注册一个该客户端可读的FD并且设置对应的客户端socket可读的处命令请求处理器 客户端socket和服务器socket连接，redis会将其封装成一个Client实例 这个FD就是以后该客户端传来命令参数，就是客户端可读的FD就会就绪，也就是然后交给命令请求处理器进行处理 如果是客户端可读，那么就会调用命令请求处理器来进行处理 说明之前该客户端已经和服务端建立了连接了，这次传的是具体的命令参数 命令请求处理器会获得每一个clientSocket封装的client（包含了客户端所有信息：请求信息…），并且把请求数据写入对应client的输入缓冲区 然后再将输入缓冲区的数据解析成redis命令并单线程执行 最后把结果写入客户端输出缓冲区，如果客户端输出缓冲区满了（缓冲区有上限），就写入reply（链表无上限）中 最后将每个client实例放入一个队列中 每次在aeApiPoll之前，也就是beforeSleep方法，会注册一个命令回复处理器，为监听客户端socket可写 它会遍历要写的队列中的client，将client中的buf中的数据写会socket中，这样客户端就可以拿到结果了 综上：其实就是一个IO多路复用+事件派发的过程，只不过回调函数都设置成了各种处理器，然后相应的去执行这些任务 ②多线程 命令请求处理器中需要从socket中读出请求数据（client中封装了请求信息：发了什么样的命令），并且解析命令，这部分涉及到IO操作，所以是多线程 从client读取请求数据：命令等，写入客户端的输入缓冲区，也是client中的queryBuf中，涉及到IO操作（多线程） 然后解析queryBuf中的命令（多线程） 单线程执行命令并放入client中的输出缓冲区replyBuf中，buf放不下就放入reply链表（多线程） 命令回复处理器：将队列中的client中buf存的结果写入客户端socket，涉及到网络IO（多线程） 4.Redis通信协议1）RESPRedis是一个CS架构的软件，通信一般分两步（不包括pipeline和PubSub）： 客户端（client）向服务端（server）发送一条命令 服务端解析并执行命令，返回响应结果给客户端 因此客户端发送命令的格式、服务端响应结果的格式必须有一个规范，这个规范就是通信协议。 在Redis中采用的是RESP（Redis Serialization Protocol）协议，是一种应用层协议： Redis 1.2版本引入了RESP协议 Redis 2.0版本中成为与Redis服务端通信的标准，称为RESP2 Redis 6.0版本中，从RESP2升级到了RESP3协议，增加了更多数据类型并且支持6.0的新特性–客户端缓存 但目前因为RESP2和RESP3的不能兼容，所以默认使用的依然是RESP2协议，也是我们要学习的协议版本（以下简称RESP）。 补充：https://toutiao.io/posts/2c0g5j/preview RESP2： 在RESP中，通过首字节的字符来区分不同数据类型，常用的数据类型包括5种： 单行字符串：首字节是 ‘+’ ，后面跟上单行字符串，以CRLF（ “\\r\\n” ）结尾。例如返回”OK”： “+OK\\r\\n” 错误（Errors）：首字节是 ‘-’ ，与单行字符串格式一样，只是字符串是异常信息，例如：”-Error message\\r\\n” 数值：首字节是 ‘:’ ，后面跟上数字格式的字符串，以CRLF结尾。例如：”:10\\r\\n” 多行字符串：首字节是 ‘$’ ，表示二进制安全的字符串，最大支持512MB： 如果大小为0，则代表空字符串：”$0\\r\\n\\r\\n” 如果大小为-1，则代表不存在：”$-1\\r\\n” 数组：首字节是 ‘*’，后面跟上数组元素个数，再跟上元素，元素数据类型不限: 2）基于Socket自定义Redis的客户端Redis支持TCP通信，因此我们可以使用Socket来模拟客户端，与Redis服务端建立连接： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108public class Main &#123; static Socket s; static PrintWriter writer; static BufferedReader reader; public static void main(String[] args) &#123; try &#123; // 1.建立连接 String host = &quot;192.168.150.101&quot;; int port = 6379; s = new Socket(host, port); // 2.获取输出流、输入流 writer = new PrintWriter(new OutputStreamWriter(s.getOutputStream(), StandardCharsets.UTF_8)); reader = new BufferedReader(new InputStreamReader(s.getInputStream(), StandardCharsets.UTF_8)); // 3.发出请求 // 3.1.获取授权 auth 123321 sendRequest(&quot;auth&quot;, &quot;123321&quot;); Object obj = handleResponse(); System.out.println(&quot;obj = &quot; + obj); // 3.2.set name yy sendRequest(&quot;set&quot;, &quot;name&quot;, &quot;yy&quot;); // 4.解析响应 obj = handleResponse(); System.out.println(&quot;obj = &quot; + obj); // 3.2.set name yy sendRequest(&quot;get&quot;, &quot;name&quot;); // 4.解析响应 obj = handleResponse(); System.out.println(&quot;obj = &quot; + obj); // 3.2.set name yy sendRequest(&quot;mget&quot;, &quot;name&quot;, &quot;num&quot;, &quot;msg&quot;); // 4.解析响应 obj = handleResponse(); System.out.println(&quot;obj = &quot; + obj); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; // 5.释放连接 try &#123; if (reader != null) reader.close(); if (writer != null) writer.close(); if (s != null) s.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; private static Object handleResponse() throws IOException &#123; // 读取首字节 int prefix = reader.read(); // 判断数据类型标示 switch (prefix) &#123; case &#x27;+&#x27;: // 单行字符串，直接读一行 return reader.readLine(); case &#x27;-&#x27;: // 异常，也读一行 throw new RuntimeException(reader.readLine()); case &#x27;:&#x27;: // 数字 return Long.parseLong(reader.readLine()); case &#x27;$&#x27;: // 多行字符串 // 先读长度 int len = Integer.parseInt(reader.readLine()); if (len == -1) &#123; return null; &#125; if (len == 0) &#123; return &quot;&quot;; &#125; // 再读数据,读len个字节。我们假设没有特殊字符，所以读一行（简化） return reader.readLine(); case &#x27;*&#x27;: return readBulkString(); default: throw new RuntimeException(&quot;错误的数据格式！&quot;); &#125; &#125; private static Object readBulkString() throws IOException &#123; // 获取数组大小 int len = Integer.parseInt(reader.readLine()); if (len &lt;= 0) &#123; return null; &#125; // 定义集合，接收多个元素 List&lt;Object&gt; list = new ArrayList&lt;&gt;(len); // 遍历，依次读取每个元素 for (int i = 0; i &lt; len; i++) &#123; list.add(handleResponse()); &#125; return list; &#125; // set name yy private static void sendRequest(String ... args) &#123; writer.println(&quot;*&quot; + args.length); for (String arg : args) &#123; writer.println(&quot;$&quot; + arg.getBytes(StandardCharsets.UTF_8).length); writer.println(arg); &#125; writer.flush(); &#125;&#125; 5.Redis过期策略1）过期key的记录​ Redis本身是一个典型的key-value内存存储数据库，因此所有的key、value都保存在之前学习过的Dict结构中。不过在其database结构体中，有两个Dict：一个用来记录key-value；另一个用来记录key-TTL。 可以看到redisDb中定义了很多dict dict：用来存储所有的key和value expires：用来存储所有的带ttl的key和对应的ttl 2）惰性删除惰性删除：顾明思议并不是在TTL到期后就立刻删除，而是在访问一个key的时候，检查该key的存活时间，如果已经过期才执行删除。 可以看到在查找一个key执行读写操作的之前都会检查该key是否过期，如果过期了就会删除该key 惰性思想就是用到的时候再去处理 惰性删除并不能保证过期key能按时删除，如果一直没有操作到该key，那么该key就有可能永远不会被删除，所以我们还需要一个任务来周期性地检查并删除过期key 3）周期删除周期删除：顾明思议是通过一个定时任务，周期性的抽样部分过期的key，然后执行删除。 执行周期有两种： Redis服务初始化函数initServer()中设置定时任务，按照server.hz的频率来执行过期key清理，模式为SLOW Redis的每个事件循环前会调用beforeSleep()函数，执行过期key清理，模式为FAST ①slow模式SLOW模式规则： 执行频率受server.hz影响，默认为10，即每秒执行10次，每个执行周期100ms。 执行清理耗时不超过一次执行周期的25%.默认slow模式耗时不超过25ms 逐个遍历db，逐个遍历db中的bucket，抽取20个key判断是否过期 如果没达到时间上限（25ms）并且过期key比例大于10%，再进行一次抽样，否则结束 slow模式的频率是可以配置的，slow模式定时任务在初始化server的时候就会设置，有一个全局变量来记录上一次遍历到哪儿了，这一次该从哪儿遍历 slow模式是一种低频，长时间的清理 ②fast模式FAST模式规则（过期key比例小于10%不执行 ）： 执行频率受beforeSleep()调用频率影响，但两次FAST模式间隔不低于2ms 执行清理耗时不超过1ms 逐个遍历db，逐个遍历db中的bucket，抽取20个key判断是否过期如果没达到时间上限（1ms）并且过期key比例大于10%，再进行一次抽样，否则结束 fast模式在每次beforeSleep()都会执行过期key，有一个全局变量来记录上一次遍历到哪儿了，这一次该从哪儿遍历 fast模式是一种高频，短时间的清理 4）总结 RedisKey的TTL记录方式： 在RedisDB中通过一个Dict记录每个Key的TTL时间 过期key的删除策略： 惰性清理：每次查找key时判断是否过期，如果过期则删除 定期清理：定期抽样部分key，判断是否过期，如果过期则删除。定期清理的两种模式： SLOW模式执行频率默认为10，每次不超过25ms FAST模式执行频率不固定，但两次间隔不低于2ms，每次耗时不超过1ms 6.内存淘汰策略1）基本策略内存淘汰：就是当Redis内存使用达到设置的上限时，主动挑选部分key删除以释放更多内存的流程。Redis会在处理客户端命令的方法processCommand()中尝试做内存淘汰： Redis支持8种不同淘汰策略来选择要删除的key： noeviction： 不淘汰任何key，但是内存满时不允许写入新数据，默认就是这种策略。 volatile-ttl： 对设置了TTL的key，比较key的剩余TTL值，TTL越小越先被淘汰 allkeys-random：对全体key ，随机进行淘汰。也就是直接从db-&gt;dict中随机挑选 volatile-random：对设置了TTL的key ，随机进行淘汰。也就是从db-&gt;expires中随机挑选。 allkeys-lru： 对全体key，基于LRU算法进行淘汰 volatile-lru： 对设置了TTL的key，基于LRU算法进行淘汰 allkeys-lfu： 对全体key，基于LFU算法进行淘汰 volatile-lfu： 对设置了TTL的key，基于LFI算法进行淘汰比较容易混淆的有两个： LRU（Least Recently Used），最少最近使用。用当前时间减去最后一次访问时间，这个值越大则淘汰优先级越高。 LFU（Least Frequently Used），最少频率使用。会统计每个key的访问频率，值越小淘汰优先级越高。 可以通过修改配置来选择淘汰策略： 1234##The default is:##maxmemory-policy noeviction 2）基本流程Redis的数据都会被封装为RedisObject结构： redisObject的内存淘汰参数会根据配置来进行修改： LRU： 记录最近一次访问时间，因为只有24bit，所以会以秒为单位 LFU的访问次数之所以叫做逻辑访问次数，是因为并不是每次key被访问都计数，而是通过运算： 生成0~1之间的随机数R 计算1&#x2F;(旧次数 * lfu_log_factor + 1)，记录为P，lfu_log_factor默认是10 如果 R &lt; P ，则计数器 + 1，且最大不超过255 访问次数会随时间衰减，距离上一次访问时间每隔 lfu_decay_time 分钟，计数器 -1 因为可能一个key最开始被访问的很多，很快就到255了，但是后来可能近一年都没被访问 即LFU的高16位记录上一次访问时间，如果距离上一次访问时间间隔lfu_decay_time，计数器-1 整体的流程如下 ①在进行淘汰的时候不能将所有DB中的key拿来做比较，这样开销太大了。redis会逐个从DB中随机挑选一部分key放入驱逐池中 ②驱逐池向外提供了一个统一的接口：驱逐池会按某个标准升序排序，排序越大，淘汰优先级越高，即倒序淘汰key，因此三种淘汰法都需要实现该接口： TTL越小越先被淘汰： 用maxTTL-TTL&#x3D;生命已经度过的时间，那么算出来结果越大，说明越需要被淘汰 LRU： 用now-LRU的值（最近一次访问时间），越大说明最近一次访问时间离现在越远，越该被淘汰 LFU： 有255-LFU计数，说明访问的次数越小，那么就越该被淘汰","categories":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/categories/Redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://example.com/tags/redis/"}]},{"title":"JVM","slug":"JVM","date":"2022-04-14T08:05:12.000Z","updated":"2022-08-17T06:14:12.631Z","comments":true,"path":"2022/04/14/JVM/","link":"","permalink":"http://example.com/2022/04/14/JVM/","excerpt":"内存布局：程序计数器、虚拟机栈、本地方法区、堆、方法区 垃圾回收：回收对象判断、分代回收、垃圾回收器 类加载：类文件结构、字节码指令、语法糖、类加载阶段：加载-链接-初始化、类加载器、运行器优化 内存模型 JMM：原子性、可见性、有序性","text":"内存布局：程序计数器、虚拟机栈、本地方法区、堆、方法区 垃圾回收：回收对象判断、分代回收、垃圾回收器 类加载：类文件结构、字节码指令、语法糖、类加载阶段：加载-链接-初始化、类加载器、运行器优化 内存模型 JMM：原子性、可见性、有序性 一、内存布局 1.程序计数器1）作用用于保存JVM中下一条所要执行的指令的地址 2）特点 线程私有 CPU会为每个线程分配时间片，当当前线程的时间片使用完以后，CPU就会去执行另一个线程中的代码 程序计数器是每个线程所私有的，当另一个线程的时间片用完，又返回来执行当前线程的代码时，通过程序计数器可以知道应该执行哪一句指令 类似操作系统的PCB一样，程序计数器记录了线程上一次结束运行时的环境，当再次运行到该环境的时候可以根据程序计数器来恢复现场 不会存在内存溢出 eg： 1234567891011121314151617180: getstatic #20 // PrintStream out = System.out; 3: astore_1 // -- 4: aload_1 // out.println(1); 5: iconst_1 // -- 6: invokevirtual #26 // -- 9: aload_1 // out.println(2); 10: iconst_2 // -- 11: invokevirtual #26 // -- 14: aload_1 // out.println(3); 15: iconst_3 // -- 16: invokevirtual #26 // -- 19: aload_1 // out.println(4); 20: iconst_4 // -- 21: invokevirtual #26 // -- 24: aload_1 // out.println(5); 25: iconst_5 // -- 26: invokevirtual #26 // -- 29: return 如该线程本次执行到了19行，那么下一次再选中该线程，就会从19行开始 2.虚拟机栈1）定义 每个线程运行需要的内存空间，称为虚拟机栈 每个栈由多个栈帧组成，对应着每次调用方法时所占用的内存 每个线程只能有一个活动栈帧，对应着当前正在执行的方法，也就是栈顶的方法 -Xss 栈内存大小 ​ 2）演示代码： 1234567891011121314public class Main &#123; public static void main(String[] args) &#123; method1(); &#125; private static void method1() &#123; method2(1, 2); &#125; private static int method2(int a, int b) &#123; int c = a + b; return c; &#125;&#125; 在控制台中可以看到，main方法首先调用method1方法，method1方法再调用method2方法，符合栈的特点 3）栈帧​ best origin：https://mp.weixin.qq.com/s/fY0DKPwcyNIhjtBWvaKnew ​ JVM的虚拟机栈是描述Java方法执行的内存区域，并且是线程私有的。栈中的元素用于支持虚拟机进行方法调用，每个方法从开始调用到执行完成的过程，就是栈帧从入帧到出帧的过程。当调用到某方法时，该方法进栈，如该方法没调用其它方法，那么它就是当前方法，执行完以后会出栈，完成此次方法调用。 ​ 在活动线程中，只有位于栈顶的帧才是有效的，称为当前栈帧。正在执行的方法称为当前方法，栈帧是方法运行的基本结构。在执行引擎运行时，所有指令都只能针对当前栈帧进行操作。 ​ StackOverflowError表示请求的栈溢出，导致内存耗尽，通常出现在递归方法中。如果把JVM当做一个棋盘，虚拟机栈就是棋盘上的将&#x2F;帅，当前方法的栈帧就是棋子能走的区域，而操作栈就是每一个棋子。操作栈的压栈和出栈如下图所示 ​ 虚拟机栈通过压栈和出栈的方式，对每个方法对应的活动栈帧进行运算处理，方法正常执行结束，肯定会跳转到下一个栈帧上。在执行的过程中，如果出现异常，会进行异常回溯，返回地址通过异常处理表确定。栈帧在整个JVM体系中的地位颇高，包括局部变量表、操作栈、动态连接、方法返回地址等。 ①局部变量表​ 局部变量表是存放方法参数和局部变量的区域。我们都知道，类属性变量一共要经历两个阶段，分为准备阶段和初始化阶段，而局部变量是没有准备阶段，只有初始化阶段，而且必须是显示的。 ​ 如果是非静态方法，则在index[0]位置上存储的是方法所属对象的实例引用，随后存储的是参数和局部变量。字节码指令中的STORE指令就是将操作栈中计算完成的局部变量写回局部变量表的存储空间内。 ​ 下方有具体例子描述了局部变量表和操作栈如何工作的。 ②操作栈操作栈是一个初始状态为空的桶式结构栈。在方法执行过程中，会有各种指令往栈中写入和提取信息。JVM的执行引擎是基于栈的执行引擎，其中的栈指的就是操作栈。字节码指令集的定义都是基于栈类型的，栈的深度在方法元信息的stack属性中，下面就通过一个例子来说明下操作栈与局部变量表的交互： 1234567public int add() &#123; int x = 10; int y = 20; int z = x + y; return z;&#125; 字节码操作顺序如下： 123456789101112public int add(); Code: 0: bipush 10 // 常量 10 压入操作栈 2: istore_1 // 并保存到局部变量表的 slot_1 中 （第 1 处） 3: bipush 20 // 常量 20 压入操作栈 5: istore_2 // 并保存到局部变量表的 slot_2 中 6: iload_1 // 把局部变量表的 slot_1 元素（int x）压入操作栈 7: iload_2 // 把局部变量表的 slot_2 元素（int y）压入操作栈 8: iadd // 把上方的两个数都取出来，在 CPU 里加一下，并压回操作栈的栈顶 9: istore_3 // 把栈顶的结果存储到局部变量表的 slot_3 中 10: iload_3 11: ireturn // 返回栈顶元素值 第 1 处说明：局部变量表就像一个快递柜，有着很多的柜子，依次编号为1,2,3，…，n，字节码指令 istore_1 就代表打开了 1 号柜子，再把栈顶中的值 10 存进去。栈就好如一个桶，任何时候只能对桶口的元素进行操作，所以数据只能在栈顶进行存取。部分指令可以直接在柜子里面直接进行，比如 iinc指令，直接对抽屉里的数值进行 +1操作。我们经常遇到的 i++ 和 ++i，通过字节码对比起来，答案一下子就一目了然了。如下表格所示： ​ 左列中，iload_1 从局部变量表的第1号柜子取出一个数，压入栈顶，下一步直接在柜子里实现 + 1的操作，而这个操作时对栈顶元素的值没有任何影响，所以 istore_2 只是把栈顶元素赋值给 a，而右列，它是先在柜子里面进行 +1的操作，然后再通过 iload_1 把第1号柜子里的数压入栈顶，所以istore_2赋给a的值是 +1 之后的值。扩展下，i++ 并非是原子操作。即使通过volatile关键字来修饰，多线程情况下，还是会出现数据互相覆盖的情况。 ③动态链接​ 主要服务一个方法需要调用其他方法的场景。在 Java 源文件被编译成字节码文件时，所有的变量和方法引用都作为符号引用（Symbilic Reference）保存在 Class 文件的常量池里。当一个方法要调用其他方法，需要将常量池中指向方法的符号引用转化为其在内存地址中的直接引用。动态链接的作用就是为了将符号引用转换为调用方法的直接引用。 ④方法返回地址方法执行时有两种退出情况： 第一，正常退出，即正常执行到任何方法的返回字节码指令，如 RETURN、IRETURN、ARETURN等； 第二，异常退出。无论何种退出情况，都将返回方法当前被调用的位置。方法退出的过程相当于弹出当前栈帧，而退出可能有三种方式： 返回值压入上层调用栈帧。 异常信息抛给能够处理的栈帧。 PC 计数器指向方法调用后的下一条指令 4）问题辨析 垃圾回收是否涉及栈内存？ 不需要。因为虚拟机栈中是由一个个栈帧组成的，在方法执行完毕后，对应的栈帧就会被弹出栈。所以无需通过垃圾回收机制去回收内存。 栈内存的分配越大越好吗？ 不是。因为物理内存是一定的，栈内存越大，可以支持更多的递归调用， 内存空间被大量占用，可执行的线程数就会越少。 方法内的局部变量是否是线程安全的？ 如果方法内局部变量没有逃离方法的作用范围，则是线程安全的 如果如果局部变量引用了对象，并逃离了方法的作用范围，则需要考虑线程安全问题 1234567891011121314public static void f1 (StringBuilder sb) &#123; sb.append(1); sb.append(2); sb.append(3); System.out.println(sb.toString());) public static StringBuilder f1()&#123; StringBuilder sb = new StringBuilder( ); sb.append(1);, sb.append(2); sb.append(3); return sb;&#125; 以上两种方式中的sb对象都有可能被其它线程锁修改，其它线程有可能获得该sb对象的引用。 ​ 内存溢出 Java.lang.stackOverflowError 栈内存溢出 发生原因 虚拟机栈中，栈帧过多（无限递归） 每个栈帧所占用过大 5）线程运行诊断CPU占用过高 Linux环境下运行某些程序的时候，可能导致CPU的占用过高，这时需要定位占用CPU过高的线程 top命令，查看是哪个进程占用CPU过高 ps H -eo pid, tid（线程id）, %cpu | grep 刚才通过top查到的进程号 通过ps命令进一步查看是哪个线程占用CPU过高 jstack 进程id 通过查看进程中的线程的nid，刚才通过ps命令看到的tid来对比定位，注意jstack查找出的线程id是16进制的，需要转换，可以看到线程状态是Runnable，还可以定位到哪一行代码有问题 jstack还会报告死锁等问题 3.本地方法区 origin：https://mp.weixin.qq.com/s/fY0DKPwcyNIhjtBWvaKnew ​ 本地方法栈（Native Method Stack）在JVM内存布局中，也是线程对象私有的，但是虚拟机栈“主内”，而本地方法栈“主外”。这个“内外”是针对JVM来说的，本地方法栈为Native方法服务。线程开始调用本地方法时，会进入一个不再受JVM约束的世界。本地方法可以通过JNI（Java Native Interface）来访问虚拟机运行时的数据区，甚至可以调用寄存器，具有和JVM相同的能力和权限。当大量本地方法出现时，势必会削弱JVM对系统的控制力，因为它的出错信息都比较黑盒，难以捉摸。对于内存不足的情况，本地方法栈还是会抛出 native heap OutOfMemory。 ​ 重点说下JNI类本地方法，最常用的本地方法应该是System.currentTimeMills()，JNI使Java深度使用操作系统的特性功能，复用非Java代码。但是在项目过程中，如果大量使用其他语言来实现JNI，就会丧失跨平台特性，威胁到程序运行的稳定性。假如需要与本地代码交互，就可以用中间标准框架来进行解耦，这样即使本地方法崩溃也不至于影响到JVM的稳定。 本地方法栈是内存私有的，本地方法栈为Native方法服务（系统调用等） 本地方法区可以访问JVM运行的数据，具有和JVM相同的能力和权限 JNI类本地方法如System.currentTimeMills()，依赖于操作系统的特性功能，大量使用会失去跨平台性。若需要与本地代码交互，可以用中间标准框架解耦。 4.堆1）定义Heap 堆 ​ 通过new关键字创建的对象都会被放在堆内存 2）特点 它是线程共享，堆内存中的对象都需要考虑线程安全问题 有垃圾回收机制 3）堆内存溢出 java.lang.OutofMemoryError ：java heap space. 堆内存溢出 可以使用 -Xmx 来指定堆内存，如-Xmx8m指定堆内存为8M 4）堆内存诊断 jps 工具 查看当前系统中有哪些 java 进程 jmap 工具 查看堆内存某一时刻占用情况 jmap - heap 进程id jconsole 工具 图形界面的，多功能的监测工具，可以连续监测 jvisualvm 工具 可视化JVM，比Jconsle功能更多 eg： 通过JPS查到java进程号 通过jmap和进程id抓取某时刻堆内存情况，老年代、新生代情况 可以通过Jconsle和jvisualvm进行分析 查看是哪一些类占用了大量内存，可以看到类中有什么属性 5.方法区 ​ 方法区是是一个规范，逻辑上的定义，不同厂商有不同实现，1.6的永久代实现是在堆内存中，1.8之后元空间实现就是在本地内存中了。 ​ 方法区域类似于用于传统语言的编译代码的存储区域，或者类似于操作系统进程中的“文本”段。它存储每个类的结构，例如运行时常量池、字段和方法数据，以及方法和构造函数的代码，包括特殊方法，用于类和实例初始化以及接口初始化方法区域是在虚拟机启动时创建的。尽管方法区域在逻辑上是堆的一部分，但简单的实现可能不会选择垃圾收集或压缩它。此规范不强制指定方法区的位置或用于管理已编译代码的策略。方法区域可以具有固定的大小，或者可以根据计算的需要进行扩展，并且如果不需要更大的方法区域，则可以收缩。方法区域的内存不需要是连续的！ ​ 在JDK8版本中，元空间的前身Pern区已经被淘汰。在JDK7及之前的版本中，Hotspot还有Pern区，翻译为永久代，在启动时就已经确定了大小，难以进行调优，并且只有FGC时会移动类元信息。不同于之前版本的Pern（永久代），JDK8的元空间已经在本地内存中进行分配，并且，Pern区中的所有内容中字符串常量移至堆内存，其他内容也包括了类元信息、字段、静态属性、方法、常量等等都移至元空间内 1）方法区内存溢出 1.8 之前会导致永久代内存溢出 使用 -XX:MaxPermSize&#x3D;8m 指定永久代内存大小8m 1.8 之后会导致元空间内存溢出 使用 -XX:MaxMetaspaceSize&#x3D;8m 指定元空间大小8m 2）运行时常量池下面是反编译的结果，所有地址都是**#**，运行时常量池中所有地址都是真实地址： 下面是字节码中的指令 ​ Java 语言既具有编译型语言的特征，也具有解释型语言的特征。因为 Java 程序要经过先编译，后解释两个步骤，由 Java 编写的程序需要先经过编译步骤，生成字节码（.class 文件），这种字节码必须由 Java 解释器来解释执行。 ​ 解释器在解释这些字节码指令的时候会，从运行时常量表中去查询具体的地址，找到要执行的类名、方法名、参数类型等 运行时常量池： 就是一张表，虚拟机指令根据这张常量表找到要执行的类名、方法名、参数类型、字面量信息 常量池是 *.class 文件中的，当该类被加载以后，它的常量池信息就会放入运行时常量池，并把里面的符号地址变为真实地址 3）StringTable串池 运行时常量池中的字符串仅是符号，只有在被用到时才会转化为对象 解释器直行到对应的new String指令后才会将字符串转化为对象 利用串池的机制，来避免重复创建字符串对象 字符串变量拼接的原理是StringBuilder和编译器优化 可以使用intern方法，主动将串池中还没有的字符串对象放入串池中 ①示例123456789101112String f= new String(&quot;a&quot;);//&quot;a&quot;已经在串池中，f还是在堆中String a=&quot;a&quot;+&quot;b&quot;; //此时&quot;a&quot; 和 &quot;b&quot; 已经在串池中，但是a也在String b=&quot;ab&quot;;String c = new String(&quot;a&quot;)+new String(&quot;b&quot;);String d = new String(&quot;ab&quot;);String e = c.intern();String g=&quot;a&quot;;System.out.println(a==b);//trueSystem.out.println(a==c);//falseSystem.out.println(a==d);//falseSystem.out.println(a==e);//trueSystem.out.println(f==g);//false ②字符串变量拼接字符串变量拼接的原理是StringBuilder和编译器优化 12345678String s1 = &quot;a&quot;; //懒惰的String s2 = &quot;b&quot;;String s3 = &quot;ab&quot;;// new StringBuilder ( ).append(&quot; a&quot; ).append(&quot;b&quot; ).toString() new String(&quot;ab&quot;)String s4 = s1 + s2; // javac在编译期间的优化，结果已经在编译期确定为abString s5 = &quot;a” + &quot;b&quot;; System.out.println(s3 =s5); 字符串是懒加载的，只有解释器解释到对应的语句以后才会将String加载到heap 字符串的拼接底层是StringBuilder s4最后调用了toString方法，实际上是new了一个新的String，所以s3!&#x3D;s4 s5虽然是字符串拼接，但是”a”和”b”都是常量，在编译期做了优化，可以确定为”ab”，然后去串池中找，所以s3&#x3D;&#x3D;s5 ③1.8intern方法调用字符串对象的 intern 方法，会将该字符串对象尝试放入到串池中 如果串池中没有该字符串对象，则放入成功 如果有该字符串对象，则放入失败 无论放入是否成功，都会返回串池中的字符串对象 注意：此时如果调用 intern 方法成功，堆内存与串池中的字符串对象是同一个对象；如果失败，则不是同一个对象 例1： 123456789101112131415161718public class Main &#123; public static void main(String[] args) &#123; // 1.str1 则存在于堆内存之中&quot;a&quot; &quot;b&quot; 被放入串池中，str 则存在于堆内存之中 String str1 = new String(&quot;a&quot;) + new String(&quot;b&quot;); // 2.调用 str1 的 intern 方法，这时串池中没有 &quot;ab&quot; ， // 则会将该字符串对象放入到串池中，此时堆内存与串池中的 &quot;ab&quot; 是同一个对象 String st2 = str1.intern(); // 给 str3 赋值，因为此时串池中已有 &quot;ab&quot; ，则直接将串池中的内容返回 String str3 = &quot;ab&quot;; // 因为堆内存与串池中的 &quot;ab&quot; 是同一个对象，所以以下两条语句打印的都为 true System.out.println(str1 == st2); System.out.println(str1 == str3); &#125;&#125; 2.处，串池中还没有”ab”，所以str1可以放入串池中 例2： 1234567891011121314public class Main &#123; public static void main(String[] args) &#123; // 1.此处创建字符串对象 &quot;ab&quot; ，因为串池中还没有 &quot;ab&quot; ，所以将其放入串池中 String str3 = &quot;ab&quot;; // 2.&quot;a&quot; &quot;b&quot; 被放入串池中，str 则存在于堆内存之中 String str1 = new String(&quot;a&quot;) + new String(&quot;b&quot;); // 3.此时因为在创建 str3 时，&quot;ab&quot; 已存在与串池中，所以放入失败，但是会返回串池中的 &quot;ab&quot; String str2 = str1.intern(); System.out.println(str1 == str2);// false System.out.println(str1 == str3);// false System.out.println(str2 == str3);// true &#125;&#125; 3.处，这时候串池中已经有了”ab”，所以str1不能被返回串池，str2返回的实际上是串池的对象 ④1.6intern方法调用字符串对象的intern方法，会将该字符串对象尝试放入到串池中 如果串池中没有该字符串对象，会将该字符串对象复制一份，再放入到串池中 如果有该字符串对象，则放入失败 无论放入是否成功，都会返回串池中的字符串对象 注意：此时无论调用intern方法成功与否，串池中的字符串对象和堆内存中的字符串对象都不是同一个对象 12345678910//[ &quot;a&quot;,&quot;b&quot;&quot;, &quot;ab&quot;&quot;]public static void main(String[] args) &#123; String s = new String(&quot;a&quot;)+new String(&quot;b&quot;); String s2 = s.intern(); //将这个字符串对象尝试放入串池，如果有则并不会放入，如果没有则放入串池，会把串池中的对象返回/s拷贝一份，放入串池 String x = &quot;ab&quot;; System.out.println(s2 == x); //true System.out.println(s == x); //flase，在1.8中就是true&#125; 这里是将s复制一份放入串池，所以s并不在串池中，尽管调用了intern方法 ⑤StringTable垃圾回收 -Xmx10m 指定堆内存大小-XX:+PrintStringTableStatistics 打印字符串常量池信息-XX:+PrintGCDetails-verbose:gc 打印 gc 的次数，耗费时间等信息 演示 StringTable 垃圾回收 -Xmx10m -XX:+PrintStringTableStatistics -XX:+PrintGCDetails -verbose:gc 1234567891011121314public static void main(String[] args) &#123; int i = 0; try &#123; for(int j = 0; j &lt; 10000; j++) &#123; // j = 100, j = 10000 String.valueOf(j).intern(); i++; &#125; &#125;catch (Exception e) &#123; e.printStackTrace(); &#125;finally &#123; System.out.println(i); &#125;&#125; StringTable是由HashTable实现的，虽然放入了10000个String，但最后只有7000个，说明发生了垃圾回收 ⑤StringTable调优 因为StringTable是由HashTable实现的，所以可以适当增加HashTable桶的个数，来减少字符串放入串池所需要的时间 -XX:StringTableSize=桶个数 考虑是否需要将字符串对象入池 如处理500万用户学校信息并且用list存储，这么多用户，肯定有学校信息是一样的 不入池，会在堆中大量建立String，list指向了这些String，所以并不会垃圾回收 入池，重复的String.intern会从串池返回，list指向的是串池中的String，那么堆中的String就会被GC掉 下例中addres.add的就是串池中的String，堆中的String会被gc掉 ⑥Java Integer的缓存策略 类似stringTable：https://www.jianshu.com/p/a5d3c1f03d5d 6.直接内存1）基本功能 普通IO 在读取文件的时候，文件先是到系统内存，java无法从系统内存读入，所以还需备份到java堆内存中才能读入。 堆内存中缓冲区的大小就是我们通过byte[ ] buf = new byte[_1Mb]设定的 直接内存 直接内存，java和操作系统都可以访问 磁盘文件存入 direct memory（在操作系统的内存中）后 java可以直接使用，提高了效率 2）直接内存的管理 java垃圾回收不能管理到直接内存，直接内存的释放是由底层的Unsafe类管理的（进行分配和释放） ByteBuffer 使用了Unsafe对象完成直接内存的分配回收，并且回收需要主动调用freeMemory方法 ByteBuffer 的实现内部使用了 Cleaner（虚引用）来检测 ByteBuffer 。一旦ByteBuffer 被垃圾回收，那么会由 ReferenceHandler（守护线程） 来调用 Cleaner 的 clean 方法调用 freeMemory 来释放内存 一个问题： 一般用 jvm 调优时，会加上下面的参数： -XX:+DisableExplicitGC // 静止显示的 GC 意思就是禁止我们手动的 GC，比如手动 System.gc() 无效，ByteBuffer并不会被gc掉，所以其内部的虚引用Cleaner也不会释放内存。它是一种 full gc，会回收新生代、老年代，会造成程序执行的时间比较长。在禁用后，我们需要通过 unsafe 对象调用 freeMemory 的方式释放内存。 二、垃圾回收1.回收对象判断1）引用计数法​ 当一个对象被引用时，就当引用对象的值加一，当值为 0 时，就表示该对象不被引用，可以被垃圾收集器回收。但有一个弊端，如下图所示，循环引用时，两个对象的计数都为1，导致两个对象都无法被释放。 ​ 2）可达性分析算法 JVM 中的垃圾回收器通过可达性分析来探索所有存活的对象 扫描堆中的对象，看能否沿着 GC Root 对象为起点的引用链找到该对象，如果找不到，则表示可以回收 可以作为 GC Root 的对象： 系统类：启动类加载器加载的，如Object类对象等 本地方法区中 JNI（即一般说的Native方法）引用的对象 虚拟机栈：栈帧中的局部变量引用的对象 monitor加锁的对象引用的对象 方法区中类静态属性引用的对象 方法区中常量引用的对象 https://blog.csdn.net/weixin_38007185/article/details/108093716 3）五种引用 ①强引用List&lt; Integer &gt; one = new ArrayList&lt;&gt;(); one就强引用了该对象，强引用没断，那么该对象就不会被gc ②软引用（SoftReference） 仅有软引用引用该对象时，在垃圾回收后，内存仍不足时会再次出发垃圾回收，回收软引用对象 可以配合引用队列来释放软引用自身 软引用演示： 1234567891011121314151617181920212223242526272829303132333435/*演示 软引用jvm参数：-Xmx20m -XX:+PrintGCDetails -verbose:gc*/public class Code_08_SoftReferenceTest &#123; public static int _4MB = 4 * 1024 * 1024;public static void main(String[] args) throws IOException &#123; method2(); &#125;// 设置 -Xmx20m , 因为list是强引用，不会gc掉这些byte[] public static void method1() throws IOException &#123; ArrayList&lt;byte[]&gt; list = new ArrayList&lt;&gt;(); for(int i = 0; i &lt; 5; i++) &#123; list.add(new byte[_4MB]); &#125; System.in.read(); &#125; // 演示 软引用，软引用了byte[]，所以gc的时候将前4个byte[]gc掉了 public static void method2() throws IOException &#123; ArrayList&lt;SoftReference&lt;byte[]&gt;&gt; list = new ArrayList&lt;&gt;(); for(int i = 0; i &lt; 5; i++) &#123; SoftReference&lt;byte[]&gt; ref = new SoftReference&lt;&gt;(new byte[_4MB]); System.out.println(ref.get()); list.add(ref); System.out.println(list.size()); &#125; System.out.println(&quot;循环结束：&quot; + list.size()); for(SoftReference&lt;byte[]&gt; ref : list) &#123; System.out.println(ref.get()); &#125;&#125; &#125; 软引用配合引用队列 12345678910111213141516171819202122232425// 演示 软引用 搭配引用队列 public static void method3() throws IOException &#123; ArrayList&lt;SoftReference&lt;byte[]&gt;&gt; list = new ArrayList&lt;&gt;(); // 引用队列 ReferenceQueue&lt;byte[]&gt; queue = new ReferenceQueue&lt;&gt;(); for(int i = 0; i &lt; 5; i++) &#123; // 关联了引用队列，当软引用所关联的 byte[] 被回收时，软引用自己会加入到 queue 中去 SoftReference&lt;byte[]&gt; ref = new SoftReference&lt;&gt;(new byte[_4MB], queue);//在这里关联了queue System.out.println(ref.get()); list.add(ref); System.out.println(list.size()); &#125; // 从队列中获取无用的 软引用对象，并移除 Reference&lt;? extends byte[]&gt; poll = queue.poll(); while(poll != null) &#123; list.remove(poll); poll = queue.poll(); &#125; System.out.println(&quot;=====================&quot;); for(SoftReference&lt;byte[]&gt; ref : list) &#123; System.out.println(ref.get()); &#125;&#125; ③弱引用（WeakReference） 仅有弱引用引用该对象时，在垃圾回收时，无论内存是否充足，都会回收弱引用对象 可以配合引用队列来释放弱引用自身 软、弱引用的释放 因为软、弱引用本身也是对象，当他们引用的对象被GC掉后，它们自身会进入一个引用队列，通过遍历释放掉 弱引用演示 12345678910111213141516171819202122232425262728293031323334353637383940public static int _4MB = 4 * 1024 *1024; // 演示 弱引用 public static void method1() &#123; List&lt;WeakReference&lt;byte[]&gt;&gt; list = new ArrayList&lt;&gt;(); for(int i = 0; i &lt; 10; i++) &#123; WeakReference&lt;byte[]&gt; weakReference = new WeakReference&lt;&gt;(new byte[_4MB]); list.add(weakReference); for(WeakReference&lt;byte[]&gt; wake : list) &#123; System.out.print(wake.get() + &quot;,&quot;); &#125; System.out.println(); &#125; &#125;// 演示 弱引用搭配 引用队列 public static void method2() &#123; List&lt;WeakReference&lt;byte[]&gt;&gt; list = new ArrayList&lt;&gt;(); ReferenceQueue&lt;byte[]&gt; queue = new ReferenceQueue&lt;&gt;(); for(int i = 0; i &lt; 9; i++) &#123; WeakReference&lt;byte[]&gt; weakReference = new WeakReference&lt;&gt;(new byte[_4MB], queue);//queue list.add(weakReference); for(WeakReference&lt;byte[]&gt; wake : list) &#123; System.out.print(wake.get() + &quot;,&quot;); &#125; System.out.println(); &#125; System.out.println(&quot;===========================================&quot;); Reference&lt;? extends byte[]&gt; poll = queue.poll(); while (poll != null) &#123; list.remove(poll); poll = queue.poll(); &#125; for(WeakReference&lt;byte[]&gt; wake : list) &#123; System.out.print(wake.get() + &quot;,&quot;); &#125; &#125;&#125; ④虚引用（PhantomReference） 必须配合引用队列使用，主要配合 ByteBuffer 使用，被引用对象回收时，会将虚引用入队，由 Reference Handler 线程调用虚引用相关方法释放直接内存 当ByteBuffer不再被强引用后就能够被GC掉，ByteBuffer占用的内存虽然被释放掉了，但是直接内存还没有被释放掉 ByteBuffer中有一个Cleaner虚引用，当虚引用引用的ByteBuffer被GC掉，那么虚引用就会进入一个引用队列，有Reference Handler线程来调用Cleaner其中的方法来释放掉直接内存 ⑤终结器引用（FinalReference） 内部配合引用队列使用，在垃圾回收时，终结器引用入队（被引用对象暂时没有被回收），再由 Finalizer 线程通过终结器引用找到被引用对象并调用它的 finalize 方法，第二次 GC 时才能回收被引用对象。 所以一般需要两次gc，才能把终结期引用的对象回收掉 额外参考：https://cloud.tencent.com/developer/article/1751370 2.垃圾回收算法1）标记-清除 首先标记垃圾，然后原地清除 清除就是将这些区域添加到空闲表中，类似于操作系统中的空闲表法（还有空闲链表法） 优点：速度快 缺点：空间不连续，容易产生内存碎片 2）标记整理 先标记，再整理 优点：没有内存碎片 缺点：整理会有对象移动，导致整体效率较低 3）复制算法 将内存区域划分成大小相等的两块区域，先标记，然后将存活对象放到to区，清理from区，最后交换from区和to区 将存活的对象放到to区同时也完成了整理工作 优点：没有内存碎片 缺点：需要双倍内存空间 3.分代回收1）形象理解​ 对于一个小区的垃圾，日常垃圾如卫生纸、包装纸等，还有家中不是很需要的家具等 对于需要频繁清理的日常垃圾 我们选择将其放入小区中的垃圾场，每天都要回收，发生较为频繁，清理也较快，类似新生代 对于家中不常用家具 我们首先暂存在家中，等到空间实在不够了，就来一次大清理，发生的不太频繁，清理时间也较长，类似老年代 2）具体流程 整个堆内存分为新生代和老年代，其中新生代分为伊甸园、幸存区From、幸存区to new的新对象会被存入到新生代中的伊甸园中，当伊甸园中的内存不足时，就会进行一次垃圾回收，这时的回收叫做 Minor GC Minor GC 会将伊甸园和幸存区FROM存活的对象先复制到幸存区TO中， 并让其寿命加1，再交换两个幸存区 保证幸存区From有对象的，幸存区To没有对象 再次创建对象，若新生代的伊甸园又满了，则会再次触发 Minor GC（会触发 stop the world， 暂停其他用户线程，只让垃圾回收线程工作），这时不仅会回收伊甸园中的垃圾，还会回收幸存区中的垃圾，再将活跃对象复制到幸存区TO中。回收以后会交换两个幸存区，并让幸存区中的对象寿命加1 如果幸存区中的对象的寿命超过某个阈值（最大为15，4bit），代表这个对象回收的可能性较低，就会被放入老年代中 并不是一定要达到阈值才会晋升到老年代，如果幸存区空间紧张，也有可能提前晋升到老年代（OOM大对象） 如果新生代老年代中的内存都满了，就会先触发Minor GC，尝试回收新生代内。如果新生代内存还是不足，老年代内存也不足，就会触发Full GC，扫描新生代和老年代中所有不再使用的对象并回收 3）注意事项 Minor GC 采用的复制算法 会触发stop the world，因为复制的过程涉及到对象地址的改变 Full GC 采用的标记清除或标记整理 同样会STW，STW的时间会比Minor GC长 一个线程中发生了堆内存溢出，并不会导致主线程的结束 一个线程内的out of memory不会导致整个java线程结束 大对象OOM在新生代空间不够，老年代空间足够的情况下会直接晋升到老年代 4.JVM参数 含义 参数 堆初始大小 -Xms 堆最大大小 -Xmx 或 -XX:MaxHeapSize&#x3D;size 新生代大小 -Xmn 或 (-XX:NewSize&#x3D;size + -XX:MaxNewSize&#x3D;size ) 幸存区比例 -XX:SurvivorRatio&#x3D;ratio 晋升阈值 -XX:MaxTenuringThreshold&#x3D;threshold 晋升详情 -XX:+PrintTenuringDistribution 打印GC详情 -XX:+PrintGCDetails -verbose:gc FullGC 前 MinorGC -XX:+ScavengeBeforeFullGC 5.垃圾回收器1）串行垃圾回收器（单线程） 单线程 只会有一个线程执行垃圾回收 适用于堆内存较小的情况（个人电脑） -XX:+UseSerialGC&#x3D;serial + serialOld serial：新生代垃圾回收器，使用的是复制算法 serialOld：老年代垃圾回收器，使用的是 安全点 让其他线程都在这个点停下来，以免垃圾回收时移动对象地址，使得其他线程找不到被移动的对象因为是串行的，所以只有一个垃圾回收线程。且在该线程执行回收工作时，其他线程进入阻塞状态 Serial 收集器 Serial收集器是最基本的、发展历史最悠久的收集器 特点： 单线程、简单高效（与其他收集器的单线程相比），采用复制算法。对于限定单个CPU的环境来说，Serial收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程手机效率。收集器进行垃圾回收时，必须暂停其他所有的工作线程，直到它结束（Stop The World） ParNew 收集器 ParNew收集器其实就是Serial收集器的多线程版本 特点： 多线程、ParNew收集器默认开启的收集线程数与CPU的数量相同，在CPU非常多的环境中，可以使用-XX:ParallelGCThreads参数来限制垃圾收集的线程数。和Serial收集器一样存在Stop The World问题 Serial Old 收集器 Serial Old是Serial收集器的老年代版本 特点： 同样是单线程收集器，采用标记-整理算法 2）吞吐量优先（并行） 多线程 堆内存较大，多核CPU 单位时间内，STW总体（stop the world，停掉其他所有工作线程）时间最短 JDK1.8默认使用的垃圾回收器 注意： ③和④是冲突的，③一般通过增大堆大小来减少GC频率，而当堆大小增大，则每次gc的最大时间肯定会增大 Parallel Scavenge 收集器： 与吞吐量关系密切，故也称为吞吐量优先收集器 特点： 属于新生代收集器也是采用复制算法的收集器（用到了新生代的幸存区），又是并行的多线程收集器（与ParNew收集器类似）该收集器的目标是达到一个可控制的吞吐量。还有一个值得关注的点是：GC自适应调节策略（与ParNew收集器最重要的一个区别） GC自适应调节策略： Parallel Scavenge收集器可设置-XX:+UseAdptiveSizePolicy参数。当开关打开时不需要手动指定新生代的大小（-Xmn）、Eden与Survivor区的比例（-XX:SurvivorRation）、晋升老年代的对象年龄（-XX:PretenureSizeThreshold）等，虚拟机会根据系统的运行状况收集性能监控信息，动态设置这些参数以提供最优的停顿时间和最高的吞吐量，这种调节方式称为GC的自适应调节策略。 Parallel Scavenge收集器使用两个参数控制吞吐量： XX:MaxGCPauseMillis 控制最大的垃圾收集停顿时间 XX:GCRatio 直接设置吞吐量的大小 Parallel Old 收集器： 是Parallel Scavenge收集器的老年代版本 特点： 多线程，采用标记-整理算法（老年代没有幸存区） 3）响应时间优先（并发） 多线程 堆内存较大，多核CPU 尽可能让单次STW时间变短（尽量不影响其他线程运行） 吞吐量优先和响应时间优先的差别： 吞吐量优先是一段时间内STW时间最短，单次不是最短，但可能一小时就发生2次，响应时间是单次最短，但是一小时可能发生多次（加起来就没吞吐量优先短了） 注意： 一个问题： 由于老年代采用的是标记清除算法，所以可能会产生大量碎片，当老年代碎片过多，新生代内存不，两边都没有空间的时候，老年代的CMS会退化成单线程回收器，进行标记整理工作，这样响应时间就会大大降低，不再是响应优先 https://blog.csdn.net/z69183787/article/details/104921492?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164679536916780264056627%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=164679536916780264056627&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-104921492.pc_search_result_control_group&amp;utm_term=CMS%E5%B9%B6%E5%8F%91%E5%A4%B1%E8%B4%A5&amp;spm=1018.2226.3001.4187 新生代可以晋升的时候，老年代无足够空间，就会触发Full GC，Full GC会STW并串行清理 新生代可以晋升的时候，老年代有空间但是是碎片化，无法容纳下，这时候只能STW进行整理移动工作 CMS 收集器 Concurrent Mark Sweep，一种以获取最短回收停顿时间为目标的老年代收集器 特点： 基于标记-清除算法实现。并发收集、低停顿，但是会产生内存碎片 应用场景： 适用于注重服务的响应速度，希望系统停顿时间最短，给用户带来更好的体验等场景下。如web程序、b&#x2F;s服务 CMS收集器的运行过程分为下列4步： 初始标记：标记GC Roots能直接到的对象。速度很快但是仍存在Stop The World问题 并发标记：进行GC Roots Tracing（通过root找到引用对象）的过程，找出存活对象且用户线程可并发执行 重新标记：为了修正并发标记期间因用户程序继续运行而导致标记产生变动的那一部分对象的标记记录。仍然存在Stop The World问题 并发清除：对标记的对象进行清除回收 CMS收集器的内存回收过程是与用户线程一起并发执行的 4）G1 JDK 9以后默认使用，而且替代了CMS 收集器 使用场景 同时注重吞吐量和低延迟（响应时间） 超大堆内存（内存大的），会将堆内存划分为多个大小相等的区域 整体上是标记-整理算法，两个区域之间是复制算法 ①Young Collection​ 分代是按对象的生命周期划分，分区则是将堆空间划分连续几个不同小区间，每一个小区间独立回收，可以控制一次回收多少个小区间，方便控制 GC 产生的停顿时间 E：伊甸园 S：幸存区 O：老年代 ‘ 流程 新对象会被放入伊甸园中 young Collection会将E中幸存的对象复制到幸存区中 幸存区中对象年龄达到阈值（15）就会晋升到老年代，通过复制方式 因为是通过复制方式，所以会STW ②Young Collection + CM（concurrent mark） 在 Young GC 时会对 GC Root 进行初始并发标记 在老年代占用堆内存的比例达到阈值时，对进行并发标记（不会STW），阈值可以根据用户来进行设定，默认45% ③Mixed Collection会对E S O 进行全面的回收 最终标记：为了修正并发标记期间因用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，会STW 拷贝存活 -XX:MaxGCPauseMills:xxx 用于指定最长的停顿时间 因为设定了单次最长STW时间，所以会对老年代进行选择性复制 问：为什么有的老年代被拷贝了，有的没拷贝？ 因为指定了最大停顿时间，如果对所有老年代都进行回收，耗时可能过高。为了保证时间不超过设定的停顿时间，会回收最有价值的老年代（回收后，能够得到更多内存） 5.Minor GC和Full GC辨析 SerialGC 新生代内存不足发生的垃圾收集- minor gc 老年代内存不足发生的垃圾收集- full gc ParallelGc 新生代内存不足发生的垃圾收集- minor gc 老年代内存不足发生的垃圾收集- full gc CMS 新生代内存不足发生的垃圾收集-minor gc 老年代内存不足 G1 新生代内存不足发生的垃圾收集-minor gc 老年代内存不足 注意： CMS老年代采用标记-清除算法，G1采用标记-复制算法 两者在老年代内存不足时（老年代所占内存超过阈值） 如果垃圾产生速度慢于垃圾回收速度，不会触发Full GC，还是并发地进行清理 如果垃圾产生速度快于垃圾回收速度，便会触发Full GC，并发失败退化为串行收集 6.Young Collection 跨代引用​ 有这样一个问题，在进行可达性分析的时候，有一些GC Root对象是老年代的对象，新生代的对象被这些老年代对象引用着。对于这样一种情况，我们如果通过遍历的方式来遍历老年代的GC Root，那么效率就太低了，可以通过脏卡方式来改善。 将老年代分成一个个card，当某一card中对象引用了新生代对象，就将该card标记为脏卡 在遍历的时候只需要关注脏卡对象即可 在引用发生更新的时候，由其它线程异步进行更新 7.重标记remark在垃圾回收时，收集器处理对象的过程中 黑色：已被处理，需要保留的 灰色：正在处理中的 白色：还未处理的 但是在并发标记过程中，考虑下面情况 并发标记的过程中，根据A来找引用对象的时候A还没引用C，这时候C没被引用，会被标记为需要清理。而check完后，A又引用了C对象，这时候C就不能被GC掉，再最终标记阶段就会对这种情况做检查。 重标记： 之前C未被引用，这时A引用了C，就会给C加一个写屏障，写屏障的指令会被执行，将C放入一个队列当中，并将C变为 处理中 状态 在并发标记阶段结束以后，重新标记阶段会STW，然后将放在该队列中的对象重新处理，发现有强引用引用它，就会处理它 8.JDK GC新特性1）JDK 8u20 字符串去重过程 将所有新分配的字符串（底层是char[]）放入一个队列 当新生代回收时，G1并发检查是否有重复的字符串 如果字符串的值一样，就让他们引用同一个字符串对象 注意，其与String.intern的区别 intern关注的是字符串对象 字符串去重关注的是char[] 在JVM内部，使用了不同的字符串标 12345string s1 = new String(&quot;hello&quot;); //char[]&#123; &#x27;h&#x27;,&#x27;e &#x27;,&#x27;l&#x27;,&#x27;l&#x27;,&#x27;o&#x27;&#125;String s2 = new String(&quot;hello&quot;); //char[]&#123; &#x27;h&#x27;,&#x27;e &#x27;,&#x27;l&#x27;,&#x27;l&#x27;,&#x27;o&#x27;&#125;//两个对象的char[]都是一样的，就会将s1和s2引用同一个char[] 优点与缺点 节省了大量内存 新生代回收时间略微增加，导致略微多占用CPU 2）JDK 8u40 并发标记类卸载在并发标记阶段结束以后，就能知道哪些类不再被使用。如果一个类加载器的所有类都不在使用，则卸载它所加载的所有类 3）JDK 8u60 回收巨型对象 一个对象大于region的一半时，就称为巨型对象 G1不会对巨型对象进行拷贝 回收时被优先考虑 G1会跟踪老年代所有incoming引用，如果老年代incoming引用为0的巨型对象（即老年代没有再引用该巨型对象）就可以在新生代垃圾回收时处理掉 9.GC调优查看虚拟机参数命令 1&quot;F:\\JAVA\\JDK8.0\\bin\\java&quot; -XX:+PrintFlagsFinal -version | findstr &quot;GC&quot;Copy 可以根据参数去查询具体的信息 调优领域 内存 锁竞争 CPU占用 IO GC 确定目标 低延迟&#x2F;高吞吐量？ 选择合适的GC CMS G1 ZGC ParallelGC Zing 最快的GC是不发生GC 首先排除减少因为自身编写的代码而引发的内存问题 查看Full GC前后的内存占用，考虑以下几个问题 数据是不是太多？ 数据表示是否太臃肿 对象图 对象大小 是否存在内存泄漏 定义了Static map，只存不删，可通过软、弱引用来解决 新生代调优 新生代的特点 所有的new操作分配内存都是非常廉价的 TLAB 死亡对象回收零代价 大部分对象用过即死（朝生夕死） MInor GC 所用时间远小于Full GC 新生代内存越大越好么？ 不是 新生代内存太小：频繁触发Minor GC，会STW，会使得吞吐量下降 新生代内存太大：老年代内存占比有所降低，会更频繁地触发Full GC。而且触发Minor GC时，清理新生代所花费的时间会更长 新生代内存设置为内容纳[并发量*(请求-响应)]的数据为宜 幸存区调优 幸存区需要能够保存 当前活跃对象+需要晋升的对象 晋升阈值配置得当，让长时间存活的对象尽快晋升 123-XX:MaxTenuringThreshold=threshold 晋升阈值-XX : +PrintTenuringDistribution 老年代调优 以CMS为例 CMS的老年代内存越大越好 先尝试不做调优，如果没有Full GC，那么目前的设置是OK的 否则先尝试调优新生代 观察发生Full GC时老年代内存占用，将老年代内存预设调大1&#x2F;4~1&#x2F;3 -XX:CMSInitiating0ccupancyFraction=percent 当老年代内存占用达到percent阈值时进行CMS垃圾回收 三、类加载与字节码1.类从源码到运行 编译期 1）编译期​ 创建完源文件之后，程序先要被 JVM中 的 Java 编译器进行编译为 .class 文件。Java 编译一个类时，如果这个类所依赖的类还没有被编译，编译器会自动的先编译这个所依赖的类，然后引用。如果 Java 编译器在指定的目录下找不到该类所依赖的类的 .class 文件或者 .java 源文件，就会报 “Cant found sysbol” 的异常错误。 ​ 编译后的字节码文件格式主要分为两部分：常量池和方法字节码。 常量池记录的是代码出现过的（常量、类名、成员变量等）以及符号引用（类引用、方法引用，成员变量引用等） 方法字节码中放的是各个方法的字节码。 2）运行期​ Java 类运行的过程大概分为：类的加载和类的执行。需要说明的一点的是：JVM 并不是在运行时就会把所有使用到的类都加载到内存中，JVM主要在程序第一次运行时主动使用类的时候，才会立即去加载。 ​ 在 Java 中，JVM可以理解的代码就叫做字节码（即扩展名为 .class 的文件），它不面向任何特定的处理器，只面向虚拟机。Java 语言通过字节码的方式，在一定程度上解决了传统解释型语言执行效率低的问题，同时又保留了解释型语言可移植的特点。所以 Java 程序运行时比较高效，而且，由于字节码并不针对一种特定的机器，因此，Java程序无须重新编译便可在多种不同操作系统的计算机上运行。 origin：https://blog.51cto.com/u_15127553/4328152 2.类文件结构以下是.class文件的示例： 123450000000 ca fe ba be 00 00 00 34 00 23 0a 00 06 00 15 09 0000020 00 16 00 17 08 00 18 0a 00 19 00 1a 07 00 1b 07 0000040 00 1c 01 00 06 3c 69 6e 69 74 3e 01 00 03 28 29 0000060 56 01 00 04 43 6f 64 65 01 00 0f 4c 69 6e 65 4e ....... 根据 JVM规范，类文件结构如下 12345678910111213u4 magic //魔数 ca fe ba be，u4代表占4个字节u2 minor_version; u2 major_version; u2 constant_pool_count; cp_info constant_pool[constant_pool_count-1]; u2 access_flags; u2 this_class; u2 super_class; u2 interfaces_count; u2 interfaces[interfaces_count]; u2 fields_count; field_info fields[fields_count]; u2 methods_count; method_info methods[methods_count]; u2 attributes_count; attribute_info attributes[attributes_count]; 魔数u4 magic 对应字节码文件的0~3个字节 0000000 ca fe ba be 00 00 00 34 00 23 0a 00 06 00 15 09 版本u2 minor_version; u2 major_version; 0000000 ca fe ba be 00 00 00 34 00 23 0a 00 06 00 15 09 34H &#x3D; 52，代表JDK8 常量池 3.字节码指令1）图解方法执行流程代码 12345678public class Demo3_1 &#123; public static void main(String[] args) &#123; int a = 10; int b = Short.MAX_VALUE + 1; int c = a + b; System.out.println(c); &#125; &#125; ①常量池载入运行时常量池 注意：常量池也属于方法区，在这里单独提出来了 ②方法字节码载入方法区 （stack&#x3D;2，locals&#x3D;4，.class文件中记录了） 对应操作数栈有2个空间（每个空间4个字节），局部变量表中有4个槽位 ③执行引擎开始执行字节码 bipush 10 将一个 byte 压入操作数栈（每个操作数栈要求4字节，其长度不够会补齐 4 个字节），类似的指令还有 sipush 将一个 short 压入操作数栈（其长度会补齐 4 个字节） ldc 将一个 int 压入操作数栈 ldc2_w 将一个 long 压入操作数栈（分两次压入，因为 long 是 8 个字节） 这里小的数字都是和字节码指令存在一起，超过 short 范围的数字存入了常量池 ③istore 1 将操作数栈栈顶元素弹出，放入局部变量表的slot 1中（也就是a） 对应java代码中的 1a = 10; ④ldc #3 读取运行时常量池中#3，即32768(超过short最大值范围的数会被放到运行时常量池中)，将其加载到操作数栈中 short以内的数字是和字节码指令一起的，超过了Short.MAX_VALUE + 1会存在运行时常量池 注意 Short.MAX_VALUE 是 32767，所以 32768 &#x3D; Short.MAX_VALUE + 1 实际是在编译期间计算好的 ⑤istore 2 将操作数栈中的元素弹出，放到局部变量表的2号位置，即b=Short.MAX_VALUE + 1 ⑥iload1 iload2 将局部变量表中1号位置和2号位置的元素放入操作数栈中 因为只能在操作数栈中执行运算操作 ⑦iadd 将操作数栈中的两个元素弹出栈并相加，结果在压入操作数栈中 ⑧istore 3 将操作数栈中的元素弹出，放入局部变量表的3号位置 ⑨getstatic #4 在运行时常量池中找到#4，发现是一个对象 在堆内存中找到该对象，并将其引用放入操作数栈中 ⑩iload 3 将局部变量表中3号位置的元素压入操作数栈中 ⑪invokevirtual 5 找到常量池 #5 项，定位到方法区 java&#x2F;io&#x2F;PrintStream.println:(I)V 方法， 有了一个新的方法，生成新的栈帧（分配 locals、stack等） 传递参数（32778这是上个栈帧的运算结果），执行新栈帧中的字节码 执行完毕，弹出栈帧 清除 main 操作数栈内容 ⑫return完成 main 方法调用，弹出 main 栈帧，程序结束 2）a++问题①字节码详解1234567891011//从字节码角度分析a++相关题目public class Demo3_2 &#123; public static void main(String[] args) &#123; int a = 10; int b = a++ + ++a + a--; system.out.println(a); //11 system.out.println(b); //34 &#125;&#125; 分析： 注意 iinc 指令是直接在局部变量 slot 上进行运算 a++ 和 ++a 的区别是先执行 iload 还是 先执行 iinc 首先读入10，并把其压入操作数栈 把操作数栈中的10放入局部变量表的slot 1中， 注意a++，首先是将slot 1中的10压入操作数栈，然后在让slot 1中的数+1 ++a，则是让slot 1中的数+1，然后再压入操作数栈中 a–同理，是先在slot 1中-1，然后再压入操作数栈中 最后计算完以后将结果放入slot 2也就是b对应的slot中 ②test12345678910public static void main(String[] args) &#123; int i=0; int x=0; while(i&lt;10) &#123; x = x++; i++; &#125; System.out.println(x); //结果为0 &#125; x&#x3D;x++ 首先将slot 1&#x3D;0压入操作数栈（x&#x3D;0），然后使slot 1&#x3D;0+1 此时slot 1&#x3D;1，操作数栈（x&#x3D;0），最后将操作数栈中的x&#x3D;0重新覆盖掉slot 1中的x&#x3D;1，因此结果最终结果为0 3）构造方法①静态构造方法 cinit()1234567891011121314public class Code_12_CinitTest &#123; static int i = 10; static &#123; i = 20; &#125; static &#123; i = 30; &#125;public static void main(String[] args) &#123; System.out.println(i); // 30&#125; 编译器会按从上至下的顺序，收集所有 static 静态代码块和静态成员赋值的代码，合并为一个特殊的方法 cinit()V ： 12345678stack=1, locals=0, args_size=0 0: bipush 10 2: putstatic #3 // Field i:I 5: bipush 20 7: putstatic #3 // Field i:I 10: bipush 30 12: putstatic #3 // Field i:I 15: return ②非静态构造方法 init()123456789101112131415161718192021222324public class Code_13_InitTest &#123; private String a = &quot;s1&quot;; &#123; b = 20; &#125; private int b = 10; &#123; a = &quot;s2&quot;; &#125; public Code_13_InitTest(String a, int b) &#123; this.a = a; this.b = b; &#125; public static void main(String[] args) &#123; Code_13_InitTest d = new Code_13_InitTest(&quot;s3&quot;, 30); System.out.println(d.a); System.out.println(d.b); &#125;&#125; 编译器会按从上至下的顺序，收集所有 {} 代码块和成员变量赋值的代码，形成新的构造方法init()V，但原始构造方法内的代码总是在后 1234567891011121314151617181920212223stack=2, locals=3, args_size=3 0: aload_0 1: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 4: aload_0 5: ldc #2 // String s1 7: putfield #3 // Field a:Ljava/lang/String; 10: aload_0 11: bipush 20 13: putfield #4 // Field b:I 16: aload_0 17: bipush 10 19: putfield #4 // Field b:I 22: aload_0 23: ldc #5 // String s2 25: putfield #3 // Field a:Ljava/lang/String; // 原始构造方法在最后执行------------------------------------------------- 28: aload_0 29: aload_1 30: putfield #3 // Field a:Ljava/lang/String; 33: aload_0 34: iload_2 35: putfield #4 // Field b:I 38: return 4）方法调用1234567891011121314151617181920212223242526272829public class Demo5 &#123; public Demo5() &#123; &#125; private void test1() &#123; &#125; private final void test2() &#123; &#125; public void test3() &#123; &#125; public static void test4() &#123; &#125; public static void main(String[] args) &#123; Demo5 demo5 = new Demo5(); demo5.test1(); demo5.test2(); demo5.test3(); Demo5.test4(); &#125;&#125; 不同方法在调用时，对应的虚拟机指令有所区别： 私有、构造、被final修饰的方法，在调用时都使用invokespecial指令 普通成员方法在调用时，使用invokevirtual指令。因为编译期间无法确定该方法的内容，只有在运行期间才能确定 静态方法在调用时使用invokestatic指令，如果用对象调用会先让对象进栈再出栈，然后直接通过类调用 1234567891011121314Code: stack=2, locals=2, args_size=1 0: new #2 // class com/nyima/JVM/day5/Demo5 3: dup 4: invokespecial #3 // Method &quot;&lt;init&gt;&quot;:()V 7: astore_1 8: aload_1 9: invokespecial #4 // Method test1:()V 12: aload_1 13: invokespecial #5 // Method test2:()V 16: aload_1 17: invokevirtual #6 // Method test3:()V 20: invokestatic #7 // Method test4:()V 23: returnCopy new 是创建【对象】，给对象分配堆内存，执行成功会将【对象引用】压入操作数栈 dup 是赋值操作数栈栈顶的内容，本例即为【对象引用】，为什么需要两份引用呢，一个是要配合 invokespecial 调用该对象的构造方法 “init”:()V （会消耗掉栈顶一个引用），另一个要 配合 astore_1 赋值给局部变量 终方法（ﬁnal），私有方法（private），构造方法都是由 invokespecial 指令来调用，属于静态绑定，因为通过类就可以确定 普通成员方法是由 invokevirtual 调用，属于动态绑定，因为不知道是执行父的方法还是子的方法 即支持多态成员方法与静态方法调用的另一个区别是，执行方法前是否需要【对象引用】 补充：https://developer.aliyun.com/article/651630 https://www.csdn.net/tags/MtTaIgwsNDMxMTAxLWJsb2cO0O0O.html 5）多态原理因为普通成员方法需要在运行时才能确定具体的内容，所以虚拟机需要调用invokevirtual指令 在执行invokevirtual指令时，经历了以下几个步骤 先通过栈帧中对象的引用找到对象 分析对象头，找到对象实际的Class Class结构中有vtable，vtable存储了这些普通成员方法的具体地址（父类方法的地址或子类重写方法的地址） 查询vtable找到方法的具体地址 执行方法的字节码 6）异常原理①单个catchtry-catch： 12345678910public class Demo1 &#123; public static void main(String[] args) &#123; int i = 0; try &#123; i = 10; &#125;catch (Exception e) &#123; i = 20; &#125; &#125;&#125; 对应字节码指令： 123456789101112131415Code: stack=1, locals=3, args_size=1 0: iconst_0 1: istore_1 2: bipush 10 4: istore_1 5: goto 12 8: astore_2 9: bipush 20 11: istore_1 12: return //多出来一个异常表 Exception table: from to target type 2 5 8 Class java/lang/ExceptionCopy 可以看到一个 Exception table 的结构，[from, to) 是前闭后开（也就是检测2~4行）的检测范围，一旦这个范围内的字节码执行出现异常，则通过 type 匹配异常类型，如果一致，进入 target 所指示行号 8行的字节码指令 astore_2 是将异常对象引用存入局部变量表的2号位置（就是Exception e中的e） ②多个single-catch123456789101112public class Demo1 &#123; public static void main(String[] args) &#123; int i = 0; try &#123; i = 10; &#125;catch (ArithmeticException e) &#123; i = 20; &#125;catch (Exception e) &#123; i = 30; &#125; &#125;&#125; 对应的字节码 123456789101112131415161718192021222324252627Code: stack=1, locals=3, args_size=1 0: iconst_0 1: istore_1 2: bipush 10 4: istore_1 5: goto 19 8: astore_2 //将捕获到的e存入slot 2 9: bipush 20 11: istore_1 12: goto 19 15: astore_2 //将捕获到的e存入slot 2 16: bipush 30 18: istore_1 19: return Exception table: from to target type 2 5 8 Class java/lang/ArithmeticException 2 5 15 Class java/lang/ExceptionCopy LineNumberTable: ... LocalVariableTable: Start Length Slot Name 9 3 2 e 16 3 2 e 23 3 2 e 0 27 0 args 2 25 1 i 因为异常出现时，只能进入 Exception table 中一个分支，所以局部变量表 slot 2 位置被共用，捕获到的e存入其中 ③finally123456789101112public class Demo2 &#123; public static void main(String[] args) &#123; int i = 0; try &#123; i = 10; &#125; catch (Exception e) &#123; i = 20; &#125; finally &#123; i = 30; &#125; &#125;&#125; 对应字节码 12345678910111213141516171819202122232425262728293031323334Code: stack=1, locals=4, args_size=1 0: iconst_0 1: istore_1 //try块 2: bipush 10 4: istore_1 //try块执行完后，会执行finally 5: bipush 30 7: istore_1 8: goto 27 //catch块 11: astore_2 //异常信息放入局部变量表的2号槽位 12: bipush 20 14: istore_1 //catch块执行完后，会执行finally 15: bipush 30 17: istore_1 18: goto 27 //出现异常，但未被Exception捕获，会抛出其他异常，这时也需要执行finally块中的代码 21: astore_3 22: bipush 30 24: istore_1 25: aload_3 26: athrow //抛出异常 27: return Exception table: from to target type 2 5 11 Class java/lang/Exception 2 5 21 any 11 15 21 any 可以看到 ﬁnally 中的代码被复制了 3 份，分别放入 try 流程，catch 流程以及 catch剩余的异常类型流程 在catch块中也可能会抛出异常，抛出异常后进入any异常块 注意虽然从字节码指令看来，每个块中都有finally块，但是finally块中的代码只会被执行一次 ④finally中的return123456789101112131415161718public class Demo3 &#123; public static void main(String[] args) &#123; int i = Demo3.test(); //结果为20 System.out.println(i); &#125; public static int test() &#123; int i; try &#123; i = 10; return i; &#125; finally &#123; i = 20; return i; &#125; &#125;&#125; 对应字节码 12345678910111213141516171819Code: stack=1, locals=3, args_size=0 0: bipush 10 2: istore_0 3: iload_0 4: istore_1 //暂存返回值10到slot 1 5: bipush 20 7: istore_0 //暂存返回值20到slot 2 8: iload_0 //将返回值20从slot 2压入栈顶 9: ireturn //ireturn会返回操作数栈顶的整型值20 //如果出现异常，还是会执行finally块中的内容，没有抛出异常 10: astore_2 11: bipush 20 13: istore_0 //暂存返回值20到slot 2 14: iload_0 //将返回值20从slot 2压入栈顶 15: ireturn //这里没有athrow了，也就是如果在finally块中如果有返回操作的话，且try块中出现异常，会吞掉异常！ Exception table: from to target type 0 5 10 any 由于 ﬁnally 中的 ireturn 被插入了所有可能的流程，因此返回结果肯定以ﬁnally的为准 字节码中第 2-4，暂存返回值 跟上例中的 ﬁnally 相比，发现没有 athrow 了，这告诉我们：如果在 ﬁnally 中出现了 return，会吞掉异常 所以不要在finally中进行返回操作 ⑤被吞掉的异常1234567891011121314151617181920public class Demo3 &#123; public static void main(String[] args) &#123; int i = Demo3.test(); //最终结果为20 System.out.println(i); &#125; public static int test() &#123; int i; try &#123; i = 10; //这里应该会抛出异常 i = i/0; return i; &#125; finally &#123; i = 20; return i; &#125; &#125;&#125; 会发现打印结果为20，并未抛出异常，而时return了20 ⑥finally不带return123456789101112131415public class Demo4 &#123; public static void main(String[] args) &#123; int i = Demo4.test(); System.out.println(i); &#125; public static int test() &#123; int i = 10; try &#123; return i; &#125; finally &#123; i = 20; &#125; &#125;&#125; 对应字节码： 1234567891011121314151617181920212223Code: stack=1, locals=3, args_size=0 0: bipush 10 2: istore_0 //赋值给i 10 3: iload_0 //加载到操作数栈顶 4: istore_1 //加载到局部变量表的1号位置 5: bipush 20 7: istore_0 //赋值给i 20 8: iload_1 //加载局部变量表1号位置的数10到操作数栈 9: ireturn //返回操作数栈顶元素 10 10: astore_2 11: bipush 20 13: istore_0 14: aload_2 //加载异常 15: athrow //抛出异常 Exception table: from to target type 3 5 10 any LineNumberTable: ... LocalVariableTable: //可以看到局部变量表中没有slot 1 Start Length Slot Name Signature 3 13 0 i I return 中的数会先存到一个无名的slot，return的时候会load该slot，并return该数 7）synchronized代码块test： 12345678910public class Demo5 &#123; public static void main(String[] args) &#123; int i = 10; Lock lock = new Lock(); synchronized (lock) &#123; System.out.println(i); &#125; &#125;&#125; 对应字节码： 1234567891011121314151617181920212223242526272829303132Code: stack=2, locals=5, args_size=1 0: bipush 10 2: istore_1 3: new #2 // class com/nyima/JVM/day06/Lock 6: dup //复制一份，放到操作数栈顶，用于构造函数消耗 7: invokespecial #3 // Method com/nyima/JVM/day06/Lock.&quot;&lt;init&gt;&quot;:()V 10: astore_2 //剩下的一份放到局部变量表的2号位置 11: aload_2 //加载到操作数栈 12: dup //复制一份，放到操作数栈，用于加锁时消耗 13: astore_3 //将操作数栈顶元素弹出，暂存到局部变量表的三号槽位。这时操作数栈中有一份对象的引用 14: monitorenter //加锁 //锁住后代码块中的操作 15: getstatic #4 // Field java/lang/System.out:Ljava/io/PrintStream; 18: iload_1 19: invokevirtual #5 // Method java/io/PrintStream.println:(I)V //加载局部变量表中三号槽位对象的引用，用于解锁 22: aload_3 23: monitorexit //解锁 24: goto 34 //异常操作 27: astore 4 29: aload_3 30: monitorexit //解锁 31: aload 4 33: athrow 34: return //可以看出，无论何时出现异常，都会跳转到27行，将异常放入局部变量中，并进行解锁操作，然后加载异常并抛出异常。 Exception table: from to target type 15 24 27 any 27 31 27 any Lock lock &#x3D; new Lock() new的时候会复制一份引用，消耗掉复制用于构造函数，另一份引用存入slot 2中 取出slot 2中的引用在复制一份，然后放入slot 3存储 上锁操作的monitorenter消耗掉slot 2中的引用 解锁操作取出slot 3中的引用，然后进行释放 这里利用了Exception table 如果代码块中或解锁出现any异常，都会跳转到指定的行 跳转到该行后会取出slot 3的引用，重复进行解锁操作 注意： 以上操作就保证了，当同步代码块中出现异常，或者解锁出现异常，一定能释放锁，不会占用 synchronized方法，即方法级别的synchronized不会再字节码中有所体现 4.编译期处理（语法糖）​ 编译期处理就是所谓的语法糖，其实就是指 java 编译器把 .java 源码编译为 .class 字节码的过程中，自动生成和转换的一些代码，主要是为了减轻程序员的负担。 ​ 注意，以下代码都是.class的等价转化。 1）默认构造器当我们没有重写构造器的时候： 123public class Candy1 &#123;&#125; 经过编译期优化后： 1234567public class Candy1 &#123; // 这个无参构造器是java编译器帮我们加上的 public Candy1() &#123; // 即调用父类 Object 的无参构造方法，即调用 java/lang/Object.&quot; &lt;init&gt;&quot;:()V super(); &#125;&#125; 2）自动拆装箱基本类型和其包装类型的相互转换过程，称为拆装箱。在 JDK 5 以后，它们的转换可以在编译期自动完成 123456public class Candy2 &#123; public static void main(String[] args) &#123; Integer x = 1; int y = x; &#125;&#125; 转换过程如下： 123456789public class Candy2 &#123; public static void main(String[] args) &#123; // 基本类型赋值给包装类型，称为装箱 Integer x = Integer.valueOf(1); // 包装类型赋值给基本类型，称谓拆箱 int y = x.intValue(); &#125;&#125; int-&gt;Integer：Integer.valueOf() Integer-&gt;int：x.intValue(); 3）泛型集合取值①泛型擦除泛型也是在 JDK 5 开始加入的特性，但 java 在编译泛型代码后会执行 泛型擦除 的动作，即泛型信息在编译为字节码之后就丢失了，实际的类型都当做了 Object 类型来处理： 1234567public class Demo3 &#123; public static void main(String[] args) &#123; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); list.add(10); Integer x = list.get(0); &#125;&#125; 对应字节码： 123456789101112131415161718192021Code: stack=2, locals=3, args_size=1 0: new #2 // class java/util/ArrayList 3: dup 4: invokespecial #3 // Method java/util/ArrayList.&quot;&lt;init&gt;&quot;:()V 7: astore_1 8: aload_1 9: bipush 10 11: invokestatic #4 // Method java/lang/Integer.valueOf:(I)Ljava/lang/Integer; //这里进行了泛型擦除，实际调用的是add(Objcet o) 14: invokeinterface #5, 2 // InterfaceMethod java/util/List.add:(Ljava/lang/Object;)Z 19: pop 20: aload_1 21: iconst_0 //这里也进行了泛型擦除，实际调用的是get(Object o) 22: invokeinterface #6, 2 // InterfaceMethod java/util/List.get:(I)Ljava/lang/Object; //这里进行了类型转换，将Object转换成了Integer 27: checkcast #7 // class java/lang/Integer 30: astore_2 31: returnCopy 所以调用get函数取值时，实际上有一个类型转换的操作 1Integer x = (Integer) list.get(0); 如果要将返回结果赋值给一个int类型的变量，则还有自动拆箱的操作 1int x = (Integer) list.get(0).intValue(); 泛型擦除其实就是在字节码层面向集合add或get的时候，集合中的对象都会被当做object add前会擦除泛型，调用add(Objcet o) get实际上是get(Object o) ，然后再进行类型转换 ②可变参数1234567891011public class Demo4 &#123; public static void foo(String... args) &#123; //将args赋值给arr，可以看出String...实际就是String[] String[] arr = args; System.out.println(arr.length); &#125; public static void main(String[] args) &#123; foo(&quot;hello&quot;, &quot;world&quot;);4 &#125;&#125; 可变参数 String… args 其实是一个 String[] args ，从代码中的赋值语句中就可以看出来。 同 样 java 编译器会在编译期间将上述代码变换为： 12345678910111213public class Demo4 &#123; public Demo4 &#123;&#125; public static void foo(String[] args) &#123; String[] arr = args; System.out.println(arr.length); &#125; public static void main(String[] args) &#123; foo(new String[]&#123;&quot;hello&quot;, &quot;world&quot;&#125;); &#125;&#125; 注意： 如果调用的是foo()，即未传递参数时，等价代码为foo(new String[]{})，创建了一个空数组，而不是直接传递的null 4）foreach原始代码： 123456789public class Demo5 &#123; public static void main(String[] args) &#123; //数组赋初值的简化写法也是一种语法糖。 int[] arr = &#123;1, 2, 3, 4, 5&#125;; for(int x : arr) &#123; System.out.println(x); &#125; &#125;&#125; 编译器会帮我们转换为: 1234567891011public class Demo5 &#123; public Demo5 &#123;&#125; public static void main(String[] args) &#123; int[] arr = new int[]&#123;1, 2, 3, 4, 5&#125;; for(int i=0; i&lt;arr.length; ++i) &#123; int x = arr[i]; System.out.println(x); &#125; &#125;&#125; 如果是集合使用foreach： 12345678public class Demo5 &#123; public static void main(String[] args) &#123; List&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4, 5); for (Integer x : list) &#123; System.out.println(x); &#125; &#125;&#125; 集合要使用foreach，需要该集合类实现了Iterable接口，因为集合的遍历需要用到迭代器Iterator 12345678910111213public class Demo5 &#123; public Demo5 &#123;&#125; public static void main(String[] args) &#123; List&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4, 5); //获得该集合的迭代器 Iterator&lt;Integer&gt; iterator = list.iterator(); while(iterator.hasNext()) &#123; Integer x = iterator.next(); System.out.println(x); &#125; &#125;&#125; 5）switch字符串原始代码： 123456789101112131415public class Demo6 &#123; public static void main(String[] args) &#123; String str = &quot;hello&quot;; switch (str) &#123; case &quot;hello&quot; : System.out.println(&quot;h&quot;); break; case &quot;world&quot; : System.out.println(&quot;w&quot;); break; default: break; &#125; &#125;&#125; 在编译器中执行的操作： 123456789101112131415161718192021222324252627282930313233343536373839public class Demo6 &#123; public Demo6() &#123; &#125; public static void main(String[] args) &#123; String str = &quot;hello&quot;; int x = -1; //通过字符串的hashCode+value来判断是否匹配 switch (str.hashCode()) &#123; //hello的hashCode case 99162322 : //再次比较，因为字符串的hashCode有可能相等 if(str.equals(&quot;hello&quot;)) &#123; x = 0; &#125; break; //world的hashCode case 11331880 : if(str.equals(&quot;world&quot;)) &#123; x = 1; &#125; break; default: break; &#125; //用第二个switch在进行输出判断 switch (x) &#123; case 0: System.out.println(&quot;h&quot;); break; case 1: System.out.println(&quot;w&quot;); break; default: break; &#125; &#125;&#125; 在编译期间，单个的 switch 被分为了两个 第一个用来匹配字符串，并给 x 赋值 字符串的匹配用到了字符串的 hashCode ，还用到了 equals 方法 使用 hashCode 是为了提高比较效率，使用 equals 是防止有 hashCode 相等的情况（如 BM 和 C .的hash值就是一样的） 在成功匹配后会得到一个x的值，通过该值来在第二个switch中完成对应操作 第二个用来根据x的值来决定输出语句 6）switch枚举原始代码： 12345678910111213141516171819public class Demo7 &#123; public static void main(String[] args) &#123; SEX sex = SEX.MALE; switch (sex) &#123; case MALE: System.out.println(&quot;man&quot;); break; case FEMALE: System.out.println(&quot;woman&quot;); break; default: break; &#125; &#125;&#125;enum SEX &#123; MALE, FEMALE;&#125; 编译器中执行的代码如下： 12345678910111213141516171819202122232425262728293031323334353637public class Demo7 &#123; /** * 定义一个合成类（仅 jvm 使用，对我们不可见） * 用来映射枚举的 ordinal 与数组元素的关系 * 枚举的 ordinal 表示枚举对象的序号，从 0 开始 * 即 MALE 的 ordinal()=0，FEMALE 的 ordinal()=1 */ static class $MAP &#123; //数组大小即为枚举元素个数，里面存放了case用于比较的数字 static int[] map = new int[2]; static &#123; //ordinal即枚举元素对应所在的位置，MALE为0，FEMALE为1 map[SEX.MALE.ordinal()] = 1; map[SEX.FEMALE.ordinal()] = 2; &#125; &#125; public static void main(String[] args) &#123; SEX sex = SEX.MALE; //将对应位置枚举元素的值赋给x，用于case操作 int x = $MAP.map[sex.ordinal()]; switch (x) &#123; case 1: System.out.println(&quot;man&quot;); break; case 2: System.out.println(&quot;woman&quot;); break; default: break; &#125; &#125;&#125;enum SEX &#123; MALE, FEMALE;&#125;Copy 7）枚举类原始代码： 123enum SEX &#123; MALE, FEMALE;&#125; 转换后的代码： 1234567891011121314151617181920212223242526public final class Sex extends Enum&lt;Sex&gt; &#123; //对应枚举类中的元素 public static final Sex MALE; public static final Sex FEMALE; private static final Sex[] $VALUES; static &#123; //调用构造函数，传入枚举元素的值及ordinal MALE = new Sex(&quot;MALE&quot;, 0); FEMALE = new Sex(&quot;FEMALE&quot;, 1); $VALUES = new Sex[]&#123;MALE, FEMALE&#125;; &#125; //调用父类中的方法 private Sex(String name, int ordinal) &#123; super(name, ordinal); &#125; public static Sex[] values() &#123; return $VALUES.clone(); &#125; public static Sex valueOf(String name) &#123; return Enum.valueOf(Sex.class, name); &#125; &#125; 枚举类其实一个也是一个类，但是不同于普通类，其实例对象的数量是固定的，枚举类中的每一个定义都是一个实例对象 枚举类的static方法在初始化的时候会new这些定义的变量并且分配他们枚举类编号ordinal 枚举类的构造方法是private，这也保证了不能通过构造方法的方式来修改枚举类的信息 枚举类有两个方法 values()，克隆了枚举实例数组并返回 valueOf(String name)：根据枚举类的字符串名称来得到对应的枚举实例 相关：https://www.runoob.com/java/java-enum.html 8）try with resourceDK 7 开始新增了对需要关闭的资源处理的特殊语法，‘try-with-resources’ 12345try(资源变量 = 创建资源对象) &#123; &#125; catch() &#123;&#125; ​ 其中资源对象需要实现 AutoCloseable 接口，例如 InputStream 、 OutputStream 、 Connection 、 Statement 、 ResultSet 等接口 都实现了 AutoCloseable ，使用 try-with- resources 可以不用写 finally 语句块，编译器会帮助生成关闭资源代码，例如： 123456789public class Candy9 &#123; public static void main(String[] args) &#123; try(InputStream is = new FileInputStream(&quot;d:\\\\1.txt&quot;))&#123; //这里new了一个FIS流 System.out.println(is); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; 会被转换为： 123456789101112131415161718192021222324252627282930313233343536public class Candy9 &#123; public static void main(String[] args) &#123; try &#123; InputStream is = new FileInputStream(&quot;d:\\\\1.txt&quot;); //....................以下为添加的代码................................... Throwable t = null; try &#123; System.out.println(is); &#125; catch (Throwable e1) &#123; // t 是我们代码出现的异常 t = e1; throw e1; &#125; finally &#123; // 判断了资源不为空 if (is != null) &#123; // 如果我们代码有异常 if (t != null) &#123; try &#123; is.close(); &#125; catch (Throwable e2) &#123; // 如果 close 出现异常，作为被压制异常添加 t.addSuppressed(e2); &#125; &#125; else &#123; // 如果我们代码没有异常，close 出现的异常就是最后 catch 块中的 e is.close(); &#125; &#125; &#125; //....................以上为添加的代码................................... &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 为什么要设计一个 addSuppressed(Throwable e) （添加被压制异常）的方法呢？ 是为了防止异常信息的丢失（想想 try-with-resources 生成的 fianlly 中如果抛出了异常：如close时抛出异常） 123456789101112131415public class Test6 &#123; public static void main(String[] args) &#123; try (MyResource resource = new MyResource()) &#123; int i = 1/0; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;class MyResource implements AutoCloseable &#123; public void close() throws Exception &#123; throw new Exception(&quot;close 异常&quot;); &#125; &#125; 本例中先会产生一个int i &#x3D; 1&#x2F;0导致的Exception e，然后再close的时候再产生一个new Exception(“close 异常”) 输出： 12345java.lang.ArithmeticException: / by zero //1.第一个异常 at test.Test6.main(Test6.java:7) Suppressed: java.lang.Exception: close 异常 at test.MyResource.close(Test6.java:18) //第二个关闭资源时的异常 at test.Test6.main(Test6.java:6) 可以看到两个异常都没有丢失 9）方法重写时的桥接方法我们都知道，方法重写时对返回值分两种情况： 父子类的返回值完全一致 子类返回值可以是父类返回值的子类（比较绕口，见下面的例子） 123456789101112class A &#123; public Number m() &#123; return 1; &#125; &#125;class B extends A &#123; @Override // 子类 m 方法的返回值是 Integer 是父类 m 方法返回值 Number 的子类 public Integer m() &#123; return 2; &#125; &#125; 注意Number是Integer的父类 对于子类，java 编译器会做如下处理： 12345678910class B extends A &#123; public Integer m() &#123; return 2; &#125; // 此方法才是真正重写了父类 public Number m() 方法 public synthetic bridge Number m() &#123; // 调用 public Integer m() return m(); &#125; &#125; JVM生成了合成桥接方法，在其内部调用public Integer m() &#123; &#125;，因为Integer是Number的子类，所以这也是符合要求的 其中桥接方法比较特殊，仅对 java 虚拟机可见，并且与原来的 public Integer m() 没有命名冲突，可以 用下面反射代码来验证： 12345public static void main(String[] args) &#123; for(Method m : B.class.getDeclaredMethods()) &#123; System.out.println(m); &#125; &#125; 10）匿名内部类①普通情况12345678910public class Candy10 &#123; public static void main(String[] args) &#123; Runnable runnable = new Runnable() &#123; @Override public void run() &#123; System.out.println(&quot;running...&quot;); &#125; &#125;; &#125;&#125; 转换后的代码： 123456public class Candy10 &#123; public static void main(String[] args) &#123; // 用额外创建的类来创建匿名内部类对象 Runnable runnable = new Candy10$1(); &#125;&#125; 123456789// 创建了一个额外的类，实现了 Runnable 接口final class Candy10$1 implements Runnable &#123; public Demo8$1() &#123;&#125; @Override public void run() &#123; System.out.println(&quot;running...&quot;); &#125;&#125; ②传入final 参数引用局部变量的匿名内部类，源代码： 12345678910public class Candy11 &#123; public static void test(final int x) &#123; //必须要传入final变量 Runnable runnable = new Runnable() &#123; @Override public void run() &#123; System.out.println(&quot;ok:&quot; + x); &#125; &#125;; &#125; &#125; 转换后代码： 12345678910// 额外生成的类 final class Candy11$1 implements Runnable &#123; int val$x; //传递进的final参数作为了成员变量 Candy11$1(int x) &#123; this.val$x = x; &#125; public void run() &#123; System.out.println(&quot;ok:&quot; + this.val$x); &#125; &#125; 12345public class Candy11 &#123; public static void test(final int x) &#123; Runnable runnable = new Candy11$1(x); &#125; &#125; 这解释了为什么匿名内部类引用局部变量时，局部变量必须是 final 的： 因为在创建 Candy11$1 对象时，将 x 的值赋值给了 Candy11$1 对象的 值后，如果不是 final 声明的 x 值发生了改变，匿名内部类则值不一致。 即final保证了匿名内部类中的成员变量x的值，和外面的是一样的 5.类加载阶段1）加载 将类的字节码（即.class文件）载入方法区（1.8后为元空间，在本地内存中）中，内部采用 C++ 的 instanceKlass 描述 java 类，它的重要 ﬁeld 有： _java_mirror 即 java 的类镜像，例如对 String 来说，它的镜像类就是 String.class，作用是把 klass 暴露给 java 使用 _super 即父类 _ﬁelds 即成员变量 _methods 即方法 _constants 即常量池 _class_loader 即类加载器 _vtable 虚方法表：多态 _itable 接口方法 如果这个类还有父类没有加载，先加载父类 加载和链接可能是交替运行的 instanceKlass保存在方法区。JDK 8以后，方法区位于元空间中，而元空间又位于本地内存中 java_mirror（即我们java代码中的String.class等）则是保存在堆内存中 InstanceKlass和*.class(JAVA镜像类)互相保存了对方的地址 instanceKlass、java_mirror（如String.class）、对象三者间的区别 instanceKlass具体描述了一个类的所有信息，是C++描述的 _java_mirror（如String.class）相当于一个桥梁，InstanceKlass和*.class(JAVA镜像类)互相保存了对方的地址。在java程序中我们想知道类的相关信息，可以通过java_mirror（如String.class）来找到InstanceKlass并且以java的方式获得类中的信息 类的对象在对象头中保存了*.class的地址。让对象可以通过其找到方法区中的instanceKlass，从而获取类的各种信息 2）链接①验证 验证类是否符合 JVM规范，安全性检查 根据 JVM规范，类文件结构如下： 12345678910111213u4 magic //魔数 ca fe ba be，u4代表占4个字节u2 minor_version; u2 major_version; u2 constant_pool_count; cp_info constant_pool[constant_pool_count-1]; u2 access_flags; u2 this_class; u2 super_class; u2 interfaces_count; u2 interfaces[interfaces_count]; u2 fields_count; field_info fields[fields_count]; u2 methods_count; method_info methods[methods_count]; u2 attributes_count; attribute_info attributes[attributes_count]; 可以看到.class文件首先就是magic魔术，并且是ca fe ba be才是正确的java的.class文件 在链接中的验证，就会验证加载的.class是否满足JVM的规范 ②准备为 static 变量分配空间，设置默认值 static变量在JDK 7以前是存储与instanceKlass末尾。但在JDK 7以后就存储在_java_mirror末尾了 instanceKlass在方法区，_java_mirror在heap中， static变量在分配空间和赋值是在两个阶段完成的：一般来说，分配空间在准备阶段完成，赋值在初始化阶段完成 如果 static 变量是 ﬁnal 的基本类型，以及字符串常量，那么编译阶段值就确定了，赋值在准备阶段完成 如static final String &quot;hello&quot;， static final int 1，这样的常量在编译阶段就能够确定了 如static final Integer 1就不能在编译阶段确定，因为还涉及到自动装箱，只能在初始化阶段完成 如果 static 变量是 ﬁnal 的，但属于引用类型，那么赋值也会在初始化阶段完成 如static final Object = new Object()； 1234567public class Load8 &#123; static int a; //加载阶段分配空间 static int b = 10; //加载阶段分配空间，初始化阶段赋值 static final int c = 20; //final常量，编译阶段就把值确定了，准备期间就已经赋完值了 static final string d = &quot;he1lo&quot;; //final常量，编译阶段就把值确定了，准备期间就已经赋完值了 static final 0bject e = new Object();//final的引用类型，赋值在初始化阶段完成&#125; ③解析解析的含义 将常量池中的符号引用解析为直接引用 未解析时，常量池中的看到的对象仅是符号，未真正的存在于内存中 123456789101112131415161718192021public class Demo1 &#123; public static void main(String[] args) throws IOException, ClassNotFoundException &#123; //1.只加载不解析 ClassLoader loader = Demo1.class.getClassLoader(); Class&lt;?&gt; c = loader.loadClass(&quot;com.nyima.JVM.day8.C&quot;); //2.加载、链接（验证、准备、解析）、初始化都会执行 new C(); //用于阻塞主线程 System.in.read(); &#125;&#125;class C &#123; D d = new D();&#125;class D &#123;&#125; ClassLoader loader &#x3D; Demo1.class.getClassLoader();Class&lt;?&gt; c &#x3D; loader.loadClass(“com.nyima.JVM.day8.C”); 通过类加载器加载，不会解析 查看类C的常量池，可以看到类D未被解析，只是存在于常量池中的符号 new C(); 加载、链接（验证、准备、解析）、初始化都会执行 3）初始化①基本概念初始化阶段就是执行类构造器clinit()方法的过程，静态代码块会执行，虚拟机会保证这个类的『构造方法』的线程安全 clinit()方法是由编译器自动收集类中的所有类变量的赋值动作和静态语句块（static{}块）中的语句合并产生的 注意： ​ 编译器收集的顺序是由语句在源文件中出现的顺序决定的，静态语句块中只能访问到定义在静态语句块之前的变量，定义在它之后的变量，在前面的静态语句块可以赋值，但是不能访问，如 ②发生时机类的初始化的懒惰的，以下情况会初始化： main 方法所在的类，总会被首先初始化 首次访问这个类的静态变量或静态方法时 static int a = 6; 子类初始化，如果父类还没初始化，会引发 执行A，那么B也会初始化 子类访问父类的静态变量，只会触发父类的初始化 System.out.println(B.a); Class.forName，默认第二个参数是true，也就是要初始化 new 会导致初始化 new B()； 以下情况不会初始化： 访问类的 static ﬁnal 静态常量（基本类型和字符串） 如static final String &quot;hello&quot;， static final Integer 1，在编译器已经确定，准备阶段已经赋值 如static final Integer 1就不能在编译阶段确定，因为还涉及到自动装箱，只能在初始化阶段完成 类对象.class 不会触发初始化 sout（A.class），但A中的静态代码块不会执行，_java_mirror在加载阶段就生成好了 创建该类对象的数组 System.out.println(new B[0]); 类加载器的.loadClass方法 只会进行加载阶段 Class.forName的参数2为false时 参数2代表是否需要初始化，默认为true，false是不要初始化 验证类是否被初始化，可以看该类的静态代码块是否被执行 测试代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class T1 &#123; static &#123; System.out.println(&quot;main init&quot;); &#125; public static void main(String[] args) throws ClassNotFoundException &#123; /** * 不会初始化的情况： */ // 1.静态常量不会触发初始化 System.out.println(B.b); // 2.类对象.class不会触发初始化 System.out.println(B.class); // 3.创建该类的数组不会触发初始化 System.out.println(new B[0]); // 4.不会初始化类B，但会加载B、A\\ ClassLoader c1 = Thread.currentThread().getContextClassLoader(); c1.loadClass(&quot;testAll.load3.B&quot;); // 5.不会初始化类B，但会加载B、A ClassLoader c2 = Thread.currentThread().getContextClassLoader(); Class.forName(&quot;testAll.load3.B&quot;, false, c2); // 1.首次访问这个类的静态变量或静态方法时 System.out.println(A.a); // 2.子类初始化，如果父类还没初始化，会引岗 System.out.println(B.c); // 3.子类访问父类静态变量，只触发父类初始化 System.out.println(B.a); // 4.会初始化类B，并先初始化类A Class.forName(&quot;testAll.B&quot;); &#125;&#125;class A&#123; static int a = 6; static &#123; System.out. println(&quot;a init&quot;); &#125;&#125;class B extends A &#123; final static double b = 5.0; static boolean c = false; static &#123; System.out.println(&quot;b init&quot;); &#125;&#125; 4）application①test12345678910111213141516171819public class Load2 &#123; public static void main(String[] args) &#123; System.out.println(E.a);//不会初始化 System.out.println(E.b);//不会初始化 // 会导致 E 类初始化，因为 Integer 是包装类 //涉及到自动装箱，编译器无法确定，需要到初始化阶段才行 System.out.println(E.c);//会初始化 &#125;&#125;class E &#123; public static final int a = 10; public static final String b = &quot;hello&quot;; public static final Integer c = 20; static &#123; System.out.println(&quot;E cinit&quot;); &#125;&#125; ②懒惰初始化单例模式1234567891011121314class Singleton &#123; private Singleton()&#123;&#125;; private static class LazyHolder&#123; private static final Singleton SINGLETON= new Singleton(); static&#123; System.out.println(&quot;lazyHolder init...&quot;); &#125; &#125; public static Singleton getInstance()&#123; return LazyHolder.SINGLETON; &#125;&#125; 通过单例类的内部类来new单例对象 线程安全问题是可以保证的，因为类加载器可以保证构造方法的线程安全 调用getInstance方法才会使用到LazyHolder这个类 然后该类才会初始化SINGLETON对象，类加载器保证了构造方法的线程安全 如果只用到Singleton是不会初始化的 6.类加载器1）基本概念类加载器虽然只用于实现类的加载动作，但它在Java程序中起到的作用却远超类加载阶段 对于任意一个类，都必须由加载它的类加载器和这个类本身一起共同确立其在Java虚拟机中的唯一性，每一个类加载器，都拥有一个独立的类名称空间。这句话可以表达得更通俗一些：比较两个类是否“相等”，只有在这两个类是由同一个类加载器加载的前提下才有意义，否则，即使这两个类来源于同一个Class文件，被同一个Java虚拟机加载，只要加载它们的类加载器不同，那这两个类就必定不相等 名称 加载的类 说明 Bootstrap ClassLoader（启动类加载器） JAVA_HOME&#x2F;jre&#x2F;lib 无法直接访问 Extension ClassLoader(拓展类加载器) JAVA_HOME&#x2F;jre&#x2F;lib&#x2F;ext 上级为Bootstrap，显示为nul Application ClassLoader(应用程序类加载器) classpath 上级为Extension 自定义类加载器 自定义 上级为Application Extension ClassLoader(拓展类加载器)在getParent的时候会打印一个null，因为Bootstrap ClassLoader（启动类加载器）并不是java代码写的，是c++代码写的，不允许java代码访问 可通过在控制台输入指令，使得类被启动类加器加载 如果classpath和JAVA_HOME&#x2F;jre&#x2F;lib&#x2F;ext 下有同名类，加载时会使用拓展类加载器加载。当应用程序类加载器发现拓展类加载器已将该同名类加载过了，则不会再次加载 2）双亲委派模式​ 在加载一个类的时候，会从下往上查询看是和否有上级加载器加载过该类，如果加载过了，下级就不会再加载了。 源码分析： 12345678910111213141516171819202122232425262728293031323334353637383940protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException&#123; synchronized (getClassLoadingLock(name)) &#123; // 首先查找该类是否已经被该类加载器加载过了（从该类加载器的缓存中查找） Class&lt;?&gt; c = findLoadedClass(name); // 如果没有被加载过 if (c == null) &#123; long t0 = System.nanoTime(); try &#123; // 1.如果父类是null，那么说明现在是扩展类加载器，父类因为是启动加载器（c++写的），所以是null if (parent != null) &#123; //2.调用父类加载器的loadClass方法 c = parent.loadClass(name, false); &#125; else &#123; // 3.如果父类是启动加载器，那么就会执行该方法，因为parent是null c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader //捕获异常，但不做任何处理，继续执行下面代码 &#125; if (c == null) &#123; // 如果还是没有找到，先让拓展类加载器调用 findClass 方法去找到该类，如果还是没找到，就抛出异常 long t1 = System.nanoTime(); //4.根据本类定义的方式去找，找不到会抛出异常回到上一级的catch中 c = findClass(name); // 记录时间 sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; return c; &#125;&#125; 首先类加载器会在自己的缓存中寻找，如果没有找到，则会调用父类的loadClass方法：parent.loadClass 直到this是扩展类加载器，得到的parent就是null，那么就会执行findBootstrapClassOrNull方法 如果所有缓存中没找到（此时已经到启动类加载器了），就会根据该类加载器的规则来寻找 如启动类加载器，就会从JAVA_HOME&#x2F;jre&#x2F;lib下找 如扩展类加载器，就会从JAVA_HOME&#x2F;jre&#x2F;lib&#x2F;ext下找 如我们写的Stu类，最后就回被应用类加载器加载，因为会在classpath路径下找到 sun.misc.Launcher$AppClassLoader &#x2F;&#x2F;1 处， 开始查看已加载的类，结果没有 sun.misc.Launcher$AppClassLoader &#x2F;&#x2F; 2 处，委派上级 sun.misc.Launcher$ExtClassLoader.loadClass() sun.misc.Launcher$ExtClassLoader &#x2F;&#x2F; 1 处，查看已加载的类，结果没有 sun.misc.Launcher$ExtClassLoader &#x2F;&#x2F; 3 处，没有上级了，则委派 BootstrapClassLoader 查找 BootstrapClassLoader 是在 JAVA_HOME&#x2F;jre&#x2F;lib 下找 H 这个类，显然没有 sun.misc.Launcher$ExtClassLoader &#x2F;&#x2F; 4 处，调用自己的 findClass 方法，是在 JAVA_HOME&#x2F;jre&#x2F;lib&#x2F;ext 下找 H 这个类，显然没有，抛出异常，回到 sun.misc.Launcher$AppClassLoader 的 2 处 继续执行到 sun.misc.Launcher$AppClassLoader &#x2F;&#x2F; 4 处，调用它自己的 findClass 方法，在classpath 下查找，找到了 3）线程上下文类加载器我们在使用 JDBC 时，不写 1Class.forName(&quot;com.mysql.jdbc.Driver&quot;) 也是可以让 com.mysql.jdbc.Driver 正确加载的，你知道是怎么做的吗？ 让我们追踪一下源码： 123456789public class DriverManager &#123; // 注册驱动的集合 private final static CopyOnWriteArrayList&lt;DriverInfo&gt; registeredDrivers = new CopyOnWriteArrayList&lt;&gt;(); // 初始化驱动 static &#123; loadInitialDrivers(); //在这里加载com.mysql.jdbc.Driver println(&quot;JDBC DriverManager initialized&quot;); &#125; 先不看别的，看看 DriverManager 的类加载器： 1System.out.println(DriverManager.class.getClassLoader()); 打印 null，表示它的类加载器是 Bootstrap ClassLoader，会到 JAVA_HOME&#x2F;jre&#x2F;lib 下搜索类，但JAVA_HOME&#x2F;jre&#x2F;lib 下显然没有 mysql-connector-java-5.1.47.jar 包，这样问题来了，在DriverManager 的静态代码块中，怎么能正确加载 com.mysql.jdbc.Driver 呢？ 继续看 loadInitialDrivers() 方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445private static void loadInitialDrivers() &#123; String drivers; try &#123; drivers = AccessController.doPrivileged(new PrivilegedAction&lt;String&gt; () &#123; public String run() &#123; return System.getProperty(&quot;jdbc.drivers&quot;); &#125; &#125;); &#125; catch (Exception ex) &#123; drivers = null; &#125; // 1）使用 ServiceLoader 机制加载驱动，即 SPI AccessController.doPrivileged(new PrivilegedAction&lt;Void&gt;() &#123; public Void run() &#123; ServiceLoader&lt;Driver&gt; loadedDrivers = ServiceLoader.load(Driver.class); Iterator&lt;Driver&gt; driversIterator = loadedDrivers.iterator(); try&#123; while(driversIterator.hasNext()) &#123; driversIterator.next(); &#125; &#125; catch(Throwable t) &#123; // Do nothing &#125; return null; &#125; &#125;); println(&quot;DriverManager.initialize: jdbc.drivers = &quot; + drivers); // 2）使用 jdbc.drivers 定义的驱动名加载驱动 if (drivers == null || drivers.equals(&quot;&quot;)) &#123; return; &#125; String[] driversList = drivers.split(&quot;:&quot;); println(&quot;number of Drivers:&quot; + driversList.length); for (String aDriver : driversList) &#123; try &#123; println(&quot;DriverManager.Initialize: loading &quot; + aDriver); // 这里的 ClassLoader.getSystemClassLoader() 就是应用程序类加载器 Class.forName(aDriver, true, ClassLoader.getSystemClassLoader()); &#125; catch (Exception ex) &#123; println(&quot;DriverManager.Initialize: load failed: &quot; + ex); &#125; &#125; &#125; ​ 先看 2）发现它最后是使用 Class.forName 完成类的加载和初始化，关联的是应用程序类加载器，因此可以顺利完成类加载，破坏了双亲委派机制，直接让应用程序类加载器加载。 ​ 再看 1）它就是大名鼎鼎的 Service Provider Interface （SPI） 约定如下： ​ 在 jar 包的 META-INF&#x2F;services 包下，以接口全限定名为文件名，文件内容是实现类名称 这样就可以使用 12345ServiceLoader&lt;接口类型&gt; allImpls = ServiceLoader.load(接口类型.class); Iterator&lt;接口类型&gt; iter = allImpls.iterator(); while(iter.hasNext()) &#123; iter.next(); &#125; SPI通过接口来得到实现类，体现的是【面向接口编程+解耦】的思想，在下面一些框架中都运用了此思想： JDBC Servlet 初始化器 Spring 容器 Dubbo（对 SPI 进行了扩展） 接着看 ServiceLoader.load 方法： 12345public static &lt;S&gt; ServiceLoader&lt;S&gt; load(Class&lt;S&gt; service) &#123; // 获取线程上下文类加载器 ClassLoader cl = Thread.currentThread().getContextClassLoader(); return ServiceLoader.load(service, cl); &#125; 线程上下文类加载器是当前线程使用的类加载器，默认就是应用程序类加载器，它内部又是由Class.forName 调用了线程上下文类加载器完成类加载，具体代码在 ServiceLoader 的内部类LazyIterator 中： 1234567891011121314151617181920212223private S nextService() &#123; if (!hasNextService()) throw new NoSuchElementException(); String cn = nextName; nextName = null; Class&lt;?&gt; c = null; try &#123; // loader即线程上下文类加载器 c = Class.forName(cn, false, loader); &#125; catch (ClassNotFoundException x) &#123; fail(service,&quot;Provider &quot; + cn + &quot; not found&quot;); &#125; if (!service.isAssignableFrom(c)) &#123; fail(service, &quot;Provider &quot; + cn + &quot; not a subtype&quot;); &#125;try &#123; Sp = service.cast(c.newInstance()); providers.put(cn, p); return p; &#125; catch (Throwable x) &#123; fail(service, &quot;Provider &quot; + cn + &quot; could not be instantiated&quot;, x); &#125; throw new Error(); // This cannot happen &#125; 总结： SPI内部首先获得了线程上下文加载器（默认是应用加载器），然后通过线程上下文加载器来加载对应的类，具体也是使用了forName的方式来加载类 DriverManager的加载器是启动加载器，但是其内部应用了SPI，所以内部使用的是线程上下文加载器，也就是默认的应用加载器，来加载com.mysql.jdbc.Driver，打破了双亲委派机制 4）自定义类加载器①使用场景 想加载非 classpath 随意路径中的类文件 通过接口来使用实现，希望解耦时，常用在框架设计 这些类希望予以隔离，不同应用的同名类都可以加载，不冲突，常见于 tomcat 容器 ②步骤 继承 ClassLoader 父类 要遵从双亲委派机制，重写 ﬁndClass 方法，也就是在什么路径中去找 不是重写 loadClass 方法，否则不会走双亲委派机制 读取类文件的字节码 调用父类的 deﬁneClass 方法来加载类 使用者调用该类加载器的 loadClass 方法 ③example自定义加载器： 12345678910111213141516171819class MyClassLoader extends ClassLoader &#123; @Override // name_就是类名称 protected Class&lt;?&gt; findclass(String name) throws ClassNotFoundException &#123; String path = &quot;e:\\\\myclasspath\\\\&quot; + name + &quot;.class&quot;;//这个路径下的class都会走自定义 try &#123; ByteArrayOutputStream os = new ByteArrayOutputStream(); Files.copy(Paths.get(path), os); //得到字节数组 byte[] bytes = os.toByteArray(); //byte[] -&gt; *.class return defineClass(name, bytes, 0, bytes.length); &#125;catch (IOException e)&#123; e.printStackTrace(); throw new ClassNotFoundException(&quot;类文件未到&quot;, e); &#125; &#125; &#125; 1234567891011121314public class Load7 &#123; public static void main(String[] args) throws Exception &#123; MyClassLoader classLoader = new MyclassLoader(); Class&lt;?&gt; c1 = classLoader.loadClass(&quot;MapImpl1&quot;); Class&lt;?&gt; c2 = classLoader. loadClass(&quot;MapImp1&quot;); //true,无论加载同一个.class多少次，得到的_java_mirror都是一样的 System.out.println(c1 == c2); MyclassLoader classLoader2 = new MyClassLoader(); Class&lt;?&gt; c3 = classLoader2.loadClass(&quot;HapImpl1&quot;); System.out.println(c1 == c3);//false,两者不是同一个类加载器加载的 c1.newInstance(); &#125;&#125; ④破坏双亲委派模式 双亲委派模型的第一次“被破坏”其实发生在双亲委派模型出现之前——即JDK1.2面世以前的“远古”时代 建议用户重写findClass()方法，在类加载器中的loadClass()方法中也会调用该方法 双亲委派模型的第二次“被破坏”是由这个模型自身的缺陷导致的 如果有基础类型又要调用回用户的代码，此时也会破坏双亲委派模式 双亲委派模型的第三次“被破坏”是由于用户对程序动态性的追求而导致的 这里所说的“动态性”指的是一些非常“热”门的名词：代码热替换（Hot Swap）、模块热部署（Hot Deployment）等 例：tomcat打破双亲委派机制 tomcat本身也是一个java的应用，也会用到一些jar包A 2.5，而部署的应用也要用到jar包 3.5，这样两个jar的版本就不同，但是仍然是同名，如果走双亲委派，让同一个类加载器加载，就会出现jar包覆盖而报错 如果部署了多个应用，如两个应用中分别有同名的两个servlet，那么在加载了第一个后，第二个加载的时候就会去加载缓存中查，查到了直接返回，这是不可接受的，所以需要做到web应用间的隔离 多个web应用可能会用到同样的jar包，如spring3.5，那么这部分公共内容应该公用，以免多次加载 详见：https://cloud.tencent.com/developer/article/1890187 7.运行期优化1）分层编译JVM 将执行状态分成了 5 个层次： 0层：解释执行，用解释器将字节码翻译为机器码 1层：使用 C1 即时编译器编译执行（不带 proﬁling） 2层：使用 C1 即时编译器编译执行（带基本的profiling） 3层：使用 C1 即时编译器编译执行（带完全的profiling） 4层：使用 C2 即时编译器编译执行 proﬁling 是指在运行过程中收集一些程序执行状态的数据，例如【方法的调用次数】，【循环的 回边次数】等 即时编译器（JIT）与解释器的区别 解释器 将字节码解释为机器码，下次即使遇到相同的字节码，仍会执行重复的解释 是将字节码解释为针对所有平台都通用的机器码 即时编译器 将一些字节码编译为机器码，并存入 Code Cache，下次遇到相同的代码，直接执行，无需再编译 根据平台类型，生成平台特定的机器码 ​ 对于大部分的不常用的代码，我们无需耗费时间将其编译成机器码，而是采取解释执行的方式运行；另一方面，对于仅占据小部分的热点代码，我们则可以将其编译成机器码，以达到理想的运行速度。 执行效率上简单比较一下 Interpreter &lt; C1 &lt; C2，总的目标是发现热点代码（hotspot名称的由 来），并优化这些热点代码。 2）逃逸分析先来看个例子 123456789101112public class T01_RunTime_EscapeAnalysis &#123; public static void main(String[] args) &#123; for (int i = 0; i &lt; 200; i++) &#123; long start = System.nanoTime(); for (int j = 0; j &lt; 1000; j++) &#123; new Object(); // 循环创建对象 &#125; long end = System.nanoTime(); System.out.printf(&quot;%d\\t%d\\n&quot;, i, (end - start)); &#125; &#125;&#125; 执行时间片段如下： 12345678910111213141516171819200 868611 895572 683333 640214 672925 63446-------------67 2236968 4450169 3054570 1796371 21750-------------195 13104196 14453197 23107198 13574199 14051 首先第一次优化是针对热点代码的优化，由于new Object(); // 循环创建对象执行频率较高，所以JVM针对其做了优化，存储在了code cache中，在后面要执行相同的代码的时候不再解释，而是使用即时编译器直接执行 第二次用到了逃逸分析的策略，因为JVM分析出该对象只是new出来了，在整个环境中根本没有被使用，对象逃逸了，所以JVM后面就直接不再new对象了 可以使用 -XX:-DoEscapeAnalysis 关闭，默认打开 3）方法内联（Inlining） 12345private static int square(final int i) &#123; return i * i; &#125;System.out.println(square(9)); 如果发现 square 是热点方法，并且长度不太长时，会进行内联，所谓的内联就是把方法内代码拷贝、粘贴到调用者的位置： 1System.out.println(9 * 9); 还能够进行常量折叠（constant folding）的优化 1System.out.println(81); test： 1234567891011121314151617181920public class JIT2 &#123; // -XX:+UnlockDiagnosticVMOptions -XX:+PrintInlining （解锁隐藏参数）打印 inlining 信息 // -XX:CompileCommand=dontinline,*JIT2.square 禁止某个方法 inlining // -XX:+PrintCompilation 打印编译信息 public static void main(String[] args) &#123; int x = 0; for (int i = 0; i &lt; 500; i++) &#123; long start = System.nanoTime(); for (int j = 0; j &lt; 1000; j++) &#123; x = square(9); &#125; long end = System.nanoTime(); System.out.printf(&quot;%d\\t%d\\t%d\\n&quot;,i,x,(end - start)); &#125; &#125; private static int square(final int i) &#123; return i * i; &#125; &#125; 在执行的过程中，JVM发现square是hot spot代码，所以会针对其进行优化 因为方法较简单，可以直接复制一份在调用处，这样 x &#x3D; 9*9 又因为这里的传参一直都是9，所以结果也一致，最后会直接使x&#x3D;81 4）循环中的字段优化利用JMH观察，JMH 基准测试请参考： http://openjdk.java.net/projects/code-tools/jmh/ 创建 maven 工程，添加依赖如下 12345678910&lt;dependency&gt; &lt;groupId&gt;org.openjdk.jmh&lt;/groupId&gt; &lt;artifactId&gt;jmh-core&lt;/artifactId&gt; &lt;version&gt;1.21&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.openjdk.jmh&lt;/groupId&gt; &lt;artifactId&gt;jmh-generator-annprocess&lt;/artifactId&gt; &lt;version&gt;1.21&lt;/version&gt;&lt;/dependency&gt; 字段优化代码示例如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// 运行期优化 —— 字段优化// 热身，先热身再优化@Warmup(iterations = 5, time = 1)// 5轮测试@Measurement(iterations = 5, time = 1)@State(Scope.Benchmark)public class T03_RunTime_FieldOptimize &#123;int[] elements = randomInts(1_000); private static int[] randomInts(int size) &#123; ThreadLocalRandom random = ThreadLocalRandom.current(); int[] values = new int[size]; for (int i = 0; i &lt; size; i++) &#123; values[i] = random.nextInt(); &#125; return values;&#125; @Benchmark //要测试的方法public void test1() &#123; for (int i = 0; i &lt; elements.length; i++) &#123; doSum(elements[i]); &#125;&#125; @Benchmarkpublic void test2() &#123; int[] local = this.elements; for (int i = 0; i &lt; elements.length; i++) &#123; doSum(elements[i]); &#125;&#125; @Benchmarkpublic void test3() &#123; for (int element : elements) &#123; doSum(element); &#125;&#125; static int sum = 0; @CompilerControl(CompilerControl.Mode.INLINE) // 控制调用方法时是不是要进行方法内联；允许内联static void doSum(int x) &#123; sum += x;&#125; public static void main(String[] args) throws RunnerException &#123; Options opt = new OptionsBuilder() .include(T03_RunTime_FieldOptimize.class.getSimpleName()) .forks(1) .build(); new Runner(opt).run(); &#125;&#125; ①test1 123456@Benchmark //要测试的方法public void test1() &#123; for (int i = 0; i &lt; elements.length; i++) &#123; doSum(elements[i]); &#125;&#125; 在允许方法内联的情况下，这里会在生成一个本地局部变量，不再去堆中读取成员变量（可能在读取了多次成员变量后才会生成本地局部变量） ②test2 1234567@Benchmarkpublic void test2() &#123; int[] local = this.elements; for (int i = 0; i &lt; elements.length; i++) &#123; doSum(elements[i]); &#125;&#125; 实际上就是手动添加了一个本地局部变量，只会去堆中读取一次成员变量 ③test3 123456@Benchmarkpublic void test3() &#123; for (int element : elements) &#123; doSum(element); &#125;&#125; foreach针对集合就是iterate遍历，针对数组fori遍历，字节码其实适合test2一致的 5）反射优化1234567891011public class Reflect1 &#123; public static void foo() &#123; System.out.println(&quot;foo...&quot;); &#125; public static void main(String[] args) throws NoSuchMethodException, InvocationTargetException, IllegalAccessException &#123; Method foo = Demo3.class.getMethod(&quot;foo&quot;); for(int i = 0; i&lt;=16; i++) &#123; foo.invoke(null); &#125; &#125; foo.invoke 前面 0 ~ 15 次调用使用的是 MethodAccessor 的 NativeMethodAccessorImpl 实现的 由于是native方法执行的，所以效率较低 invoke 方法源码 123456789101112131415161718@CallerSensitivepublic Object invoke(Object obj, Object... args) throws IllegalAccessException, IllegalArgumentException, InvocationTargetException&#123; if (!override) &#123; if (!Reflection.quickCheckMemberAccess(clazz, modifiers)) &#123; Class&lt;?&gt; caller = Reflection.getCallerClass(); checkAccess(caller, clazz, obj, modifiers); &#125; &#125; //MethodAccessor是一个接口，有3个实现类，其中有一个是抽象类 MethodAccessor ma = methodAccessor; // read volatile if (ma == null) &#123; ma = acquireMethodAccessor(); &#125; return ma.invoke(obj, args);&#125; acquireMethodAccessor会由 DelegatingMehodAccessorImpl 去调用 NativeMethodAccessorImplNativeMethodAccessorImpl 源码 123456789101112131415161718192021222324class NativeMethodAccessorImpl extends MethodAccessorImpl &#123; private final Method method; private DelegatingMethodAccessorImpl parent; private int numInvocations;NativeMethodAccessorImpl(Method var1) &#123; this.method = var1;&#125;//每次进行反射调用，会让numInvocation与ReflectionFactory.inflationThreshold的值（15）进行比较，并使使得numInvocation的值加一//如果numInvocation&gt;ReflectionFactory.inflationThreshold，则会调用本地方法invoke0方法public Object invoke(Object var1, Object[] var2) throws IllegalArgumentException, InvocationTargetException &#123; if (++this.numInvocations &gt; ReflectionFactory.inflationThreshold() &amp;&amp; !ReflectUtil.isVMAnonymousClass(this.method.getDeclaringClass())) &#123; MethodAccessorImpl var3 = (MethodAccessorImpl)(new MethodAccessorGenerator()).generateMethod(this.method.getDeclaringClass(), this.method.getName(), this.method.getParameterTypes(), this.method.getReturnType(), this.method.getExceptionTypes(), this.method.getModifiers()); this.parent.setDelegate(var3); &#125; return invoke0(this.method, var1, var2);&#125;void setParent(DelegatingMethodAccessorImpl var1) &#123; this.parent = var1;&#125;private static native Object invoke0(Method var0, Object var1, Object[] var2); 12//ReflectionFactory.inflationThreshold()方法的返回值private static int inflationThreshold = 15; 一开始if条件不满足，就会调用本地方法 invoke0 随着 numInvocation 的增大，当它大于 ReflectionFactory.inflationThreshold 的值 16 时，就会本地方法访问器替换为一个运行时动态生成的访问器，来提高效率 这时会从反射调用变为正常调用，即直接调用 Reflect1.foo()（因为该方法是静态方法，所以直接通过类调用） 总结： 最开始使用反射调用方法的时候，使用的方法执行器中的一个native方法的执行 它有一个阈值，默认是15 调用次数超过15后，说明这个方法调用是经常执行的，JVM就会在运行期间动态生成一段代码，运行期间通过这段代码正常调用对应的方法（从反射方法变成了直接调用） 四、内存模型JMM详情见JUC：https://f1ashades.github.io/2022/04/14/JUC/#more","categories":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/categories/JVM/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"}]},{"title":"JUC并发","slug":"JUC","date":"2022-04-14T08:05:12.000Z","updated":"2022-09-24T09:35:52.628Z","comments":true,"path":"2022/04/14/JUC/","link":"","permalink":"http://example.com/2022/04/14/JUC/","excerpt":"线程与进程、synchronized（重量级锁、轻量级锁、偏向锁）、CAS、Volatile、原子类 Java内存模型 JMM：原子性、可见性、有序性 ExecutorService、线程池、AQS、ReentrantLock 不可变类、线程安全集合：ConcurentHashMap","text":"线程与进程、synchronized（重量级锁、轻量级锁、偏向锁）、CAS、Volatile、原子类 Java内存模型 JMM：原子性、可见性、有序性 ExecutorService、线程池、AQS、ReentrantLock 不可变类、线程安全集合：ConcurentHashMap 目录[toc] 一、java线程1.线程原理1）运行机制Java Virtual Machine Stacks（Java 虚拟机栈）：每个线程启动后，虚拟机就会为其分配一块栈内存 每个栈由多个栈帧（Frame）组成，对应着每次方法调用时所占用的内存 每个线程只能有一个活动栈帧，对应着当前正在执行的那个方法 线程上下文切换（Thread Context Switch）：一些原因导致 CPU 不再执行当前线程，转而执行另一个线程 线程的 CPU 时间片用完 垃圾回收 有更高优先级的线程需要运行 线程自己调用了 sleep、yield、wait、join、park 等方法 程序计数器（Program Counter Register）：记住下一条 JVM 指令的执行地址，是线程私有的 当 Context Switch 发生时，需要由操作系统保存当前线程的状态（PCB 中），并恢复另一个线程的状态，包括程序计数器、虚拟机栈中每个栈帧的信息，如局部变量、操作数栈、返回地址等 JVM 规范并没有限定线程模型，以 HotSopot 为例： Java 的线程是内核级线程（1:1 线程模型），每个 Java 线程都映射到一个操作系统原生线程，需要消耗一定的内核资源（堆栈） 线程的调度是在内核态运行的，而线程中的代码是在用户态运行，所以线程切换（状态改变）会导致用户与内核态转换进行系统调用，这是非常消耗性能 Java 中 main 方法启动的是一个进程也是一个主线程，main 方法里面的其他线程均为子线程，main 线程是这些线程的父线程 2）线程调度线程调度指系统为线程分配处理器使用权的过程，方式有两种：协同式线程调度（非抢占式）、抢占式线程调度（Java 选择） 协同式线程调度：线程的执行时间由线程本身控制 优点：线程做完任务才通知系统切换到其他线程，相当于所有线程串行执行，不会出现线程同步问题 缺点：线程执行时间不可控，如果代码编写出现问题，可能导致程序一直阻塞，引起系统的奔溃 抢占式线程调度：线程的执行时间由系统分配 优点：线程执行时间可控，不会因为一个线程的问题而导致整体系统不可用 缺点：无法主动为某个线程多分配时间 Java 提供了线程优先级的机制，优先级会提示（hint）调度器优先调度该线程，但这仅仅是一个提示，调度器可以忽略它。在线程的就绪状态时，如果 CPU 比较忙，那么优先级高的线程会获得更多的时间片，但 CPU 闲时，优先级几乎没作用 说明：并不能通过优先级来判断线程执行的先后顺序 3）未来优化内核级线程调度的成本较大，所以引入了更轻量级的协程。用户线程的调度由用户自己实现（一对多的线程模型），被设计为协同式调度，所以叫协程 有栈协程：协程会完整的做调用栈的保护、恢复工作，所以叫有栈协程 无栈协程：本质上是一种有限状态机，状态保存在闭包里，比有栈协程更轻量，但是功能有限 有栈协程中有一种特例叫纤程，在新并发模型中，一段纤程的代码被分为两部分，执行过程和调度器： 执行过程：用于维护执行现场，保护、恢复上下文状态 调度器：负责编排所有要执行的代码顺序 4）线程状态进程的状态参考操作系统：创建态、就绪态、运行态、阻塞态、终止态 线程由生到死的完整过程（生命周期）：当线程被创建并启动以后，既不是一启动就进入了执行状态，也不是一直处于执行状态，在 API 中 java.lang.Thread.State 这个枚举中给出了六种线程状态： 线程状态 导致状态发生条件 NEW（新建） 类似创建态，线程刚被创建，但是并未启动，还没调用 start 方法，只有线程对象，没有线程特征 Runnable（可运行） 包含就绪态、运行态、阻塞态，线程可以在 Java 虚拟机中运行的状态，可能正在运行自己代码，也可能没有，这取决于操作系统处理器，调用了 t.start() 方法：就绪（经典叫法） Blocked（阻塞） 当一个线程试图获取一个对象锁，而该对象锁被其他的线程持有，则该线程进入 Blocked 状态；当该线程持有锁时，该线程将变成 Runnable 状态 Waiting（无限等待） 一个线程在等待另一个线程执行一个（唤醒）动作时，该线程进入 Waiting 状态，进入这个状态后不能自动唤醒，必须等待另一个线程调用 notify 或者 notifyAll 方法才能唤醒。eg：join() Timed Waiting （限期等待） 有几个方法有超时参数，调用将进入 Timed Waiting 状态，这一状态将一直保持到超时期满或者接收到唤醒通知。带有超时参数的常用方法有 Thread.sleep 、Object.wait Teminated（结束） run 方法正常退出而死亡，或者因为没有捕获的异常终止了 run 方法而死亡 NEW → RUNNABLE：当调用 t.start() 方法时，由 NEW → RUNNABLE RUNNABLE → WAITING： 当调用 obj.wait() 方法时 而调用 obj.notify()、obj.notifyAll()、t.interrupt()： 竞争锁成功，t 线程从 WAITING → RUNNABLE 竞争锁失败，t 线程从 WAITING → BLOCKED 当前线程调用 t.join() 方法，注意是当前线程在 t 线程对象的监视器上等待 当前线程调用 LockSupport.park() 方法 RUNNABLE→ TIMED_WAITING：调用 obj.wait(long n) 方法、当前线程调用 t.join(long n) 方法、当前线程调用 Thread.sleep(long n) RUNNABLE→ BLOCKED：t 线程用 synchronized(obj) 获取了对象锁时竞争失败 5）查看线程Windows： 任务管理器可以查看进程和线程数，也可以用来杀死进程 tasklist 查看进程 taskkill 杀死进程 Linux： ps -ef 查看所有进程 ps -fT -p 查看某个进程（PID）的所有线程 kill 杀死进程 top 按大写 H 切换是否显示线程 top -H -p 查看某个进程（PID）的所有线程 Java： jps 命令查看所有 Java 进程 jstack 查看某个 Java 进程（PID）的所有线程状态 jconsole 来查看某个 Java 进程中线程的运行情况（图形界面） 2.线程的创建与运行1）方法一：使用 Thread1234567891011public static void main(String[] args) &#123; // 匿名内部类方式创建 Thread Thread t = new Thread(&quot;t1&quot;) &#123; @Override public void run() &#123; log.debug(&quot;running&quot;); &#125; &#125;; t.start(); log.debug(&quot;running&quot;);&#125; 2）方法二：使用 Runnable 配合 Thread123456public static void main(String[] args) &#123; // 使用 lambda 表达式，因为 Runnable 接口 // 标注了 @FunctionalInterface 这个注解，表示是一个函数式接口，可以使用 lambda 表达式 Runnable r = () -&gt; log.debug(&quot;running&quot;); new Thread(r, &quot;t1&quot;).start(); &#125; 比较方法一和方法二： 方法1是把线程和任务合并在了一起，继承了Thread类，就无法再继承其它列 方法2是把线程和任务分开了，用 Runnable 更容易与线程池等高级 API 配合，用 Runnable 让任务类脱离了 Thread 继承体系，更灵活。没有继承Thread类，所以还可以继承其它类。 通过查看源码可以发现，方法二其实还是通过使用 Thread 类中的 run 方法执行的！ 3）方法三：FutureTask 配合 Thread1234567891011121314public static void main(String[] args) throws ExecutionException, InterruptedException &#123; // 1. 使用 FutureTask 传入 Callable 接口方式创建 FutureTask&lt;Integer&gt; future = new FutureTask&lt;Integer&gt;(() -&gt; &#123; log.debug(&quot;running...&quot;); Thread.sleep(2000); // 休眠 return 100; &#125;); // 2. 传入future, 因为FutureTask这个类是实现了RunnableFuture接口，RunnableFuture继承了Runnable接口 Thread t1 = new Thread(future, &quot;t1&quot;); t1.start(); // 3. 获取返回结果时 // 当主线程获取 t1 线程的返回值时, 需要等 2 秒，此时主线程进入阻塞状态 log.debug(&quot;&#123;&#125;&quot;, future.get()); &#125; Future 就是对于具体的 Runnable 或者 Callable 任务的执行结果进行取消、查询是否完成、获取结果。必要时可以通过 get 方法获取执行结果，该方法会阻塞直到任务返回结果。 1234567891011public interface Future&lt;V&gt; &#123; // 取消任务 boolean cancel(boolean mayInterruptIfRunning); // 获取任务执行结果 V get() throws InterruptedException, ExecutionException; // 获取任务执行结果，带有超时时间限制 V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException; // 判断任务是否已经取消 boolean isCancelled(); // 判断任务是否已经结束 boolean isDone();&#125; 具体细节可查看：https://mp.weixin.qq.com/s/RX5rVuGr6Ab0SmKigmZEag 3.Thread的常用方法1）方法概览 方法名 static 功能说明 注意 start() 启动一个新线程，在新线程中运行 run 方法中的代码 start 方法只是让线程进入就绪状态，里面代码不一定立刻运行，只有当 CPU 将时间片分给线程时，才能进入运行状态，执行代码。每个线程的 start 方法只能调用一次，调用多次就会出现 IllegalThreadStateException run() 新线程启动会调用的方法 如果在构造 Thread 对象时传递了 Runnable 参数，则线程启动后会调用 Runnable 中的 run 方法，否则默认不执行任何操作。但可以创建 Thread 的子类对象，来覆盖默认行为 join() 等待线程运行结束 join(long n) 等待线程运行结束,最多等待 n 毫秒 getId() 获取线程长整型的 id id 唯一 getName() 获取线程名 setName(String) 修改线程名 getPriority() 获取线程优先级 setPriority(int) 修改线程优先级 java中规定线程优先级是1~10 的整数，较大的优先级能提高该线程被 CPU 调度的机率 getState() 获取线程状态 Java 中线程状态是用 6 个 enum 表示，分别为：NEW, RUNNABLE, BLOCKED, WAITING, TIMED_WAITING, TERMINATED isInterrupted() 判断是否被打断 不会清除 打断标记 isAlive() 线程是否存活（还没有运行完毕） interrupt() 打断线程 如果被打断线程正在 sleep，wait，join 会导致被打断的线程抛出 InterruptedException，并清除 打断标记 ；如果打断的正在运行的线程，则会设置 打断标记，park 的线程被打断，也会设置 打断标记 interrupted() static 判断当前线程是否被打断 会清除 打断标记 currentThread() static 获取当前正在执行的线程 sleep(long n) static 让当前执行的线程休眠n毫秒，休眠时让出 cpu 的时间片给其它线程 yield() static 提示线程调度器让出当前线程对CPU的使用 主要是为了测试和调试 2）start() VS run()直接调用 run() 方法： 12345678910111213public static void main(String[] args) &#123; Thread t1 = new Thread(new Runnable() &#123; @Override public void run() &#123; log.info(Thread.currentThread().getName() + &quot; running....&quot;); &#125; &#125;, &quot;t1&quot;); // 测试通过 Thread 类实例 t1 对象直接调用 run 方法 t1.run(); log.info(Thread.currentThread().getName() + &quot; running...&quot;);&#125; 结果： 1214:56:56 [main] c.Code_05_Test - main running....14:56:56 [main] c.Code_05_Test - main running... 可以看出，是main方法执行了t1中的run方法 调用 start() 方法： 1234567891011121314public static void main(String[] args) &#123; Thread t1 = new Thread(new Runnable() &#123; @Override public void run() &#123; log.info(Thread.currentThread().getName() + &quot; running....&quot;); &#125; &#125;, &quot;t1&quot;); // 测试通过 Thread 类实例 t1 对象直接调用 run 方法 // t1.run(); // 调用 start 方法 t1.start(); log.info(Thread.currentThread().getName() + &quot; running...&quot;);&#125; 结果： 1214:59:35 [main] c.Code_05_Test - main running...14:59:35 [t1] c.Code_05_Test - t1 running.... 发现两种结果是不一样的，使用 start 方式，CPU 会为创建的线程分配时间片，线程进入运行状态，然后线程调用 run 方法执行逻辑。直接使用 run 的方式，虽然会创建了线程，但是它是直接调用方法，而不是像 start 方式那样触发的，这个线程对象会处一直处在新建状态，从结果上也可以看出，run 方法是 main 线程调用，而不是 t1 线程。 new 一个 Thread，线程进入了新建状态。调用 start()方法，会启动一个线程并使线程进入了就绪状态，当分配到时间片后就可以开始运行了。 start() 会执行线程的相应准备工作，然后自动执行 run() 方法的内容，这是真正的多线程工作。 但是，直接执行 run() 方法，会把 run() 方法当成一个 main 线程下的普通方法去执行，并不会在某个线程中执行它，所以这并不是多线程工作。 总结： 调用 start() 方法方可启动线程并使线程进入就绪状态，直接执行 run() 方法的话不会以多线程的方式执行。 2）sleep()与yield()①sleep (使线程阻塞) 调用 sleep 会让当前线程从 Running 进入 Timed Waiting 状态（阻塞），可通过state()方法查看 其它线程可以使用 interrupt 方法打断正在睡眠的线程，这时 sleep 方法会抛出 InterruptedException 睡眠结束后的线程未必会立刻得到执行 建议用 TimeUnit 的 sleep 代替 Thread 的 sleep 来获得更好的可读性 其内部也调用了Thread.sleep()，作为一个桥梁把时间单位规范了 1234//休眠一秒TimeUnit.SECONDS.sleep(1);//休眠一分钟TimeUnit.MINUTES.sleep(1); ②yield （让出当前线程） 调用 yield 会让当前线程从 Running 进入 Runnable 就绪状态（仍然有可能被执行），然后调度执行其它线程 如果没有其它线程，只有该线程，那么就算yield让出了CPU，下一刻该线程也会被CPU选中执行 具体的实现依赖于操作系统的任务调度器 ③线程优先级 线程优先级会提示（hint）调度器优先调度该线程，但它仅仅是一个提示，调度器可以忽略它 调度器甚至可能不理会其优先级 最低是1，默认是5，最高是10 如果 cpu 比较忙，那么优先级高的线程会获得更多的时间片，但 cpu 闲时，优先级几乎没作用 设置方法： 1thread1.setPriority(Thread.MAX_PRIORITY); //设置为优先级最高 3）join()方法①基本使用 用于等待某个线程A结束。线程B内调用A.join()方法，就等待A结束，然后再继续执行B中A.join()后的代码 如在主线程中调用ti.join()，则是主线程等待t1线程结束 12345Thread thread = new Thread();//等待thread线程执行结束thread.join();//最多等待1000ms,如果1000ms内线程执行完毕，则会直接执行下面的语句，不会等够1000msthread.join(1000);Copy ②原理见 二、7. 4） 4）interrupt() 方法①interrupt 打断线程有两种情况，如下： 如果一个线程在在运行中被打断，打断标记会被置为 true 。 如果是打断因sleep wait join 方法而被阻塞的线程，会将打断标记置为 false 这三个方法在被打断后都会catch一个InterruptedException e异常，可以在catch中将打断标记置为 ture 通过current.interrupt()再次将本线程打断标志设置为true ②isInterrupted() 与 interrupted() 比较 首先，isInterrupted 是实例方法，interrupted 是静态方法： Thread.currentThread.isInterrupted() Thread.interrupted() 它们的用处都是查看当前打断的状态： isInterrupted 方法查看线程的时候，不会将打断标记清空，也就是置为 false， isInterrupted() 类似于 getter 获取中断值 interrupted 查看线程打断状态后，会将打断标志置为 false，也就是清空打断标记， interrupted() 类似于 getter 先获取中断值，然后 setter 清除标志 ③两阶段终止模式 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/**使用 interrupt 进行两阶段终止模式*/@Slf4j(topic = &quot;c.Code_13_Test&quot;)public class Code_13_Test &#123; public static void main(String[] args) throws InterruptedException &#123; TwoParseTermination twoParseTermination = new TwoParseTermination(); twoParseTermination.start(); Thread.sleep(3500); twoParseTermination.stop(); &#125;&#125;@Slf4j(topic = &quot;c.TwoParseTermination&quot;)class TwoParseTermination &#123; private Thread monitor; // 启动线程 public void start() &#123; monitor = new Thread(() -&gt; &#123; while (true) &#123; Thread current = Thread.currentThread(); if(thread.isInterrupted()) &#123; // 调用 isInterrupted 不会清除标记 log.info(&quot;料理后事 ...&quot;); break; &#125; else &#123; try &#123; Thread.sleep(1000); log.info(&quot;执行监控的功能 ...&quot;); &#125; catch (InterruptedException e) &#123; log.info(&quot;设置打断标记 ...&quot;); current.interrupt();//重新设置标记 e.printStackTrace(); &#125; &#125; &#125; &#125;, &quot;monitor&quot;); monitor.start(); &#125; // 终止线程 public void stop() &#123; monitor.interrupt(); &#125;&#125; 这里模拟了监控功能 开启监控就是开启一个线程通过while true循环执行任务 Thread.sleep(1000);保证CPU占用率不会100% 并且也可以表明每隔1s监控一次 关闭监控利用了interrupt()方法 如果监控线程是运行状态，那么会使打断标志位true，然后break推出while true 如果监控线程是阻塞状态（sleep），这时候interrupt打断后，打断标志位false 阻塞状态会抛出一个异常，可以catch这个InterruptedException e异常，可以通过current.interrupt()再次将本线程打断标志设置为true ④打断parkpark 作用类似 sleep，打断 park 线程，不会清空打断状态（true） 1234567891011public static void main(String[] args) throws Exception &#123; Thread t1 = new Thread(() -&gt; &#123; System.out.println(&quot;park...&quot;); LockSupport.park(); System.out.println(&quot;unpark...&quot;); System.out.println(&quot;打断状态：&quot; + Thread.currentThread().isInterrupted());//打断状态：true &#125;, &quot;t1&quot;); t1.start(); Thread.sleep(2000); t1.interrupt();&#125; 如果打断标记已经是 true, 则 park 会失效 1234567891011121314public static void main(String[] args) throws Exception &#123; Thread t1 = new Thread(() -&gt; &#123; System.out.println(&quot;park...&quot;); LockSupport.park(); System.out.println(&quot;unpark...&quot;); System.out.println(&quot;打断状态：&quot; + Thread.currentThread().isInterrupted());//打断状态：true LockSupport.park();//失效，不会阻塞 System.out.println(&quot;unpark...&quot;);//和上一个unpark同时执行 &#125;, &quot;t1&quot;); t1.start(); Thread.sleep(2000); t1.interrupt();&#125; 可以修改获取打断状态方法，使用 Thread.interrupted()，清除打断标记 1234567891011121314public static void main(String[] args) throws Exception &#123; Thread t1 = new Thread(() -&gt; &#123; System.out.println(&quot;park...&quot;); LockSupport.park(); System.out.println(&quot;unpark...&quot;); //打断状态：true，interrupted()后变成false，可以继续park System.out.println(&quot;打断状态：&quot; + Thread.currentThread().interrupted()); //此时打断标记变为false，打断 LockSupport.park(); &#125;, &quot;t1&quot;); t1.start(); Thread.sleep(2000); t1.interrupt();&#125; 5）daemon守护线程 考虑这样的一种情景： 主线程中start了线程1，线程1中while循环 当主线程代码执行完以后，程序并不会停止，线程1还会不断运行 守护线程： public final void setDaemon(boolean on)：如果是 true ，将此线程标记为守护线程 线程启动前调用此方法： 123456789Thread t = new Thread() &#123; @Override public void run() &#123; System.out.println(&quot;running&quot;); &#125;&#125;;// 设置该线程为守护线程t.setDaemon(true);t.start(); 用户线程：平常创建的普通线程 守护线程：服务于用户线程，只要其它非守护线程运行结束了，即使守护线程代码没有执行完，也会强制结束。守护进程是脱离于终端并且在后台运行的进程，脱离终端是为了避免在执行的过程中的信息在终端上显示 当运行的线程都是守护线程，Java 虚拟机将退出，因为普通线程执行完后，JVM 是守护线程，不会继续运行下去 常见的守护线程： 垃圾回收器线程就是一种守护线程 Tomcat 中的 Acceptor 和 Poller 线程都是守护线程，所以 Tomcat 接收到 shutdown 命令后，不会等待它们处理完当前请求 6）不推荐使用的打断方法 以下方法容易破坏同步代码块，造成共享资源无法被释放，使其他线程无法使用这些共享资源 stop方法 停止线程运行 推荐两阶段终止模式 suspend（暂停线程）&#x2F;resume（恢复线程）方法 推荐wait方法 二、共享模型之管程1.临界区临界资源：一次仅允许一个进程使用的资源成为临界资源 临界区：访问临界资源的代码块 竞态条件：多个线程在临界区内执行，由于代码的执行序列不同而导致结果无法预测，称之为发生了竞态条件 一个程序运行多个线程是没有问题，多个线程读共享资源也没有问题，在多个线程对共享资源读写操作时发生指令交错，就会出现问题 为了避免临界区的竞态条件发生（解决线程安全问题）： 阻塞式的解决方案：synchronized，lock 非阻塞式的解决方案：原子变量 管程（monitor）：由局部于自己的若干公共变量和所有访问这些公共变量的过程所组成的软件模块，保证同一时刻只有一个进程在管程内活动，即管程内定义的操作在同一时刻只被一个进程调用（由编译器实现） synchronized：对象锁，保证了临界区内代码的原子性，采用互斥的方式让同一时刻至多只有一个线程能持有对象锁，其它线程获取这个对象锁时会阻塞，保证拥有锁的线程可以安全的执行临界区内的代码，不用担心线程上下文切换 互斥和同步都可以采用 synchronized 关键字来完成，区别： 互斥是保证临界区的竞态条件发生，同一时刻只能有一个线程执行临界区代码 同步是由于线程执行的先后、顺序不同、需要一个线程等待其它线程运行到某个点 性能： 线程安全，性能差 线程不安全性能好，假如开发中不会存在多线程安全问题，建议使用线程不安全的设计类 2.synchronized1）基本概念​ synchronized俗称【对象锁】，它采用互斥的方式让同一时刻至多只有一个线程能持有【对象锁】，其它线程再想获取这个【对象锁】时就会阻塞住(blocked)。这样就能保证拥有锁的线程可以安全的执行临界区内的代码，不用担心线程上下文切换。 ​ 线程1持有锁钥匙，那么即使线程1时间片用完了，也不会交出钥匙，只有当其完全执行完临界区中的代码以后，才会交出钥匙。可以这样说，synchronized利用了对象锁保证了临界区中代码的原子性，不会被线程切换打断。 eg： 1234567891011121314151617181920212223242526public class demo &#123; static int counter = 0; //static修饰，则元素是属于类本身的，不属于对象，与类一起加载一次，只有一个 static final Object room = new Object(); public static void main(String[] args) throws InterruptedException &#123; Thread t1 = new Thread(() -&gt; &#123; for (int i = 0; i &lt; 5000; i++) &#123; synchronized (room) &#123; counter++; &#125; &#125; &#125;, &quot;t1&quot;); Thread t2 = new Thread(() -&gt; &#123; for (int i = 0; i &lt; 5000; i++) &#123; synchronized (room) &#123; counter--; &#125; &#125; &#125;, &quot;t2&quot;); t1.start(); t2.start(); t1.join(); t2.join(); System.out.println(counter); &#125;&#125; 2）synchronized方法把出现线程安全问题的核心方法锁起来，每次只能一个线程进入访问 synchronized 修饰的方法的不具备继承性，所以子类是线程不安全的，如果子类的方法也被 synchronized 修饰，两个锁对象其实是一把锁，而且是子类对象作为锁：https://blog.csdn.net/liuyancainiao/article/details/81707025 用法：直接给方法加上一个修饰符 synchronized 12345678//同步方法修饰符 synchronized 返回值类型 方法名(方法参数) &#123; 方法体；&#125;//同步静态方法修饰符 static synchronized 返回值类型 方法名(方法参数) &#123; 方法体；&#125; 同步方法底层也是有锁对象的： 如果方法是实例方法：同步方法默认用 this 作为的锁对象 1234public synchronized void test() &#123;&#125; //等价于 public void test() &#123; synchronized(this) &#123;&#125;&#125; 如果方法是静态方法：同步方法默认用类名 .class 作为的锁对象 123456789class Test&#123; public synchronized static void test() &#123;&#125;&#125;//等价于class Test&#123; public void test() &#123; synchronized(Test.class) &#123;&#125; &#125;&#125; 3）线程”八锁”线程八锁就是考察 synchronized 锁住的是哪个对象，直接百度搜索相关的实例 说明：主要关注锁住的对象是不是同一个 锁住类对象，所有类的实例的方法都是安全的，类的所有实例都相当于同一把锁 锁住 this 对象，只有在当前实例对象的线程内是安全的，如果有多个实例就不安全 下面是两个代表 线程不安全：因为锁住的不是同一个对象，线程 1 调用 a 方法锁住的类对象，线程 2 调用 b 方法锁住的 n2 对象，不是同一个对象 123456789101112131415class Number&#123; public static synchronized void a()&#123; Thread.sleep(1000); System.out.println(&quot;1&quot;); &#125; public synchronized void b() &#123; System.out.println(&quot;2&quot;); &#125;&#125;public static void main(String[] args) &#123; Number n1 = new Number(); Number n2 = new Number(); new Thread(()-&gt;&#123; n1.a(); &#125;).start(); new Thread(()-&gt;&#123; n2.b(); &#125;).start();&#125; 这里2个new出来的线程仍然是并发执行的，所以可能会出现线程安全问题 线程执行方法的时候锁住的不是同一个对象，不会有互斥的效果 线程安全：因为 n1 调用 a() 方法，锁住的是类对象，n2 调用 b() 方法，锁住的也是类对象，所以线程安全 123456789101112131415class Number&#123; public static synchronized void a()&#123; Thread.sleep(1000); System.out.println(&quot;1&quot;); &#125; public static synchronized void b() &#123; System.out.println(&quot;2&quot;); &#125;&#125;public static void main(String[] args) &#123; Number n1 = new Number(); Number n2 = new Number(); new Thread(()-&gt;&#123; n1.a(); &#125;).start(); new Thread(()-&gt;&#123; n2.b(); &#125;).start();&#125; 4）线程安全分析123456789101112131415161718192021222324class ThreadSafe &#123; public final void method1(int loopNumber) &#123; ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; loopNumber; i++) &#123; method2(list); method3(list); &#125; &#125; public void method2(ArrayList&lt;String&gt; list) &#123; list.add(&quot;1&quot;); &#125; public void method3(ArrayList&lt;String&gt; list) &#123; list.remove(0); &#125;&#125;class ThreadSafeSubClass extends ThreadSafe&#123; @Override public void method3(ArrayList&lt;String&gt; list) &#123; new Thread(() -&gt; &#123; list.remove(0); &#125;).start(); &#125;&#125; 12345678910static final int THREAD_NUMBER = 2;static final int LOOP_NUMBER = 200;public static void main(String[] args) &#123; ThreadSafeSubClass test = new ThreadSafeSubClass(); for (int i = 0; i &lt; THREAD_NUMBER; i++) &#123; new Thread(() -&gt; &#123; test.method1(LOOP_NUMBER); &#125;, &quot;Thread&quot; + i).start(); &#125;&#125; 这样是会出问题的，因为子类重写了method3，在method3中重新开了一个线程来执行remove，考虑下面这种情况： ​ psvm中线程1执行了method1，method1先执行method2，add+1了，然后执行method3，method3重开了一个线程2，线程2去执行remove操作，线程2执行remove操作是不受控的，可能在这一时刻不会remove，然后到method1下一轮循环，add+1，此时list是1-&gt;1，有两个数字，因为线程2可能还没remove 5）线程安全类①常见线程安全类 String Integer StringBuffffer Random Vector Hashtable java.util.concurrent 包下的类 这里说它们是线程安全的是指，多个线程调用它们同一个实例的某个方法时，是线程安全的。也可以理解为 123456789Hashtable table = new Hashtable();new Thread(()-&gt;&#123; table.put(&quot;key&quot;, &quot;value1&quot;);&#125;).start();new Thread(()-&gt;&#123; table.put(&quot;key&quot;, &quot;value2&quot;);&#125;).start(); 它们的每个方法是原子的 但注意它们多个方法的组合不是原子的，见后面分析 ②线程安全类方法的组合分析下面代码是否线程安全？ 12345Hashtable table = new Hashtable();// 线程1，线程2if( table.get(&quot;key&quot;) == null) &#123; table.put(&quot;key&quot;, value);&#125; 线程1在判断完table.get(“key”) &#x3D;&#x3D; null后，准备要table.put(“key”, value);这期间发生了线程切换，线程2也table.get(“key”) &#x3D;&#x3D; null，然后table.put(“key”, value);执行完了，线程切换到1，它已经判断完为null，实际上现在table有值 ③不可变类线程安全性String、Integer 等都是不可变类，因为其内部的状态不可以改变，因此它们的方法都是线程安全的 如String的subString方法，最后的return实际上是new String，new了一个新的对象，并没有改变原来的对象 ④实例分析例1： 12345678910111213141516public class MyServlet extends HttpServlet &#123; // 不安全 多个线程个以对map进行添加和删除 Map&lt;String,Object&gt; map = new HashMap&lt;&gt;(); // 安全 String S1 = &quot;...&quot;; // 安全 final String S2 = &quot;...&quot;; // 不安全 多个线程可以对修改Date中的属性日期等 Date D1 = new Date(); // 不安全，final只是说引用值不能变， final Date D2 = new Date(); public void doGet(HttpServletRequest request, HttpServletResponse response) &#123; // 使用上述变量 &#125;&#125; 例2： 123456789101112131415161718public class MyServlet extends HttpServlet &#123; // 不安全，因为该UserService是有状态的 private UserService userService = new UserServiceImpl(); public void doGet(HttpServletRequest request, HttpServletResponse response) &#123; userService.update(...); &#125;&#125;public class UserServiceImpl implements UserService &#123; // 记录调用次数 private int count = 0; public void update() &#123; // ... count++; &#125;&#125; 例3: 1234567891011121314151617@Aspect@Componentpublic class MyAspect &#123; // 不安全 private long start = 0L; @Before(&quot;execution(* *(..))&quot;) public void before() &#123; start = System.nanoTime(); &#125; @After(&quot;execution(* *(..))&quot;) public void after() &#123; long end = System.nanoTime(); System.out.println(&quot;cost time:&quot; + (end-start)); &#125;&#125; 因为是单例，所以多个线程可能对其进行并发修改 解决：使用环绕通知，将start定义成一个局部变量 例4 1234567891011121314151617181920212223242526272829public class MyServlet extends HttpServlet &#123; // 安全，无状态 private UserService userService = new UserServiceImpl(); public void doGet(HttpServletRequest request, HttpServletResponse response) &#123; userService.update(...); &#125;&#125;public class UserServiceImpl implements UserService &#123; // 安全，无状态 private UserDao userDao = new UserDaoImpl(); public void update() &#123; userDao.update(); &#125;&#125;public class UserDaoImpl implements UserDao &#123; public void update() &#123; String sql = &quot;update user set password = ? where username = ?&quot;; // 安全，因为是成员变量 try (Connection conn = DriverManager.getConnection(&quot;&quot;,&quot;&quot;,&quot;&quot;))&#123; // ... &#125; catch (Exception e) &#123; // ... &#125; &#125;&#125; 例5 12345678910111213141516171819202122232425262728public class MyServlet extends HttpServlet &#123; // 不安全 private UserService userService = new UserServiceImpl(); public void doGet(HttpServletRequest request, HttpServletResponse response) &#123; userService.update(...); &#125;&#125;public class UserServiceImpl implements UserService &#123; // 不安全 private UserDao userDao = new UserDaoImpl(); public void update() &#123; userDao.update(); &#125;&#125;public class UserDaoImpl implements UserDao &#123; // 不安全，这里是成员变量，只有一份UserDaoImpl，多个线程会并发修改 private Connection conn = null; public void update() throws SQLException &#123; String sql = &quot;update user set password = ? where username = ?&quot;; conn = DriverManager.getConnection(&quot;&quot;,&quot;&quot;,&quot;&quot;); // ... conn.close(); &#125;&#125; 例6 123456789101112131415161718192021222324252627public class MyServlet extends HttpServlet &#123; // 安全，虽然dao不安全，但service每次都是new dao private UserService userService = new UserServiceImpl(); public void doGet(HttpServletRequest request, HttpServletResponse response) &#123; userService.update(...); &#125;&#125;public class UserServiceImpl implements UserService &#123; public void update() &#123; UserDao userDao = new UserDaoImpl(); userDao.update(); &#125;&#125;public class UserDaoImpl implements UserDao &#123; // 不安全 private Connection = null; public void update() throws SQLException &#123; String sql = &quot;update user set password = ? where username = ?&quot;; conn = DriverManager.getConnection(&quot;&quot;,&quot;&quot;,&quot;&quot;); // ... conn.close(); &#125;&#125; 例7 12345678910111213141516171819public abstract class Test &#123; public void bar() &#123; //sdf虽然是局部变量，但是逃逸出了局部方法 // 不安全，sdf传到foo，逃出了局部方法 SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); //foo方法不确定，如果另开线程来操作sdf，那就会有线程 foo(sdf); &#125; // 抽象方法，不知道子类是怎样实现的，如果子类另开一个线程来对sdf进行修改，那就是不安全的 public abstract foo(SimpleDateFormat sdf); public static void main(String[] args) &#123; new Test().bar(); &#125; &#125; 其中 foo 的行为是不确定的，可能导致不安全的发生，被称之为外星方法 123456789101112public void foo(SimpleDateFormat sdf) &#123; String dateStr = &quot;1999-10-11 00:00:00&quot;; for (int i = 0; i &lt; 20; i++) &#123; new Thread(() -&gt; &#123; try &#123; sdf.parse(dateStr); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; &#125;).start(); &#125;&#125; 例8 1234567891011121314151617181920212223private static Integer i = 0;public static void main(String[] args) throws InterruptedException &#123; List&lt;Thread&gt; list = new ArrayList&lt;&gt;(); for (int j = 0; j &lt; 2; j++) &#123; Thread thread = new Thread(() -&gt; &#123; for (int k = 0; k &lt; 5000; k++) &#123; synchronized (i) &#123; i++; &#125; &#125; &#125;, &quot;&quot; + j); list.add(thread); &#125; list.stream().forEach(t -&gt; t.start()); list.stream().forEach(t -&gt; &#123; try &#123; t.join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); log.debug(&quot;&#123;&#125;&quot;, i);&#125; 3.重量级锁1）Java 对象头普通对象： Mark Word 主要用来存储对象自身的运行时数据 Klass Word 指向Class对象 数组对象： 相对于普通对象多了记录数组长度 Mark Word 结构： 32位： 在没有monitor的情况下,markword记录的是hashcode、GC年龄等 64 位虚拟机 Mark Word： 2）基本原理 ​ 每一个java对象都可以关联一个monitor对象，如果使用synchronized给对象上锁（重量级锁）以后，该对象头中的Mark Word中就会被设置指向monitor对象的指针： obj是java的对象，monitor对象是操作系统提供的（C实现的） Mark Word的前30位会变成指向monitor的指针，后两位会为置为10（代表重量级锁） hashcode和GC年龄会存入monitor中，当释放锁的时候会还原这些信息到对象头 刚开始 Monitor 中 Owner 为 null 当 Thread-2 执行 synchronized(obj) 就会将 Monitor 的所有者 Owner 置为 Thread-2，Monitor中只能有一个 Owner 在 Thread-2 上锁的过程中，如果 Thread-3，Thread-4，Thread-5 也来执行 synchronized(obj)，就会进入EntryList BLOCKED Thread-2 执行完同步代码块的内容，然后唤醒 EntryList 中等待的线程来竞争锁 竞争的时是非公平的，不是按3,4,5这样先来先到的顺序，如果此时有t-6到来，有可能插队先竞争到锁 图中 WaitSet 中的 Thread-0，Thread-1 是之前获得过锁，但条件不满足进入 WAITING 状态的线程，详情请看6.wait-notify 注意： synchronized 必须是进入同一个对象的 monitor 才有上述的效果 每个对象都对应不同的monitor 不加 synchronized 的对象不会关联monitor，不遵从以上规则 3）字节码解释1234567static final Object lock = new Object();static int counter = 0;public static void main(String[] args) &#123; synchronized (lock) &#123; counter++; &#125;&#125; 对应的字节码如下： 从直接中可以看出，出现异常锁还是会被释放： 异常表中记录的范围如果出现异常，就会到19行继续执行 19行以后存储了变量信息，然后释放了锁唤醒entryList，最后throw了异常 4.轻量级锁1）轻量级锁​ 如果一个对象虽然有多线程要加锁，但加锁的时间是错开的（也就是没有竞争），那么可以使用轻量级锁来优化。 ​ 轻量级锁对使用者是透明的，即语法仍然是 synchronized eg： 假设有两个方法同步块，利用同一个对象加锁 1234567891011121314static final Object obj = new Object();public static void method1() &#123; synchronized( obj ) &#123; // 同步块 A method2(); &#125;&#125;public static void method2() &#123; synchronized( obj ) &#123; // 同步块 B &#125;&#125; 创建锁记录（Lock Record）对象，每个线程的栈帧都会包含一个锁记录的结构，内部可以存储锁定对象的Mark Word 让锁记录中 Object reference 指向锁对象，并尝试用 cas 替换 Object 的 Mark Word，将 Mark Word 的值存入锁记录 如果 cas 替换成功，对象头中存储了 锁记录地址和状态 00 ，表示由该线程给对象加锁，这时图示如下 如果 cas 失败，有两种情况 如果是其它线程已经持有了该 Object 的轻量级锁，这时表明有竞争，进入锁膨胀过程 如果是自己线程执行了 synchronized 锁重入，那么再添加一条 Lock Record 作为重入的计数 当退出 synchronized 代码块（解锁时）如果有取值为 null 的锁记录，表示有重入，这时重置锁记录，表示重入计数减一 2）锁膨胀​ 即(轻量级)锁膨胀(为重量级锁)的过程 ​ 如果在尝试加轻量级锁的过程中，CAS 操作无法成功，这时一种情况就是有其它线程为此对象加上了轻量级锁（有竞争），这时需要进行锁膨胀，将轻量级锁变为重量级锁。 123456static Object obj = new Object();public static void method1() &#123; synchronized( obj ) &#123; // 同步块 &#125;&#125; 当 Thread-1 进行轻量级加锁时，Thread-0 已经对该对象加了轻量级锁 这时 Thread-1 加轻量级锁失败，进入锁膨胀流程 即为 Object 对象申请 Monitor 锁，让 Object 指向重量级锁地址 然后自己进入 Monitor 的 EntryList BLOCKED 当 Thread-0 退出同步块解锁时，使用 cas 将 Mark Word 的值恢复给对象头，失败。这时会进入重量级解锁流程，即按照 Monitor 地址找到 Monitor 对象，设置 Owner 为 null，唤醒 EntryList 中 BLOCKED 线程 3）自旋优化​ 重量级锁竞争的时候，还可以使用自旋(循环尝试获取重量级锁)来进行优化，如果当前线程自旋成功（即自旋期间持锁线程已经退出了同步块，释放了锁），这时当前线程就可以避免阻塞。 (进入阻塞再恢复,会发生上下文切换,比较耗费性能) 自旋重试成功的情况： 自旋重试失败的情况： 自旋会占用 CPU 时间，单核 CPU 自旋就是浪费，多核 CPU 自旋才能发挥优势。 线程串行执行，线程2在自旋的时候，线程1根本执行不了（不可能释放锁），所以线程2的自旋是没意义的 在 Java 6 之后自旋锁是自适应的，比如对象刚刚的一次自旋操作成功过，那么认为这次自旋成功的可能性会高，就多自旋几次；反之，就少自旋甚至不自旋，总之，比较智能。 Java 7 之后不能控制是否开启自旋功能 5.偏向锁考虑这样一个问题：轻量级锁在没有竞争时（就自己这个线程），每次重入仍然需要执行 CAS 操作。 ​ Java 6 中引入了偏向锁来做进一步优化：只有第一次使用 CAS 将线程 ID 设置到对象的 Mark Word 头，之后重入的时候发现这个线程 ID 是自己的就表示没有竞争，不用重新 CAS。以后只要不发生竞争，这个对象就归该线程所有 。 注意：这里的线程id是操作系统赋予的id 和 Thread的id是不同的 1）example123456789101112131415161718192021static final Object obj = new Object();public static void m1() &#123; synchronized( obj ) &#123; // 同步块 A m2(); &#125;&#125;public static void m2() &#123; synchronized( obj ) &#123; // 同步块 B m3(); &#125;&#125;public static void m3() &#123; synchronized( obj ) &#123; // 同步块 C &#125;&#125; 2）偏向状态①延迟特性​ 偏向锁是默认是延迟的，不会在程序启动时立即生效，如果想避免延迟，可以加 VM 参数 -XX:BiasedLockingStartupDelay=0 来禁用延迟 ②测试偏向锁test： 1234567891011121314public static void main(String[] args) throws IOException &#123; Dog d = new Dog(); ClassLayout classLayout = ClassLayout.parseInstance(d); new Thread(() -&gt; &#123; log.debug(&quot;synchronized 前&quot;); System.out.println(classLayout.toPrintableSimple(true)); synchronized (d) &#123; log.debug(&quot;synchronized 中&quot;); System.out.println(classLayout.toPrintableSimple(true)); &#125; log.debug(&quot;synchronized 后&quot;); System.out.println(classLayout.toPrintableSimple(true)); &#125;, &quot;t1&quot;).start();&#125; 输出 123456711:08:58.117 c.TestBiased [t1] - synchronized 前 //未进入同步代码块00000000 00000000 00000000 00000000 *00000000 00000000 00000000 00000101 11:08:58.121 c.TestBiased [t1] - synchronized 中 //已经进入同步代码块00000000 00000000 00000000 00000000 *00011111 11101011 11010000 00000101 11:08:58.121 c.TestBiased [t1] - synchronized 后 //已经出了同步代码块00000000 00000000 00000000 00000000 *00011111 11101011 11010000 00000101 注意 处于偏向锁的对象解锁后，线程 id 仍存储于对象头中 也就是偏(心)向某个线程了 ③测试禁用在上面测试代码运行时在添加 VM 参数 -XX:-UseBiasedLocking 禁用偏向锁 1234567//*处是线程id11:13:10.018 c.TestBiased [t1] - synchronized 前00000000 00000000 00000000 00000000 *00000000 00000000 00000000 00000001 11:13:10.021 c.TestBiased [t1] - synchronized 中00000000 00000000 00000000 00000000 *00100000 00010100 11110011 10001000 //末尾是00，轻量级锁11:13:10.021 c.TestBiased [t1] - synchronized 后00000000 00000000 00000000 00000000 *00000000 00000000 00000000 00000001 ④测试 hashCode在Dog d = new Dog();再后加上一句 d.hashCode(); 正常状态对象一开始是没有 hashCode 的，第一次调用才生成 调用了 hashCode() 后会撤销该对象的偏向锁 3）撤销偏向锁①调用对象 hashCode调用了对象的 hashCode，但偏向锁的对象 MarkWord 中存储的是线程 id，如果调用 hashCode 会导致偏向锁被撤销 轻量级锁会在锁记录中记录 hashCode 重量级锁会在 Monitor 中记录 hashCode 记得去掉 -XX:-UseBiasedLocking，在调用 hashCode 后使用偏向锁， 123456711:22:10.386 c.TestBiased [main] - 调用 hashCode:1778535015 11:22:10.391 c.TestBiased [t1] - synchronized 前00000000 00000000 00000000 01101010 00000010 01001010 01100111 00000001 11:22:10.393 c.TestBiased [t1] - synchronized 中00000000 00000000 00000000 00000000 00100000 11000011 11110011 01101000 11:22:10.393 c.TestBiased [t1] - synchronized 后00000000 00000000 00000000 01101010 00000010 01001010 01100111 00000001 ②其它线程(错开)使用对象线程错开使用，并不会产生竞争，偏向锁会升级成为轻量锁，因为没有竞争，所以不会升级成为重量级锁 12345678910111213141516171819202122232425262728293031323334353637383940414243private static void test2() throws InterruptedException &#123; Dog d = new Dog(); Thread t1 = new Thread(() -&gt; &#123; log.debug(ClassLayout.parseInstance(d).toPrintableSimple(true)); synchronized (d) &#123; log.debug(ClassLayout.parseInstance(d).toPrintableSimple(true)); &#125; log.debug(ClassLayout.parseInstance(d).toPrintableSimple(true)); synchronized (TestBiased.class) &#123; TestBiased.class.notify(); &#125; // 如果不用 wait/notify 使用 join 必须打开下面的注释 // 因为：t1 线程不能结束，否则底层线程可能被 jvm 重用作为 t2 线程，底层线程 id 是一样的 /*try &#123; System.in.read(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;*/ &#125;, &quot;t1&quot;); t1.start(); Thread t2 = new Thread(() -&gt; &#123; synchronized (TestBiased.class) &#123; try &#123; TestBiased.class.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; log.debug(ClassLayout.parseInstance(d).toPrintableSimple(true)); synchronized (d) &#123; log.debug(ClassLayout.parseInstance(d).toPrintableSimple(true)); &#125; log.debug(ClassLayout.parseInstance(d).toPrintableSimple(true)); &#125;, &quot;t2&quot;); t2.start();&#125; 输出： 最后的偏向状态也被撤销了 最后三位101-&gt;000-&gt;001 ③调用 wait&#x2F;notify重量级锁才支持，使用wait&#x2F;notify会使偏向锁升级为重量级锁 1234567891011121314151617181920212223242526272829public static void main(String[] args) throws InterruptedException &#123; Dog d = new Dog(); Thread t1 = new Thread(() -&gt; &#123; log.debug(ClassLayout.parseInstance(d).toPrintableSimple(true)); synchronized (d) &#123; log.debug(ClassLayout.parseInstance(d).toPrintableSimple(true)); try &#123; d.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; log.debug(ClassLayout.parseInstance(d).toPrintableSimple(true)); &#125; &#125;, &quot;t1&quot;); t1.start(); new Thread(() -&gt; &#123; try &#123; Thread.sleep(6000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; synchronized (d) &#123; log.debug(&quot;notify&quot;); d.notify(); &#125; &#125;, &quot;t2&quot;).start();&#125; 输出 1234[t1] - 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000101 [t1] - 00000000 00000000 00000000 00000000 00011111 10110011 11111000 00000101 [t2] - notify [t1] - 00000000 00000000 00000000 00000000 00011100 11010100 00001101 11001010 4）批量重偏向​ 如果对象虽然被多个线程访问，但没有竞争，这时偏向了线程 T1 的对象仍有机会重新偏向 T2，重偏向会重置对象的 Thread ID ​ 当(某类型对象)撤销偏向锁阈值超过 20 次后，jvm 会这样觉得，我是不是偏向错了呢，于是会在给(所有这种类型的状态为偏向锁的)对象加锁时重新偏向至新的加锁线程 注意t2-19处的变化: 发生了批量重偏向 1234567891011121314151617181920212223242526272829303132333435363738private static void test3() throws InterruptedException &#123; Vector&lt;Dog&gt; list = new Vector&lt;&gt;(); Thread t1 = new Thread(() -&gt; &#123; for (int i = 0; i &lt; 30; i++) &#123; Dog d = new Dog(); list.add(d); synchronized (d) &#123; log.debug(i + &quot;\\t&quot; + ClassLayout.parseInstance(d).toPrintableSimple(true)); &#125; &#125; synchronized (list) &#123; list.notify(); &#125; &#125;, &quot;t1&quot;); t1.start(); Thread t2 = new Thread(() -&gt; &#123; synchronized (list) &#123; try &#123; list.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; log.debug(&quot;===============&gt; &quot;); for (int i = 0; i &lt; 30; i++) &#123; Dog d = list.get(i); log.debug(i + &quot;\\t&quot; + ClassLayout.parseInstance(d).toPrintableSimple(true)); synchronized (d) &#123; log.debug(i + &quot;\\t&quot; + ClassLayout.parseInstance(d).toPrintableSimple(true)); &#125; log.debug(i + &quot;\\t&quot; + ClassLayout.parseInstance(d).toPrintableSimple(true)); &#125; &#125;, &quot;t2&quot;); t2.start();&#125; 输出 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122[t1] - 0 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t1] - 1 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t1] - 2 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t1] - 3 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t1] - 4 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t1] - 5 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t1] - 6 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t1] - 7 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t1] - 8 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t1] - 9 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t1] - 10 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t1] - 11 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t1] - 12 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t1] - 13 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t1] - 14 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t1] - 15 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t1] - 16 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t1] - 17 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t1] - 18 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t1] - 19 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t1] - 20 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t1] - 21 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t1] - 22 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t1] - 23 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t1] - 24 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t1] - 25 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t1] - 26 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t1] - 27 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t1] - 28 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t1] - 29 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t2] - ===============&gt; [t2] - 0 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t2] - 0 00000000 00000000 00000000 00000000 00100000 01011000 11110111 00000000 [t2] - 0 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000001 [t2] - 1 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t2] - 1 00000000 00000000 00000000 00000000 00100000 01011000 11110111 00000000 [t2] - 1 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000001 [t2] - 2 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t2] - 2 00000000 00000000 00000000 00000000 00100000 01011000 11110111 00000000 [t2] - 2 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000001 [t2] - 3 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t2] - 3 00000000 00000000 00000000 00000000 00100000 01011000 11110111 00000000 [t2] - 3 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000001 [t2] - 4 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t2] - 4 00000000 00000000 00000000 00000000 00100000 01011000 11110111 00000000 [t2] - 4 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000001 [t2] - 5 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t2] - 5 00000000 00000000 00000000 00000000 00100000 01011000 11110111 00000000 [t2] - 5 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000001 [t2] - 6 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t2] - 6 00000000 00000000 00000000 00000000 00100000 01011000 11110111 00000000 [t2] - 6 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000001 [t2] - 7 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101[t2] - 7 00000000 00000000 00000000 00000000 00100000 01011000 11110111 00000000 [t2] - 7 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000001 [t2] - 8 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t2] - 8 00000000 00000000 00000000 00000000 00100000 01011000 11110111 00000000 [t2] - 8 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000001 [t2] - 9 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t2] - 9 00000000 00000000 00000000 00000000 00100000 01011000 11110111 00000000 [t2] - 9 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000001 [t2] - 10 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t2] - 10 00000000 00000000 00000000 00000000 00100000 01011000 11110111 00000000 [t2] - 10 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000001 [t2] - 11 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t2] - 11 00000000 00000000 00000000 00000000 00100000 01011000 11110111 00000000 [t2] - 11 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000001 [t2] - 12 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t2] - 12 00000000 00000000 00000000 00000000 00100000 01011000 11110111 00000000 [t2] - 12 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000001 [t2] - 13 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t2] - 13 00000000 00000000 00000000 00000000 00100000 01011000 11110111 00000000 [t2] - 13 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000001 [t2] - 14 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t2] - 14 00000000 00000000 00000000 00000000 00100000 01011000 11110111 00000000 [t2] - 14 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000001 [t2] - 15 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t2] - 15 00000000 00000000 00000000 00000000 00100000 01011000 11110111 00000000 [t2] - 15 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000001 [t2] - 16 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t2] - 16 00000000 00000000 00000000 00000000 00100000 01011000 11110111 00000000 [t2] - 16 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000001 [t2] - 17 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t2] - 17 00000000 00000000 00000000 00000000 00100000 01011000 11110111 00000000 [t2] - 17 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000001 [t2] - 18 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t2] - 18 00000000 00000000 00000000 00000000 00100000 01011000 11110111 00000000 [t2] - 18 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000001 //可以看到在19次后，Dog类型对象偏向锁直接从t1偏向了t2[t2] - 19 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t2] - 19 00000000 00000000 00000000 00000000 00011111 11110011 11110001 00000101 [t2] - 19 00000000 00000000 00000000 00000000 00011111 11110011 11110001 00000101 [t2] - 20 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t2] - 20 00000000 00000000 00000000 00000000 00011111 11110011 11110001 00000101 [t2] - 20 00000000 00000000 00000000 00000000 00011111 11110011 11110001 00000101 [t2] - 21 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t2] - 21 00000000 00000000 00000000 00000000 00011111 11110011 11110001 00000101 [t2] - 21 00000000 00000000 00000000 00000000 00011111 11110011 11110001 00000101 [t2] - 22 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t2] - 22 00000000 00000000 00000000 00000000 00011111 11110011 11110001 00000101 [t2] - 22 00000000 00000000 00000000 00000000 00011111 11110011 11110001 00000101 [t2] - 23 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t2] - 23 00000000 00000000 00000000 00000000 00011111 11110011 11110001 00000101 [t2] - 23 00000000 00000000 00000000 00000000 00011111 11110011 11110001 00000101 [t2] - 24 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t2] - 24 00000000 00000000 00000000 00000000 00011111 11110011 11110001 00000101 [t2] - 24 00000000 00000000 00000000 00000000 00011111 11110011 11110001 00000101[t2] - 25 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t2] - 25 00000000 00000000 00000000 00000000 00011111 11110011 11110001 00000101 [t2] - 25 00000000 00000000 00000000 00000000 00011111 11110011 11110001 00000101 [t2] - 26 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t2] - 26 00000000 00000000 00000000 00000000 00011111 11110011 11110001 00000101 [t2] - 26 00000000 00000000 00000000 00000000 00011111 11110011 11110001 00000101 [t2] - 27 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t2] - 27 00000000 00000000 00000000 00000000 00011111 11110011 11110001 00000101 [t2] - 27 00000000 00000000 00000000 00000000 00011111 11110011 11110001 00000101 [t2] - 28 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t2] - 28 00000000 00000000 00000000 00000000 00011111 11110011 11110001 00000101 [t2] - 28 00000000 00000000 00000000 00000000 00011111 11110011 11110001 00000101 [t2] - 29 00000000 00000000 00000000 00000000 00011111 11110011 11100000 00000101 [t2] - 29 00000000 00000000 00000000 00000000 00011111 11110011 11110001 00000101 [t2] - 29 00000000 00000000 00000000 00000000 00011111 11110011 11110001 00000101 5）批量撤销(偏向)​ 当撤销偏向锁阈值超过 40 次后，jvm 会这样觉得，自己确实偏向错了，根本就不该偏向。于是整个类的所有对象都会变为不可偏向的，新建的该类型对象也是不可偏向的 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849static Thread t1,t2,t3;private static void test4() throws InterruptedException &#123; Vector&lt;Dog&gt; list = new Vector&lt;&gt;(); int loopNumber = 39; t1 = new Thread(() -&gt; &#123; for (int i = 0; i &lt; loopNumber; i++) &#123; Dog d = new Dog(); list.add(d); synchronized (d) &#123; log.debug(i + &quot;\\t&quot; + ClassLayout.parseInstance(d).toPrintableSimple(true)); &#125; &#125; LockSupport.unpark(t2); &#125;, &quot;t1&quot;); t1.start(); t2 = new Thread(() -&gt; &#123; LockSupport.park(); log.debug(&quot;===============&gt; &quot;); for (int i = 0; i &lt; loopNumber; i++) &#123; Dog d = list.get(i); log.debug(i + &quot;\\t&quot; + ClassLayout.parseInstance(d).toPrintableSimple(true)); synchronized (d) &#123; log.debug(i + &quot;\\t&quot; + ClassLayout.parseInstance(d).toPrintableSimple(true)); &#125; log.debug(i + &quot;\\t&quot; + ClassLayout.parseInstance(d).toPrintableSimple(true)); &#125; LockSupport.unpark(t3); &#125;, &quot;t2&quot;); t2.start(); t3 = new Thread(() -&gt; &#123; LockSupport.park(); log.debug(&quot;===============&gt; &quot;); for (int i = 0; i &lt; loopNumber; i++) &#123; Dog d = list.get(i); log.debug(i + &quot;\\t&quot; + ClassLayout.parseInstance(d).toPrintableSimple(true)); synchronized (d) &#123; log.debug(i + &quot;\\t&quot; + ClassLayout.parseInstance(d).toPrintableSimple(true)); &#125; log.debug(i + &quot;\\t&quot; + ClassLayout.parseInstance(d).toPrintableSimple(true)); &#125; &#125;, &quot;t3&quot;); t3.start(); t3.join(); log.debug(ClassLayout.parseInstance(new Dog()).toPrintableSimple(true));&#125; ​ t1首先获取了39个偏向t1的锁，再唤醒t2获取该锁的时候，前19个会升级成为轻量锁，后20个会批量重偏向t2，然后再唤醒t3，前19个因为是不可偏向（t2的前19个），从20个开始也不会发生重偏向，因为JVM觉得该类的对象偏向改变太多了，根本就不该偏向 参考资料 origin：https://www.yuque.com/mo_ming/gl7b70/rr1o32#l568M https://github.com/farmerjohngit/myblog/issues/12 https://www.cnblogs.com/LemonFive/p/11246086.html https://www.cnblogs.com/LemonFive/p/11248248.html https://www.oracle.com/technetwork/java/biasedlocking-oopsla2006-wp-149958.pdf 6）锁消除锁消除 JIT即时编译器会对字节码做进一步优化 123456789101112131415161718192021@Fork(1)@BenchmarkMode(Mode.AverageTime)@Warmup(iterations=3)@Measurement(iterations=5)@OutputTimeUnit(TimeUnit.NANOSECONDS)public class MyBenchmark &#123; static int x = 0; @Benchmark public void a() throws Exception &#123; x++; &#125; @Benchmark public void b() throws Exception &#123; //这里的o是局部变量,不会被共享,JIT做热点代码优化时会做锁消除 Object o = new Object(); synchronized (o) &#123; x++; &#125; &#125;&#125;java -jar benchmarks.jar 发现两部分的差别并不大,甚至b加了锁比a没加锁还快 1xxxxxxxxxx java -XX:-EliminateLocks -jar benchmarks.jar 使用 -XX:-EliminateLocks禁用锁消除后就会发现 b性能比a差劲多了 原因 JIT发现该局部变量并不会逃逸出该方法，不会被共享，所以在优化的时候就干脆取消synchronized 6.wait-notify1）为什么需要 wait 由于条件不满足，小南不能继续进行计算 但小南如果一直占用着锁，其它人就得一直阻塞，效率太低 于是老王单开了一间休息室（调用 wait 方法），让小南到休息室（WaitSet）等着去了，但这时锁释放开，其它人可以由老王随机安排进屋 直到小M将烟送来，大叫一声 [ 你的烟到了 ] （调用 notify 方法） 小南于是可以离开休息室，重新进入竞争锁的队列 2）原理 Owner 线程发现条件不满足，调用 wait 方法，即可进入 WaitSet 变为 WAITING 状态 BLOCKED 和 WAITING 的线程都处于阻塞状态，不占用 CPU 时间片 BLOCKED 线程会在 Owner 线程释放锁时唤醒 WAITING 线程会在 Owner 线程调用 notify 或 notifyAll 时唤醒，但唤醒后并不意味者立刻获得锁，仍需进入EntryList 重新竞争 3）API 介绍 obj.wait() 让进入 object 监视器的线程到 waitSet 等待 obj.wait(long)在无锁竞争情况下，在等待时间过去后就直接重新获取锁，往后执行；但是在竞争条件下，都会等获取到锁了才可以往下执行 obj.notify() 在 object 上正在 waitSet 等待的线程中挑一个唤醒 obj.notifyAll() 让 object 上正在 waitSet 等待的线程全部唤醒 它们都是线程之间进行协作的手段，都属于 Object 对象的方法。必须获得此对象的锁，才能调用这几个方法 1234567891011121314151617181920212223242526272829303132333435final static Object obj = new Object();public static void main(String[] args) &#123; new Thread(() -&gt; &#123; synchronized (obj) &#123; log.debug(&quot;执行....&quot;); try &#123; obj.wait(); // 让线程在obj上一直等待下去 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; log.debug(&quot;其它代码....&quot;); &#125; &#125;).start(); new Thread(() -&gt; &#123; synchronized (obj) &#123; log.debug(&quot;执行....&quot;); try &#123; obj.wait(); // 让线程在obj上一直等待下去 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; log.debug(&quot;其它代码....&quot;); &#125; &#125;).start(); // 主线程两秒后执行 sleep(2); log.debug(&quot;唤醒 obj 上其它线程&quot;); synchronized (obj) &#123; obj.notify(); // 唤醒obj上一个线程 // obj.notifyAll(); // 唤醒obj上所有等待线程 &#125;&#125; notify 的一种结果 123420:00:53.096 [Thread-0] c.TestWaitNotify - 执行.... 20:00:53.099 [Thread-1] c.TestWaitNotify - 执行.... 20:00:55.096 [main] c.TestWaitNotify - 唤醒 obj 上其它线程20:00:55.096 [Thread-0] c.TestWaitNotify - 其它代码.... notifyAll 的结果 1234519:58:15.457 [Thread-0] c.TestWaitNotify - 执行.... 19:58:15.460 [Thread-1] c.TestWaitNotify - 执行.... 19:58:17.456 [main] c.TestWaitNotify - 唤醒 obj 上其它线程19:58:17.456 [Thread-1] c.TestWaitNotify - 其它代码.... 19:58:17.456 [Thread-0] c.TestWaitNotify - 其它代码.... wait() 方法会释放对象的锁，进入 WaitSet 等待区，从而让其他线程就机会获取对象的锁。无限制等待，直到notify 为止 wait(long n) 有时限的等待 4）sleep(long n) 和 wait(long n) 的区别 sleep 是 Thread 方法，而 wait 是 Object 的方法 sleep 不需要强制和 synchronized 配合使用，但 wait 需要和 synchronized 一起用 sleep 在睡眠的同时，不会释放对象锁的，但 wait 在等待的时候会释放对象锁 它们状态都是 TIMED_WAITING 5）具体使用①sleep会阻碍其它线程执行123static final Object room = new Object();static boolean hasCigarette = false;static boolean hasTakeout = false; 思考下面的解决方案好不好，为什么？ 1234567891011121314151617181920212223242526272829new Thread(() -&gt; &#123; synchronized (room) &#123; log.debug(&quot;有烟没？[&#123;&#125;]&quot;, hasCigarette); if (!hasCigarette) &#123; log.debug(&quot;没烟，先歇会！&quot;); sleep(2); &#125; log.debug(&quot;有烟没？[&#123;&#125;]&quot;, hasCigarette); if (hasCigarette) &#123; log.debug(&quot;可以开始干活了&quot;); &#125; &#125;&#125;, &quot;小南&quot;).start();for (int i = 0; i &lt; 5; i++) &#123; new Thread(() -&gt; &#123; synchronized (room) &#123; log.debug(&quot;可以开始干活了&quot;); &#125; &#125;, &quot;其它人&quot;).start();&#125;sleep(1);new Thread(() -&gt; &#123; // 这里能不能加 synchronized (room)？ 不能 hasCigarette = true; log.debug(&quot;烟到了噢！&quot;);&#125;, &quot;送烟的&quot;).start(); 输出 1234567891020:49:49.883 [小南] c.TestCorrectPosture - 有烟没？[false] 20:49:49.887 [小南] c.TestCorrectPosture - 没烟，先歇会！20:49:50.882 [送烟的] c.TestCorrectPosture - 烟到了噢！20:49:51.887 [小南] c.TestCorrectPosture - 有烟没？[true] 20:49:51.887 [小南] c.TestCorrectPosture - 可以开始干活了20:49:51.887 [其它人] c.TestCorrectPosture - 可以开始干活了20:49:51.887 [其它人] c.TestCorrectPosture - 可以开始干活了20:49:51.888 [其它人] c.TestCorrectPosture - 可以开始干活了20:49:51.888 [其它人] c.TestCorrectPosture - 可以开始干活了20:49:51.888 [其它人] c.TestCorrectPosture - 可以开始干活了 其它干活的线程，都要一直阻塞，效率太低 小南线程必须睡足 2s 后才能醒来，就算烟提前送到，也无法立刻醒来 加了 synchronized (room) 后，就好比小南在里面反锁了门睡觉，烟根本没法送进门，main 没加 synchronized 就好像 main 线程是翻窗户进来的 sleep妨碍其它人干活 解决方法，使用 wait - notify ②wait替代sleep思考下面的实现行吗，为什么？ 12345678910111213141516171819202122232425262728293031323334new Thread(() -&gt; &#123; synchronized (room) &#123; log.debug(&quot;有烟没？[&#123;&#125;]&quot;, hasCigarette); if (!hasCigarette) &#123; log.debug(&quot;没烟，先歇会！&quot;); try &#123; room.wait(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; log.debug(&quot;有烟没？[&#123;&#125;]&quot;, hasCigarette); if (hasCigarette) &#123; log.debug(&quot;可以开始干活了&quot;); &#125; &#125;&#125;, &quot;小南&quot;).start();for (int i = 0; i &lt; 5; i++) &#123; new Thread(() -&gt; &#123; synchronized (room) &#123; log.debug(&quot;可以开始干活了&quot;); &#125; &#125;, &quot;其它人&quot;).start();&#125;sleep(1);new Thread(() -&gt; &#123; synchronized (room) &#123; hasCigarette = true; log.debug(&quot;烟到了噢！&quot;); room.notify(); &#125;&#125;, &quot;送烟的&quot;).start(); 输出 1234567891020:51:42.489 [小南] c.TestCorrectPosture - 有烟没？[false] 20:51:42.493 [小南] c.TestCorrectPosture - 没烟，先歇会！20:51:42.493 [其它人] c.TestCorrectPosture - 可以开始干活了20:51:42.493 [其它人] c.TestCorrectPosture - 可以开始干活了20:51:42.494 [其它人] c.TestCorrectPosture - 可以开始干活了20:51:42.494 [其它人] c.TestCorrectPosture - 可以开始干活了20:51:42.494 [其它人] c.TestCorrectPosture - 可以开始干活了20:51:43.490 [送烟的] c.TestCorrectPosture - 烟到了噢！20:51:43.490 [小南] c.TestCorrectPosture - 有烟没？[true] 20:51:43.490 [小南] c.TestCorrectPosture - 可以开始干活了 解决了其它干活的线程阻塞的问题 但如果有其它线程也在等待条件呢？ ③会发生虚假唤醒12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849new Thread(() -&gt; &#123; synchronized (room) &#123; log.debug(&quot;有烟没？[&#123;&#125;]&quot;, hasCigarette); if (!hasCigarette) &#123; log.debug(&quot;没烟，先歇会！&quot;); try &#123; room.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; log.debug(&quot;有烟没？[&#123;&#125;]&quot;, hasCigarette); if (hasCigarette) &#123; log.debug(&quot;可以开始干活了&quot;); &#125; else &#123; log.debug(&quot;没干成活...&quot;); &#125; &#125;&#125;, &quot;小南&quot;).start();new Thread(() -&gt; &#123; synchronized (room) &#123; Thread thread = Thread.currentThread(); log.debug(&quot;外卖送到没？[&#123;&#125;]&quot;, hasTakeout); if (!hasTakeout) &#123; log.debug(&quot;没外卖，先歇会！&quot;); try &#123; room.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; log.debug(&quot;外卖送到没？[&#123;&#125;]&quot;, hasTakeout); if (hasTakeout) &#123; log.debug(&quot;可以开始干活了&quot;); &#125; else &#123; log.debug(&quot;没干成活...&quot;); &#125; &#125;&#125;, &quot;小女&quot;).start();sleep(1);new Thread(() -&gt; &#123; synchronized (room) &#123; hasTakeout = true; log.debug(&quot;外卖到了噢！&quot;); room.notify(); &#125;&#125;, &quot;送外卖的&quot;).start(); 输出 123456720:53:12.173 [小南] c.TestCorrectPosture - 有烟没？[false] 20:53:12.176 [小南] c.TestCorrectPosture - 没烟，先歇会！20:53:12.176 [小女] c.TestCorrectPosture - 外卖送到没？[false] 20:53:12.176 [小女] c.TestCorrectPosture - 没外卖，先歇会！20:53:13.174 [送外卖的] c.TestCorrectPosture - 外卖到了噢！20:53:13.174 [小南] c.TestCorrectPosture - 有烟没？[false] 20:53:13.174 [小南] c.TestCorrectPosture - 没干成活... notify 只能随机唤醒一个 WaitSet 中的线程，这时如果有其它线程也在等待，那么就可能唤醒不了正确的线程，称之为【虚假唤醒】 发生虚假唤醒: 解决方法，改为 notifyAll ④if+wait 仅由1次判断机会12345678if (!hasCigarette) &#123; log.debug(&quot;没烟，先歇会！&quot;); try &#123; room.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;&#125; 1234567new Thread(() -&gt; &#123; synchronized (room) &#123; hasTakeout = true; log.debug(&quot;外卖到了噢！&quot;); room.notifyAll(); &#125;&#125;, &quot;送外卖的&quot;).start(); 输出 12345678920:55:23.978 [小南] c.TestCorrectPosture - 有烟没？[false] 20:55:23.982 [小南] c.TestCorrectPosture - 没烟，先歇会！20:55:23.982 [小女] c.TestCorrectPosture - 外卖送到没？[false] 20:55:23.982 [小女] c.TestCorrectPosture - 没外卖，先歇会！20:55:24.979 [送外卖的] c.TestCorrectPosture - 外卖到了噢！20:55:24.979 [小女] c.TestCorrectPosture - 外卖送到没？[true] 20:55:24.980 [小女] c.TestCorrectPosture - 可以开始干活了20:55:24.980 [小南] c.TestCorrectPosture - 有烟没？[false] 20:55:24.980 [小南] c.TestCorrectPosture - 没干成活... 用 notifyAll 仅解决某个线程的唤醒问题，但使用 if + wait 判断仅有一次机会，一旦条件不成立，就没有重新判断的机会了 notifyAll唤醒了所有,但使用if+wait仅有一次机会,解决方法，一旦条件不成立，就没有重新判断的机会了.解决办法: 用 while + wait，当条件不成立，再次 wait ⑤while+wait 将 if 改为 while 12345678if (!hasCigarette) &#123; log.debug(&quot;没烟，先歇会！&quot;); try &#123; room.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;&#125; 改动后 12345678while (!hasCigarette) &#123; log.debug(&quot;没烟，先歇会！&quot;); try &#123; room.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;&#125; 输出 1234567820:58:34.322 [小南] c.TestCorrectPosture - 有烟没？[false] 20:58:34.326 [小南] c.TestCorrectPosture - 没烟，先歇会！20:58:34.326 [小女] c.TestCorrectPosture - 外卖送到没？[false] 20:58:34.326 [小女] c.TestCorrectPosture - 没外卖，先歇会！20:58:35.323 [送外卖的] c.TestCorrectPosture - 外卖到了噢！20:58:35.324 [小女] c.TestCorrectPosture - 外卖送到没？[true] 20:58:35.324 [小女] c.TestCorrectPosture - 可以开始干活了20:58:35.324 [小南] c.TestCorrectPosture - 没烟，先歇会！ 1234567891011synchronized(lock) &#123; while(条件不成立) &#123; lock.wait(); &#125; // 干活 &#125; //另一个线程 synchronized(lock) &#123; lock.notifyAll(); &#125; 7. (同步)模式之保护性暂停1） 定义即 Guarded Suspension，用在一个线程等待另一个线程的执行结果 要点 有一个结果需要从一个线程传递到另一个线程，让他们关联同一个 GuardedObject 如果有结果不断从一个线程到另一个线程那么可以使用消息队列（见生产者&#x2F;消费者） JDK 中，join 的实现、Future 的实现，采用的就是此模式 因为要等待另一方的结果，因此归类到同步模式 其实就是通过一个middle man来接收需要的资源 2）实现12345678910111213141516171819202122232425262728class GuardedObject &#123; private Object response; private final Object lock = new Object(); public Object get() &#123; synchronized (lock) &#123; // 条件不满足则等待 while (response == null) &#123; try &#123; lock.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; return response; &#125; &#125; public void complete(Object response) &#123; synchronized (lock) &#123; // 条件满足，通知等待线程 this.response = response; lock.notifyAll(); &#125; &#125; &#125; eg： 一个线程等待另一个线程的执行结果 1234567891011121314151617181920public static void main(String[] args) &#123; GuardedObject guardedObject = new GuardedObject(); new Thread(() -&gt; &#123; try &#123; // 子线程执行下载 List&lt;String&gt; response = download(); log.debug(&quot;download complete...&quot;); guardedObject.complete(response); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;).start(); log.debug(&quot;waiting...&quot;); // 主线程阻塞等待 Object response = guardedObject.get(); log.debug(&quot;get response: [&#123;&#125;] lines&quot;, ((List&lt;String&gt;) response).size());&#125; 执行结果： 12308:42:18.568 [main] c.TestGuardedObject - waiting...08:42:23.312 [Thread-0] c.TestGuardedObject - download complete...08:42:23.312 [main] c.TestGuardedObject - get response: [3] lines 3）超时 GuardedObject如果要控制超时时间呢 123456789101112131415161718192021222324252627282930313233343536373839404142434445class GuardedObjectV2 &#123; private Object response; private final Object lock = new Object(); public Object get(long millis) &#123; synchronized (lock) &#123; // 1) 记录最初时间 long begin = System.currentTimeMillis(); // 2) 已经经历的时间 long timePassed = 0; while (response == null) &#123; // 4) 假设 millis 是 1000，结果在 400 时被虚假唤醒了，那么还有 600 要等 long waitTime = millis - timePassed; log.debug(&quot;waitTime: &#123;&#125;&quot;, waitTime); if (waitTime &lt;= 0) &#123; log.debug(&quot;break...&quot;); break; &#125; try &#123; lock.wait(waitTime); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; // 3) 如果提前被唤醒，这时已经经历的时间假设为 400 timePassed = System.currentTimeMillis() - begin; log.debug(&quot;timePassed: &#123;&#125;, object is null &#123;&#125;&quot;, timePassed, response == null); &#125; return response; &#125; &#125; public void complete(Object response) &#123; synchronized (lock) &#123; // 条件满足，通知等待线程 this.response = response; log.debug(&quot;notify...&quot;); lock.notifyAll(); &#125; &#125;&#125; 4）join原理12345678910111213141516171819202122public final synchronized void join(long millis)throws InterruptedException &#123; long base = System.currentTimeMillis(); long now = 0; if (millis &lt; 0) &#123; throw new IllegalArgumentException(&quot;timeout value is negative&quot;); &#125; if (millis == 0) &#123; while (isAlive()) &#123; wait(0); &#125; &#125; else &#123; //这就是3）中带超时的实现 while (isAlive()) &#123; long delay = millis - now; if (delay &lt;= 0) &#123; break; &#125; wait(delay); now = System.currentTimeMillis() - base; &#125; &#125;&#125; 5） 多任务版 GuardedObject​ 图中 Futures 就好比居民楼一层的信箱（每个信箱有房间编号），左侧的 t0，t2，t4 就好比等待邮件的居民，右侧的 t1，t3，t5 就好比邮递员，如果需要在多个类之间使用 GuardedObject 对象，作为参数传递不是很方便，因此设计一个用来解耦的中间类， 这样不仅能够解耦【结果等待者】和【结果生产者】，还能够同时支持多个任务的管理 新增 id 用来标识 Guarded Object 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class GuardedObject &#123; // 标识 Guarded Object private int id; public GuardedObject(int id) &#123; this.id = id; &#125; public int getId() &#123; return id; &#125; // 结果 private Object response; // 获取结果 // timeout 表示要等待多久 2000 public Object get(long timeout) &#123; synchronized (this) &#123; // 开始时间 15:00:00 long begin = System.currentTimeMillis(); // 经历的时间 long passedTime = 0; while (response == null) &#123; // 这一轮循环应该等待的时间 long waitTime = timeout - passedTime; // 经历的时间超过了最大等待时间时，退出循环 if (timeout - passedTime &lt;= 0) &#123; break; &#125; try &#123; this.wait(waitTime); // 虚假唤醒 15:00:01 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; // 求得经历时间 passedTime = System.currentTimeMillis() - begin; // 15:00:02 1s &#125; return response; &#125; &#125; // 产生结果 public void complete(Object response) &#123; synchronized (this) &#123; // 给结果成员变量赋值 this.response = response; this.notifyAll(); &#125; &#125;&#125; 中间解耦类 1234567891011121314151617181920212223class Mailboxes &#123; private static Map&lt;Integer, GuardedObject&gt; boxes = new Hashtable&lt;&gt;(); private static int id = 1; // 产生唯一 id private static synchronized int generateId() &#123; return id++; &#125; public static GuardedObject getGuardedObject(int id) &#123; return boxes.remove(id); &#125; public static GuardedObject createGuardedObject() &#123; GuardedObject go = new GuardedObject(generateId()); boxes.put(go.getId(), go); return go; &#125; public static Set&lt;Integer&gt; getIds() &#123; return boxes.keySet(); &#125;&#125; 业务相关类 1234567891011121314151617181920212223242526class People extends Thread&#123; @Override public void run() &#123; // 收信人首先创建一个自己的邮箱并得到id GuardedObject guardedObject = Mailboxes.createGuardedObject(); log.debug(&quot;开始收信 id:&#123;&#125;&quot;, guardedObject.getId()); Object mail = guardedObject.get(5000); log.debug(&quot;收到信 id:&#123;&#125;, 内容:&#123;&#125;&quot;, guardedObject.getId(), mail); &#125;&#125;class Postman extends Thread &#123; private int id; private String mail; public Postman(int id, String mail) &#123;//邮递员首先需要获得送信对象的id，还有信件内容 this.id = id; this.mail = mail; &#125; @Override public void run() &#123; //邮递员根据邮箱id找到对应的邮箱，并放入如邮件 GuardedObject guardedObject = Mailboxes.getGuardedObject(id); log.debug(&quot;送信 id:&#123;&#125;, 内容:&#123;&#125;&quot;, id, mail); guardedObject.complete(mail); &#125;&#125; 测试 123456789public static void main(String[] args) throws InterruptedException &#123; for (int i = 0; i &lt; 3; i++) &#123; new People().start(); &#125; Sleeper.sleep(1); for (Integer id : Mailboxes.getIds()) &#123; new Postman(id, &quot;内容&quot; + id).start(); &#125;&#125; 某次运行结果 12345678910:35:05.689 c.People [Thread-1] - 开始收信 id:310:35:05.689 c.People [Thread-2] - 开始收信 id:110:35:05.689 c.People [Thread-0] - 开始收信 id:210:35:06.688 c.Postman [Thread-4] - 送信 id:2, 内容:内容210:35:06.688 c.Postman [Thread-5] - 送信 id:1, 内容:内容110:35:06.688 c.People [Thread-0] - 收到信 id:2, 内容:内容210:35:06.688 c.People [Thread-2] - 收到信 id:1, 内容:内容110:35:06.688 c.Postman [Thread-3] - 送信 id:3, 内容:内容310:35:06.689 c.People [Thread-1] - 收到信 id:3, 内容:内容3 8.(异步)模式之生产者&#x2F;消费者1） 定义要点 与前面的保护性暂停中的 GuardObject 不同，不需要产生结果和消费结果的线程一一对应 一户人家对应一个id的邮箱，需要有一个邮递员来送信到对应的邮箱中，是1对1的关系 消费队列可以用来平衡生产和消费的线程资源 生产者仅负责产生结果数据，不关心数据该如何处理，而消费者专心处理结果数据 消息队列是有容量限制的，满时不会再加入数据，空时不会再消耗数据 JDK 中各种阻塞队列，采用的就是这种模式 2.）实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556class Message &#123; private int id; private Object message; public Message(int id, Object message) &#123; this.id = id; this.message = message; &#125; public int getId() &#123; return id; &#125; public Object getMessage() &#123; return message; &#125; //没有setter是不想消息中途被修改&#125;class MessageQueue &#123; private LinkedList&lt;Message&gt; queue; private int capacity; public MessageQueue(int capacity) &#123; this.capacity = capacity; queue = new LinkedList&lt;&gt;(); &#125; public Message take() &#123; synchronized (queue) &#123; while (queue.isEmpty()) &#123; log.debug(&quot;没货了, wait&quot;); try &#123; queue.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; Message message = queue.removeFirst(); queue.notifyAll(); return message; &#125; &#125; public void put(Message message) &#123; synchronized (queue) &#123; while (queue.size() == capacity) &#123; log.debug(&quot;库存已达上限, wait&quot;); try &#123; queue.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; queue.addLast(message); queue.notifyAll(); &#125; &#125;&#125; MsgQueue的take和put方法都对msgQueue对象上锁了的，所以线程在并发访问两种会竞争 take方法会判断queue.isEmpty() 空了没货会wait等待，让出锁，唤醒waitSet中的线程，等待一个put线程上货 有货则拿第一个货，然后notifyAll唤醒waitSet（因为有可能货满了，生产者无法放货），然后交出锁，使entrySet中的线程能取 put方法会判断queue.size() &#x3D;&#x3D; capacity是否已经满仓了 满仓了也会等待，让出锁，唤醒waitSet中的线程，等待一个take线程拿货 没满仓则会防货，然后notifyAll唤醒waitSet（因为有可能放货之前就没货了，消费者无法放货），然后交出锁，使entrySet中的线程能取 ​ 3）应用12345678910111213141516171819202122232425MessageQueue messageQueue = new MessageQueue(2);// 4 个生产者线程, 下载任务for (int i = 0; i &lt; 4; i++) &#123; int id = i; new Thread(() -&gt; &#123; try &#123; log.debug(&quot;download...&quot;); List&lt;String&gt; response = Downloader.download(); log.debug(&quot;try put message(&#123;&#125;)&quot;, id); messageQueue.put(new Message(id, response)); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;, &quot;生产者&quot; + i).start();&#125;// 1 个消费者线程, 处理结果new Thread(() -&gt; &#123; while (true) &#123; Message message = messageQueue.take(); List&lt;String&gt; response = (List&lt;String&gt;) message.getMessage(); log.debug(&quot;take message(&#123;&#125;): [&#123;&#125;] lines&quot;, message.getId(), response.size()); &#125;&#125;, &quot;消费者&quot;).start(); 某次运行结果： 123456789101112131415161710:48:38.070 [生产者3] c.TestProducerConsumer - download...10:48:38.070 [生产者0] c.TestProducerConsumer - download...10:48:38.070 [消费者] c.MessageQueue - 没货了, wait10:48:38.070 [生产者1] c.TestProducerConsumer - download...10:48:38.070 [生产者2] c.TestProducerConsumer - download...10:48:41.236 [生产者1] c.TestProducerConsumer - try put message(1)10:48:41.237 [生产者2] c.TestProducerConsumer - try put message(2)10:48:41.236 [生产者0] c.TestProducerConsumer - try put message(0)10:48:41.237 [生产者3] c.TestProducerConsumer - try put message(3)10:48:41.239 [生产者2] c.MessageQueue - 库存已达上限, wait10:48:41.240 [生产者1] c.MessageQueue - 库存已达上限, wait10:48:41.240 [消费者] c.TestProducerConsumer - take message(0): [3] lines10:48:41.240 [生产者2] c.MessageQueue - 库存已达上限, wait10:48:41.240 [消费者] c.TestProducerConsumer - take message(3): [3] lines10:48:41.240 [消费者] c.TestProducerConsumer - take message(1): [3] lines10:48:41.240 [消费者] c.TestProducerConsumer - take message(2): [3] lines10:48:41.240 [消费者] c.MessageQueue - 没货了, wait 9.Park &amp; Unpark1）基本使用它们是 LockSupport 类中的方法 1234// 暂停当前线程LockSupport.park(); // 恢复某个线程的运行LockSupport.unpark(暂停线程对象) 先 park 再 unpark 123456789101112Thread t1 = new Thread(() -&gt; &#123; log.debug(&quot;start...&quot;); sleep(1); log.debug(&quot;park...&quot;); LockSupport.park(); log.debug(&quot;resume...&quot;);&#125;,&quot;t1&quot;);t1.start();sleep(2);log.debug(&quot;unpark...&quot;);LockSupport.unpark(t1); 输出 123418:42:52.585 c.TestParkUnpark [t1] - start... 18:42:53.589 c.TestParkUnpark [t1] - park... 18:42:54.583 c.TestParkUnpark [main] - unpark... 18:42:54.583 c.TestParkUnpark [t1] - resume... 先 unpark 再 park 123456789101112Thread t1 = new Thread(() -&gt; &#123; log.debug(&quot;start...&quot;); sleep(2); log.debug(&quot;park...&quot;); LockSupport.park(); log.debug(&quot;resume...&quot;);&#125;, &quot;t1&quot;);t1.start();sleep(1);log.debug(&quot;unpark...&quot;);LockSupport.unpark(t1); 输出 123418:43:50.765 c.TestParkUnpark [t1] - start... 18:43:51.764 c.TestParkUnpark [main] - unpark... 18:43:52.769 c.TestParkUnpark [t1] - park... 18:43:52.769 c.TestParkUnpark [t1] - resume... 2）特点与 Object 的 wait &amp; notify 相比 wait，notify 和 notifyAll 必须配合 Object Monitor 一起使用，而 park，unpark 不必 park &amp; unpark 是以线程为单位来【阻塞】和【唤醒】线程，而 notify 只能随机唤醒一个等待线程，notifyAll是唤醒所有等待线程，就不那么【精确】 park &amp; unpark 可以先 unpark，而 wait &amp; notify 不能先 notify 3）原理每个线程都有自己的一个(C代码实现的) Parker 对象，由三部分组成 _counter ， _cond 和_mutex 打个比喻 ： 线程就像一个旅人，Parker 就像他随身携带的背包，条件变量就好比背包中的帐篷。_counter 就好比背包中的备用干粮（0 为耗尽，1 为充足） 调用 park 就是要看需不需要停下来歇息 如果备用干粮耗尽，那么钻进帐篷歇息 如果备用干粮充足，消耗掉干粮，那么不需停留，继续前进 调用 unpark，就好比令干粮充足 如果这时线程还在帐篷，就唤醒让他继续前进 如果这时线程还在运行，那么下次他调用 park 时，仅是消耗掉备用干粮，不需停留,继续前进 因为背包空间有限，多次调用 unpark 仅会补充一份备用干粮,也就是多次unpark后只会让紧跟着的一次park失效 mine： 线程的parker对象中有一个flag park操作会消耗掉一个flag，如果此时没有flag就会停下，有flag则会使flag-1&#x3D;0继续执行 unpark会增加一个flag，flag最多为1，如果有flag则不会增加 ①先调用park 再调用unpark park： 当前线程调用 Unsafe.park() 方法 检查 _counter ，本情况为 0，这时，获得 _mutex 互斥锁 线程进入 _cond 条件变量阻塞 设置 _counter &#x3D; 0 unpark： 调用 Unsafe.unpark(Thread_0) 方法，设置 _counter 为 1 唤醒 _cond 条件变量中的 Thread_0 Thread_0 恢复运行 设置 _counter 为 0 ②先调用unpark 再调用park 调用 Unsafe.unpark(Thread_0) 方法，设置 _counter 为 1 当前线程调用 Unsafe.park() 方法 检查 _counter ，本情况为 1，这时线程无需阻塞，继续运行 设置 _counter 为 0 10.理解线程状态转换概览图 假设有线程 Thread t，以下情况都对应上述图中的一个箭头 1）情况1 NEW –&gt; RUNNABLE 当调用 t.start() 方法时，由 NEW –&gt; RUNNABLE，new只是java层面创建了一个线程，还没有和操作系统关联 调用 t.start() ，java线程和操作线程关联，可以交给cpu执行 Runnable包含操作系统的三种状态： 就绪态，可运行，可分配时间片 阻塞态，IO阻塞 运行态，已经获得时间片正在运行 2）情况2 RUNNABLE &lt;–&gt; WAITINGt 线程用 synchronized(obj) 获取了对象锁后 调用 obj.wait() 方法时，t 线程从 RUNNABLE –&gt; WAITING 调用 obj.notify() ， obj.notifyAll() ， t.interrupt() 时，会重新进入entryList竞争锁 竞争锁成功，t 线程从WAITING –&gt; RUNNABLE 竞争锁失败，t 线程从WAITING –&gt; BLOCKED，阻塞在entryList 12345678910111213141516171819202122232425262728293031323334353637public class TestWaitNotify &#123; final static Object obj = new Object(); public static void main(String[] args) &#123; new Thread(() -&gt; &#123; synchronized (obj) &#123; log.debug(&quot;执行....&quot;); try &#123; obj.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; log.debug(&quot;其它代码....&quot;); // 断点 &#125; &#125;,&quot;t1&quot;).start(); new Thread(() -&gt; &#123; synchronized (obj) &#123; log.debug(&quot;执行....&quot;); try &#123; obj.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; log.debug(&quot;其它代码....&quot;); // 断点 &#125; &#125;,&quot;t2&quot;).start(); sleep(0.5); log.debug(&quot;唤醒 obj 上其它线程&quot;); synchronized (obj) &#123; obj.notifyAll(); // 唤醒obj上所有等待线程 断点 &#125; &#125;&#125; 3）情况 3 RUNNABLE &lt;–&gt; WAITING 当前线程a调用 t.join() 方法时，当前线程a从 RUNNABLE –&gt; WAITING 注意是当前线程a在t 线程对象的监视器上等待 t 线程运行结束会notify当前线程a，或调用了当前线程a的a.interrupt() 时，当前线程从 WAITING –&gt; RUNNABLE 4）情况 4 RUNNABLE &lt;–&gt; WAITING 当前线程t调用 LockSupport.park() 方法会让当前线程t从 RUNNABLE –&gt; WAITING 调用了当前线程t的LockSupport.unpark(目标线程) 或调用了线程t的 interrupt() ，会让目标线程从 WAITING –&gt;RUNNABLE 5）情况 5 RUNNABLE &lt;–&gt; TIMED_WAITINGt 线程用 synchronized(obj) 获取了对象锁后 调用 obj.wait(long n) 方法时，t 线程从 RUNNABLE –&gt; TIMED_WAITING t 线程等待时间超过了 n 毫秒，或调用 obj.notify() ， obj.notifyAll() ， t.interrupt() 时 竞争锁成功，t 线程从TIMED_WAITING –&gt; RUNNABLE 竞争锁失败，t 线程从TIMED_WAITING –&gt; BLOCKED 6）情况 6 RUNNABLE &lt;–&gt; TIMED_WAITING 当前线程a调用 t.join(long n) 方法时，当前线程a从 RUNNABLE –&gt; TIMED_WAITING 注意是当前线程在t线程对象的监视器上等待 当前线程a等待时间超过了 n 毫秒，或t线程运行结束，或调用了当前线程a的interrupt() 时，当前线程a从 TIMED_WAITING –&gt; RUNNABLE 7）情况 7 RUNNABLE &lt;–&gt; TIMED_WAITING 当前线程调用 Thread.sleep(long n) ，当前线程从 RUNNABLE –&gt; TIMED_WAITING 当前线程等待时间超过了 n 毫秒，当前线程从TIMED_WAITING –&gt; RUNNABLE 8）情况 8 RUNNABLE &lt;–&gt; TIMED_WAITING 当前线程调用 LockSupport.parkNanos(long nanos) 或 LockSupport.parkUntil(long millis) 时，当前线程a从 RUNNABLE –&gt; TIMED_WAITING 调用 LockSupport.unpark(目标线程) 或调用了线程 的 interrupt() ，或是等待超时，会让目标线程从 TIMED_WAITING–&gt; RUNNABLE 9）情况 9 RUNNABLE &lt;–&gt; BLOCKED t 线程用synchronized(obj) 获取对象锁时如果竞争失败，从RUNNABLE –&gt; BLOCKED 持obj锁线程的同步代码块执行完毕，会唤醒该对象上所有BLOCKED的线程重新竞争 如果其中 t 线程竞争 成功，从 BLOCKED –&gt; RUNNABLE ，其它失败的线程仍然BLOCKED 10）情况 10 RUNNABLE –&gt; TERMINATED当前线程所有代码运行完毕，进入 TERMINATED 11. 多把锁&amp;活跃性1）多把不相干的锁一间大屋子有两个功能：睡觉、学习，互不相干。 现在小南要学习，小女要睡觉，但如果只用一间屋子（一个对象锁）的话，那么并发度很低 解决方法是准备多个房间（多个对象锁） 例如： 123456789101112131415class BigRoom &#123; public void study() &#123; synchronized (this) &#123; log.debug(&quot;study 1 小时&quot;); Sleeper.sleep(1); &#125; &#125; public void sleep() &#123; synchronized (this) &#123; log.debug(&quot;sleeping 2 小时&quot;); Sleeper.sleep(2); &#125; &#125;&#125; 执行 123456789BigRoom bigRoom = new BigRoom();new Thread(() -&gt; &#123; bigRoom.study();&#125;,&quot;小南&quot;).start();new Thread(() -&gt; &#123; bigRoom.sleep();&#125;,&quot;小女&quot;).start(); 某次结果 1212:13:54.471 [小南] c.BigRoom - study 1 小时12:13:55.476 [小女] c.BigRoom - sleeping 2 小时 改进 123456789101112131415161718class BigRoom &#123; private final Object studyRoom = new Object(); private final Object bedRoom = new Object(); public void study() &#123; synchronized (studyRoom) &#123; log.debug(&quot;study 1 小时&quot;); Sleeper.sleep(1); &#125; &#125; public void sleep() &#123; synchronized (bedRoom) &#123; log.debug(&quot;sleeping 2 小时&quot;); Sleeper.sleep(2); &#125; &#125;&#125; 某次执行结果 1212:15:35.069 [小南] c.BigRoom - study 1 小时12:15:35.069 [小女] c.BigRoom - sleeping 2 小时 将锁的粒度细分 好处，是可以增强并发度 坏处，如果一个线程需要同时获得多把锁，就容易发生死锁 2）活跃性-死锁①死锁概念有这样的情况：一个线程需要同时获取多把锁，这时就容易发生死锁 t1 线程 获得 A对象 锁，接下来想获取 B对象 的锁 t2 线程 获得 B对象 锁，接下来想获取 A对象 的锁 如下： 123456789101112131415161718192021222324252627Object A = new Object();Object B = new Object();Thread t1 = new Thread(() -&gt; &#123; synchronized (A) &#123; log.debug(&quot;lock A&quot;); sleep(1); synchronized (B) &#123; log.debug(&quot;lock B&quot;); log.debug(&quot;操作...&quot;); &#125; &#125;&#125;, &quot;t1&quot;);Thread t2 = new Thread(() -&gt; &#123; synchronized (B) &#123; log.debug(&quot;lock B&quot;); sleep(0.5); synchronized (A) &#123; log.debug(&quot;lock A&quot;); log.debug(&quot;操作...&quot;); &#125; &#125;&#125;, &quot;t2&quot;);t1.start();t2.start(); 结果 1212:22:06.962 [t2] c.TestDeadLock - lock B 12:22:06.962 [t1] c.TestDeadLock - lock A ②定位死锁 检测死锁可以使用 jconsole工具，或者使用 jps 查看所有java进程id 再用 jstack 33200 定位死锁（jstack会显示该进程中各线程信息） 可以分析各线程情况 最下方还会有JVM自己的检测 还可以使用jconsle工具 连接到对应进程，左下角 ​ 避免死锁要注意加锁顺序 另外如果由于某个线程进入了死循环，导致其它线程一直等待，对于这种情况 linux 下可以通过 top 先定位到CPU 占用高的 Java 进程，再利用 top -Hp 进程id 来定位是哪个线程，最后再用 jstack 排查 ③哲学家就餐问题有五位哲学家，围坐在圆桌旁。 他们只做两件事，思考和吃饭，思考一会吃口饭，吃完饭后接着思考。 吃饭时要用两根筷子吃，桌上共有 5 根筷子，每位哲学家左右手边各有一根筷子。 如果筷子被身边的人拿着，自己就得等待 筷子类 123456789101112class Chopstick &#123; String name; public Chopstick(String name) &#123; this.name = name; &#125; @Override public String toString() &#123; return &quot;筷子&#123;&quot; + name + &#x27;&#125;&#x27;; &#125;&#125; 哲学家类 1234567891011121314151617181920212223242526272829303132class Philosopher extends Thread &#123; Chopstick left; Chopstick right; public Philosopher(String name, Chopstick left, Chopstick right) &#123; super(name); this.left = left; this.right = right; &#125; private void eat() &#123; log.debug(&quot;eating...&quot;); Sleeper.sleep(1); &#125; @Override public void run() &#123; while (true) &#123; // 获得左手筷子 synchronized (left) &#123; // 获得右手筷子 synchronized (right) &#123; // 吃饭 eat(); &#125; // 放下右手筷子 &#125; // 放下左手筷子 &#125; &#125; &#125; 就餐 1234567891011Chopstick c1 = new Chopstick(&quot;1&quot;);Chopstick c2 = new Chopstick(&quot;2&quot;);Chopstick c3 = new Chopstick(&quot;3&quot;);Chopstick c4 = new Chopstick(&quot;4&quot;);Chopstick c5 = new Chopstick(&quot;5&quot;);new Philosopher(&quot;苏格拉底&quot;, c1, c2).start();new Philosopher(&quot;柏拉图&quot;, c2, c3).start();new Philosopher(&quot;亚里士多德&quot;, c3, c4).start();new Philosopher(&quot;赫拉克利特&quot;, c4, c5).start();new Philosopher(&quot;阿基米德&quot;, c5, c1).start(); 执行不多会，就执行不下去了 1234512:33:15.575 [苏格拉底] c.Philosopher - eating... 12:33:15.575 [亚里士多德] c.Philosopher - eating... 12:33:16.580 [阿基米德] c.Philosopher - eating... 12:33:17.580 [阿基米德] c.Philosopher - eating... // 卡在这里, 不向下运行 使用 jconsole 检测死锁，发现 12345678910111213141516171819202122232425262728293031323334353637383940-------------------------------------------------------------------------名称: 阿基米德状态: cn.itcast.Chopstick@1540e19d (筷子1) 上的BLOCKED, 拥有者: 苏格拉底总阻止数: 2, 总等待数: 1 堆栈跟踪:cn.itcast.Philosopher.run(TestDinner.java:48) - 已锁定 cn.itcast.Chopstick@6d6f6e28 (筷子5)-------------------------------------------------------------------------名称: 苏格拉底状态: cn.itcast.Chopstick@677327b6 (筷子2) 上的BLOCKED, 拥有者: 柏拉图总阻止数: 2, 总等待数: 1 堆栈跟踪:cn.itcast.Philosopher.run(TestDinner.java:48) - 已锁定 cn.itcast.Chopstick@1540e19d (筷子1)-------------------------------------------------------------------------名称: 柏拉图状态: cn.itcast.Chopstick@14ae5a5 (筷子3) 上的BLOCKED, 拥有者: 亚里士多德总阻止数: 2, 总等待数: 0 堆栈跟踪:cn.itcast.Philosopher.run(TestDinner.java:48) - 已锁定 cn.itcast.Chopstick@677327b6 (筷子2)-------------------------------------------------------------------------名称: 亚里士多德状态: cn.itcast.Chopstick@7f31245a (筷子4) 上的BLOCKED, 拥有者: 赫拉克利特总阻止数: 1, 总等待数: 1 堆栈跟踪:cn.itcast.Philosopher.run(TestDinner.java:48) - 已锁定 cn.itcast.Chopstick@14ae5a5 (筷子3)-------------------------------------------------------------------------名称: 赫拉克利特状态: cn.itcast.Chopstick@6d6f6e28 (筷子5) 上的BLOCKED, 拥有者: 阿基米德总阻止数: 2, 总等待数: 0 堆栈跟踪:cn.itcast.Philosopher.run(TestDinner.java:48) - 已锁定 cn.itcast.Chopstick@7f31245a (筷子4) 3）活跃性-活锁活锁出现在两个线程互相改变对方的结束条件，最后谁也无法结束，例如 12345678910111213141516171819202122232425public class TestLiveLock &#123; static volatile int count = 10; static final Object lock = new Object(); public static void main(String[] args) &#123; new Thread(() -&gt; &#123; // 期望减到 0 退出循环 while (count &gt; 0) &#123; sleep(0.2); count--; log.debug(&quot;count: &#123;&#125;&quot;, count); &#125; &#125;, &quot;t1&quot;).start(); new Thread(() -&gt; &#123; // 期望超过 20 退出循环 while (count &lt; 20) &#123; sleep(0.2); count++; log.debug(&quot;count: &#123;&#125;&quot;, count); &#125; &#125;, &quot;t2&quot;).start(); &#125;&#125; 4）活跃性-饥饿一个线程由于优先级太低，始终得不到 CPU 调度执行，也不能够结束，饥饿的情况不 顺序加锁容易产生饥饿问题： 123456new Philosopher(&quot;苏格拉底&quot;, c1, c2).start();new Philosopher(&quot;柏拉图&quot;, c2, c3).start();new Philosopher(&quot;亚里士多德&quot;, c3, c4).start();new Philosopher(&quot;赫拉克利特&quot;, c4, c5).start();// new Philosopher(&quot;阿基米德&quot;, c5, c1).start();new Philosopher(&quot;阿基米德&quot;, c1, c5).start(); //线程饥 12.ReentrantLock相对于 synchronized 它具备如下特点： 可中断 可以设置超时时间 可以设置为公平锁 synchronized的entryList堵塞的线程不是按先来先到的顺序竞争的 支持多个条件变量 多个waitSet（等外卖、等烟的在不同的屋） 可以唤醒特定屋的线程 synchronized只有一个（等外卖、等烟的都挤在一个屋） 一个屋的线程全唤醒了 与 synchronized 一样，都支持可重入 1）可重入可重入是指同一个线程如果首次获得了这把锁，那么因为它是这把锁的拥有者，因此有权利再次获取这把锁 如果是不可重入锁，那么自己线程第二次获得锁时，也会被锁挡住 12345678910111213141516171819202122232425262728293031323334static ReentrantLock lock = new ReentrantLock();public static void main(String[] args) &#123; method1();&#125;public static void method1() &#123; lock.lock(); try &#123; log.debug(&quot;execute method1&quot;); method2(); &#125; finally &#123; lock.unlock(); &#125;&#125;public static void method2() &#123; lock.lock(); try &#123; log.debug(&quot;execute method2&quot;); method3(); &#125; finally &#123; lock.unlock(); &#125;&#125;public static void method3() &#123; lock.lock(); try &#123; log.debug(&quot;execute method3&quot;); &#125; finally &#123; lock.unlock(); &#125;&#125; 输出： 12317:59:11.862 [main] c.TestReentrant - execute method1 17:59:11.865 [main] c.TestReentrant - execute method2 17:59:11.865 [main] c.TestReentrant - execute method3 可以看到m1上锁后，m2，m3还可以获取 2）可打断 可打断锁：lock.lockInterruptibly() 示例 12345678910111213141516171819202122232425262728293031323334ReentrantLock lock = new ReentrantLock();Thread t1 = new Thread(() -&gt; &#123; log.debug(&quot;启动...&quot;); try &#123; //没有其它线程获得锁，该线程就会获取锁 //有竞争就进入阻塞队列等待,但可以被打断 lock.lockInterruptibly(); //lock.lock(); //不可打断 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); log.debug(&quot;等锁的过程中被打断&quot;); return; &#125; try &#123; log.debug(&quot;获得了锁&quot;); &#125; finally &#123; lock.unlock(); &#125;&#125;, &quot;t1&quot;);lock.lock();log.debug(&quot;获得了锁&quot;);t1.start();try &#123; sleep(1); log.debug(&quot;执行打断&quot;); t1.interrupt();&#125; finally &#123; lock.unlock();&#125; 输出： 123456789101112131418:02:40.520 [main] c.TestInterrupt - 获得了锁18:02:40.524 [t1] c.TestInterrupt - 启动... 18:02:41.530 [main] c.TestInterrupt - 执行打断java.lang.InterruptedException at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireInterruptibly(AbstractQueuedSynchronizer.java:898) at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchron izer.java:1222) at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:335) at cn.itcast.n4.reentrant.TestInterrupt.lambda$main$0(TestInterrupt.java:17) at java.lang.Thread.run(Thread.java:748) 18:02:41.532 [t1] c.TestInterrupt - 等锁的过程中被打断 注意如果是不可中断模式，那么即使使用了 interrupt 也不会让等待中断 1234567891011121314151617181920212223242526ReentrantLock lock = new ReentrantLock();Thread t1 = new Thread(() -&gt; &#123; log.debug(&quot;启动...&quot;); lock.lock(); try &#123; log.debug(&quot;获得了锁&quot;); &#125; finally &#123; lock.unlock(); &#125;&#125;, &quot;t1&quot;);lock.lock();log.debug(&quot;获得了锁&quot;);t1.start();try &#123; sleep(1); log.debug(&quot;执行打断&quot;); t1.interrupt(); sleep(1);&#125; finally &#123; log.debug(&quot;释放了锁&quot;); lock.unlock();&#125; 输出 1234518:06:56.261 [main] c.TestInterrupt - 获得了锁18:06:56.265 [t1] c.TestInterrupt - 启动... 18:06:57.266 [main] c.TestInterrupt - 执行打断 // 这时 t1 并没有被真正打断, 而是仍继续等待锁18:06:58.267 [main] c.TestInterrupt - 释放了锁18:06:58.267 [t1] c.TestInterrupt - 获得了锁 lock.lockInterruptibly()才能被打断，lock.lock(); 不能被打断 可打断机制可以让线程不会无限制的阻塞，可以解决死锁问题 3）锁(可设置)超时①立刻返回结果 lock.tryLock()123456789101112131415161718192021222324ReentrantLock lock = new ReentrantLock();Thread t1 = new Thread(() -&gt; &#123; log.debug(&quot;启动...&quot;); if (!lock.tryLock()) &#123; log.debug(&quot;获取立刻失败，返回&quot;); return; &#125; try &#123; log.debug(&quot;获得了锁&quot;); &#125; finally &#123; lock.unlock(); &#125;&#125;, &quot;t1&quot;);lock.lock();log.debug(&quot;获得了锁&quot;);t1.start();try &#123; sleep(2);&#125; finally &#123; lock.unlock();&#125; 输出 12318:15:02.918 [main] c.TestTimeout - 获得了锁18:15:02.921 [t1] c.TestTimeout - 启动... 18:15:02.921 [t1] c.TestTimeout - 获取立刻失败，返回 主线程获得了锁，t1是无法获得锁的，t1.tryLock()尝试获得锁后会直接返回false，不再重试 ②尝试一定时间 lock.tryLock1234567891011121314151617181920212223242526272829ReentrantLock lock = new ReentrantLock();Thread t1 = new Thread(() -&gt; &#123; log.debug(&quot;启动...&quot;); try &#123; if (!lock.tryLock(1, TimeUnit.SECONDS)) &#123; log.debug(&quot;获取等待 1s 后失败，返回&quot;); return; &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; try &#123; log.debug(&quot;获得了锁&quot;); &#125; finally &#123; lock.unlock(); &#125;&#125;, &quot;t1&quot;);lock.lock();log.debug(&quot;获得了锁&quot;);t1.start();try &#123; sleep(2);&#125; finally &#123; lock.unlock();&#125; 输出 12318:19:40.537 [main] c.TestTimeout - 获得了锁18:19:40.544 [t1] c.TestTimeout - 启动... 18:19:41.547 [t1] c.TestTimeout - 获取等待 1s 后失败，返回 lock.tryLock(1, TimeUnit.SECONDS)，会在设定的时间内重试获得锁，若超时了则会返回false ③使用 tryLock 解决哲学家就餐问题123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051class Chopstick extends ReentrantLock &#123;//筷子类继承了ReentrantLock，也继承了tryLock方法 String name; public Chopstick(String name) &#123; this.name = name; &#125; @Override public String toString() &#123; return &quot;筷子&#123;&quot; + name + &#x27;&#125;&#x27;; &#125;&#125;class Philosopher extends Thread &#123; Chopstick left; Chopstick right; public Philosopher(String name, Chopstick left, Chopstick right) &#123; super(name); this.left = left; this.right = right; &#125; @Override public void run() &#123; while (true) &#123; // 尝试获得左手筷子 if (left.tryLock()) &#123; try &#123; // 尝试获得右手筷子 if (right.tryLock()) &#123; try &#123; eat(); &#125; finally &#123; right.unlock(); &#125; &#125; &#125; finally &#123; //内部在没有获得右手筷子后，会将左筷子释放掉 left.unlock(); &#125; &#125; &#125; &#125; private void eat() &#123; log.debug(&quot;eating...&quot;); Sleeper.sleep(1); &#125; &#125; 就餐 1234567891011Chopstick c1 = new Chopstick(&quot;1&quot;);Chopstick c2 = new Chopstick(&quot;2&quot;);Chopstick c3 = new Chopstick(&quot;3&quot;);Chopstick c4 = new Chopstick(&quot;4&quot;);Chopstick c5 = new Chopstick(&quot;5&quot;);new Philosopher(&quot;苏格拉底&quot;, c1, c2).start();new Philosopher(&quot;柏拉图&quot;, c2, c3).start();new Philosopher(&quot;亚里士多德&quot;, c3, c4).start();new Philosopher(&quot;赫拉克利特&quot;, c4, c5).start();new Philosopher(&quot;阿基米德&quot;, c5, c1).start(); 为什么可以解决呢，因为left.tryLock()在获取不到锁会直接返回false synchronized会堵塞等待 left.lock()方法也是会堵塞，所以这里使用的left.tryLock() 4）公平锁设置 ReentrantLock lock = new ReentrantLock(true); 保证了等待队列的先来先获得，中间不能插队先获得锁 公平锁一般没有必要，会降低并发度 5）多个条件变量synchronized 中也有条件变量，就是 waitSet 休息室，当条件不满足时进入 waitSet 等待 ReentrantLock 的条件变量比 synchronized 强大之处在于，它是支持多个条件变量的，这就好比 synchronized 是那些不满足条件的线程都在一间休息室等消息（等烟的和等早餐的都在一起） 如果烟来了，notifyAll会把休息室所有的人都叫醒，但此时早餐还没来（发生了虚假唤醒） 而 ReentrantLock 支持多间休息室，有专门等烟的休息室、专门等早餐的休息室、唤醒时也是按休息室来唤醒 使用要点： await 前需要获得锁 await 执行后，会释放锁，进入 conditionObject 等待 await 的线程被唤醒（或打断、或超时）去重新竞争 lock 锁 被对应的条件waitCigaretteQueue.signal();，会继续执行await下面的代码 竞争 lock 锁成功后，从 await 后继续执行 例子： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071static ReentrantLock lock = new ReentrantLock();static Condition waitCigaretteQueue = lock.newCondition();//lock锁中创建等烟休息室static Condition waitbreakfastQueue = lock.newCondition();//lock锁中创建等早餐休息室static volatile boolean hasCigrette = false;static volatile boolean hasBreakfast = false;public static void main(String[] args) &#123; //等烟 new Thread(() -&gt; &#123; try &#123; lock.lock();//竞争锁，如果没成功会阻塞 while (!hasCigrette) &#123; try &#123; waitCigaretteQueue.await();//被唤醒了会执行下方代码，不满足条件退出while循环 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; log.debug(&quot;等到了它的烟&quot;); &#125; finally &#123; lock.unlock(); &#125; &#125;).start(); //等早餐 new Thread(() -&gt; &#123; try &#123; lock.lock(); while (!hasBreakfast) &#123; try &#123; waitbreakfastQueue.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; log.debug(&quot;等到了它的早餐&quot;); &#125; finally &#123; lock.unlock(); &#125; &#125;).start(); sleep(1); sendBreakfast(); sleep(1); sendCigarette();&#125;private static void sendCigarette() &#123; lock.lock();//送烟也要先获得锁 try &#123; log.debug(&quot;送烟来了&quot;); hasCigrette = true; waitCigaretteQueue.signal(); &#125; finally &#123; lock.unlock(); &#125;&#125;private static void sendBreakfast() &#123; lock.lock(); try &#123; log.debug(&quot;送早餐来了&quot;); hasBreakfast = true; waitbreakfastQueue.signal(); &#125; finally &#123; lock.unlock(); &#125;&#125; 输出 123418:52:27.680 [main] c.TestCondition - 送早餐来了18:52:27.682 [Thread-1] c.TestCondition - 等到了它的早餐18:52:28.683 [main] c.TestCondition - 送烟来了18:52:28.683 [Thread-0] c.TestCondition - 等到了它的烟 13.同步模式之顺序控制1）固定运行顺序比如，必须先 2 后 1 打印 ①wait notify 版1234567891011121314151617181920212223242526272829303132333435// 用来同步的对象static Object obj = new Object();// t2 运行标记， 代表 t2 是否执行过static boolean t2runed = false;public static void main(String[] args) &#123; Thread t1 = new Thread(() -&gt; &#123; synchronized (obj) &#123; // 如果 t2 没有执行过 while (!t2runed) &#123; try &#123; // t1 先等一会 obj.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; System.out.println(1); &#125;); Thread t2 = new Thread(() -&gt; &#123; System.out.println(2); synchronized (obj) &#123; // 修改运行标记 t2runed = true; // 通知 obj 上等待的线程（可能有多个，因此需要用 notifyAll） obj.notifyAll(); &#125; &#125;); t1.start(); t2.start();&#125; ②Park Unpark 版可以看到，实现上很麻烦： 需要保证先 wait 再 notify，否则 wait 线程永远得不到唤醒。因此使用了『运行标记』来判断该不该wait 如果有些干扰线程错误地 notify 了 wait 线程，条件不满足时还要重新等待，使用了 while 循环来解决此问题 后，唤醒对象上的 wait 线程需要使用 notifyAll，因为『同步对象』上的等待线程可能不止一个 可以使用 LockSupport 类的 park 和 unpark 来简化上面的题目： 123456789101112131415Thread t1 = new Thread(() -&gt; &#123; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; &#125; // 当没有『许可』时，当前线程暂停运行；有『许可』时，用掉这个『许可』，当前线程恢复运行 LockSupport.park(); System.out.println(&quot;1&quot;);&#125;);Thread t2 = new Thread(() -&gt; &#123; System.out.println(&quot;2&quot;); // 给线程 t1 发放『许可』（多次连续调用 unpark 只会发放一个『许可』） LockSupport.unpark(t1);&#125;);t1.start();t2.start(); LockSupport.unpark(t1);，表示不管t1现在许可是0还是1，都设置为1，代表有许可 这样不管t1有没有许可，都会让t1拥有许可，然后执行 LockSupport.park(); 在许可为0的时候等待 当许可为1的时候执行 park 和 unpark 方法比较灵活，他俩谁先调用，谁后调用无所谓（9.中详细解释了）： 并且是以线程为单位进行『暂停』和『恢复』, 不需要『同步对象』：①中的obj 不需要『运行标记』：①中的t2runed 2）交替输出①wait notify 版1234567891011121314151617181920212223242526272829303132333435363738394041class SyncWaitNotify &#123; private int flag; private int loopNumber; public SyncWaitNotify(int flag, int loopNumber) &#123; this.flag = flag; this.loopNumber = loopNumber; &#125; public void print(int waitFlag, int nextFlag, String str) &#123; for (int i = 0; i &lt; loopNumber; i++) &#123; synchronized (this) &#123; while (this.flag != waitFlag) &#123; try &#123; this.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.print(str); flag = nextFlag; //flag是线程共享的，修改flag来使下一个成功获得锁并执行的是对应线程 this.notifyAll(); &#125; &#125; &#125;&#125;SyncWaitNotify syncWaitNotify = new SyncWaitNotify(1, 5);new Thread(() -&gt; &#123; syncWaitNotify.print(1, 2, &quot;a&quot;);&#125;).start();new Thread(() -&gt; &#123; syncWaitNotify.print(2, 3, &quot;b&quot;);&#125;).start();new Thread(() -&gt; &#123; syncWaitNotify.print(3, 1, &quot;c&quot;);&#125;).start(); ②ReentrantLock条件变量版123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354class AwaitSignal extends ReentrantLock&#123; private int loopNumber; public AwaitSignal(int loopNumber) &#123; this.loopNumber = loopNumber; &#125; // 参数1 打印内容， 参数2进入哪一间休息室, 参数3 下一间休息室 public void print(String str, Condition current, Condition next) &#123; for (int i = 0; i &lt; loopNumber; i++) &#123; lock(); try &#123; current.await(); //注意a,b,c对应的线程在获取到锁以后都会进入自己的休息室，等待对应的线程来唤醒自己 System.out.print(str); next.signal(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; unlock(); &#125; &#125; &#125;&#125;public static void main(String[] args) throws InterruptedException &#123; AwaitSignal awaitSignal = new AwaitSignal(5); Condition a = awaitSignal.newCondition(); Condition b = awaitSignal.newCondition(); Condition c = awaitSignal.newCondition(); new Thread(() -&gt; &#123; awaitSignal.print(&quot;a&quot;, a, b); &#125;).start(); new Thread(() -&gt; &#123; awaitSignal.print(&quot;b&quot;, b, c); &#125;).start(); new Thread(() -&gt; &#123; awaitSignal.print(&quot;c&quot;, c, a); &#125;).start(); Thread.sleep(1000); awaitSignal.lock(); try &#123; System.out.println(&quot;开始...&quot;); a.signal();//先唤醒a &#125; finally &#123; awaitSignal.unlock(); &#125; &#125; ​ a唤醒后sout完会释放lock，再尝试获取lock，这时候b被唤醒，然后sout并唤醒c，然后释放lock，a、b在这期间获取到lock都是会先进入对应休息室的。 考虑这样一种情况： ​ 第一轮for，a、b都执行完以后因为时间片问题没能获取到lock，c被唤醒并sout完以后会唤醒a休息室，但是此时a还未获取到lock进入休息室，正好时间片轮到c，c可以获取lock并进入休息室，然后时间片轮到a，a进入休息室，接着是b进入休息室，那么三个休息室中的a，b，c就都处于waiting状态了 ②Park Unpark 版123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.tobestronger.n4._4_13.JiaoTiShuChu;import lombok.extern.slf4j.Slf4j;import java.util.concurrent.locks.LockSupport;@Slf4j(topic = &quot;c.JiaoTiShuChuParkUnpark&quot;)public class JiaoTiShuChuParkUnpark &#123; static Thread t1; static Thread t2; static Thread t3; public static void main(String[] args) &#123; ParkUnpark pu = new ParkUnpark(5); t1 = new Thread(() -&gt; &#123; pu.print(&quot;a&quot;, t2); &#125;); t2 = new Thread(() -&gt; &#123; pu.print(&quot;b&quot;, t3); &#125;); t3 = new Thread(() -&gt; &#123; pu.print(&quot;c&quot;, t1); &#125;); t1.start(); t2.start(); t3.start(); LockSupport.unpark(t1); &#125;&#125;class ParkUnpark &#123; private int loopNumber; public ParkUnpark(int loopNumber) &#123; this.loopNumber = loopNumber; &#125; public void print(String str, Thread next) &#123; for (int i = 0; i &lt; loopNumber; i++) &#123; LockSupport.park(); System.out.print(str); LockSupport.unpark(next); &#125; &#125;&#125; a，b，c一开始都会unpark停下，没允许，main线程unpark了a线程，a线程unpark了b，然后又进入了下一次循环又park了，如此进行重复 三、JMM 内存模型1.Java 内存模型基本概念JMM 即 Java Memory Model，它定义了主存、工作内存抽象概念，底层对应着 CPU 寄存器、缓存、硬件内存、CPU 指令优化等。 JMM 体现在以下几个方面 原子性 - 保证指令不会受到线程上下文切换的影响 synchronized、ReentrantLcok 可见性 - 保证指令不会受 cpu 缓存的影响 有序性 - 保证指令不会受 cpu 指令并行优化的影响 2.可见性1）退不出的循环先来看一个现象，main 线程对 run 变量的修改对于 t 线程不可见，导致了 t 线程无法停止： 1234567891011121314static boolean run = true;public static void main(String[] args) throws InterruptedException &#123; Thread t = new Thread(()-&gt;&#123; while(run)&#123; //System.out.println() // 这样也不会出错，因为其内部包含了synchronized的使用，保证了可见性 &#125; &#125;); t.start(); sleep(1); run = false; // 线程t不会如预想的停下来&#125; 原因： 初始状态， t 线程刚开始从主内存读取了 run 的值到工作内存。 因为 t 线程要频繁从主内存中读取 run 的值，JIT 编译器会将 run 的值缓存至自己工作内存中的高速缓存中，减少对主存中 run 的访问，提高效率 虽然1 秒之后，main 线程修改了 run 的值，并同步至主存，但 t 是从自己工作内存中的高速缓存中读取这个变量的值，结果永远是旧值 其实就是JIT对热点代码的优化 解决方法： volatile（易变关键字） 它可以用来修饰成员变量和静态成员变量，他可以避免线程从自己的工作缓存中查找变量的值，必须到主存中获取它的值，线程操作 volatile 变量都是直接操作主存 这里的主存其实就是，.class对象中所存储的静态变量 2）可见性 vs 原子性​ 前面例子体现的实际就是可见性，它保证的是在多个线程之间，一个线程对 volatile 变量的修改对另一个线程可见，不能保证原子性，适合用在一个写线程，多个读线程的情况： 上例从字节码理解是这样的： 123456getstatic run //线程t获取run truegetstatic run //线程t获取run truegetstatic run //线程t获取run truegetstatic run //线程t获取run trueputstatic run //线程main 修改run为false，仅此一次getstatic run //线程t获取run false 保证t线程每次都是getstatic去获得，而不是直接用自己缓存中的值 比较一下之前我们讲线程安全时举的例子：两个线程一个 i++ 一个 i– ，只能保证看到最新值，不能解决指令交错 12345678910//假设i的初始值为0getstatic i //线程2-获取静态变量i的值线程内i=0getstatic i //线程1-获取静态变量i的值线程内i=0iconst_1 //线程1-准备常量1iadd //线程1-自增线程内i=1putstatic i //线程1-将修改后的值存入静态变量i 静态变量i=1iconst_1 //线程2-准备常量1isub //线程2-自减线程内i=-1putstatici //线程2-将修改后的值存入静态变量i 静态变量i=-1 注意 : synchronized 语句块既可以保证代码块的原子性，也同时保证代码块内变量的可见性。但缺点是 synchronized 是属于重量级操作，性能相对更低 如果在前面示例的死循环中加入 System.out.println() 会发现即使不加 volatile 修饰符，线程 t 也能正确看到对 run 变量的修改了，想一想为什么？ 因为其内部包含了synchronized 的使用 3）终止模式之两阶段终止①利用打断标记1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/**使用 interrupt 进行两阶段终止模式*/@Slf4j(topic = &quot;c.Code_13_Test&quot;)public class Code_13_Test &#123;public static void main(String[] args) throws InterruptedException &#123; TwoParseTermination twoParseTermination = new TwoParseTermination(); twoParseTermination.start(); Thread.sleep(3500); twoParseTermination.stop();&#125;&#125;@Slf4j(topic = &quot;c.TwoParseTermination&quot;)class TwoParseTermination &#123;private Thread monitor;// 启动线程public void start() &#123; monitor = new Thread(() -&gt; &#123; while (true) &#123; Thread current = Thread.currentThread(); if(thread.isInterrupted()) &#123; // 调用 isInterrupted 不会清除标记 log.info(&quot;料理后事 ...&quot;); break; &#125; else &#123; try &#123; Thread.sleep(1000); log.info(&quot;执行监控的功能 ...&quot;); &#125; catch (InterruptedException e) &#123; log.info(&quot;设置打断标记 ...&quot;); current.interrupt();//重新设置标记 e.printStackTrace(); &#125; &#125; &#125; &#125;, &quot;monitor&quot;); monitor.start();&#125;// 终止线程public void stop() &#123; monitor.interrupt(); &#125;&#125; 这里模拟了监控功能 开启监控就是开启一个线程通过while true循环执行任务 Thread.sleep(1000);保证CPU占用率不会100% 并且也可以表明每隔1s监控一次 关闭监控利用了interrupt()方法 如果监控线程是运行状态，那么会使打断标志位true，然后break推出while true 如果监控线程是阻塞状态（sleep），这时候interrupt打断后，打断标志位false 阻塞状态会抛出一个异常，可以catch这个InterruptedException e异常，通过 通过current.interrupt()再次将本线程打断标志设置为true ②volatile利用volatile保证停止标记stop多线程间可见 12345678910111213141516171819202122232425262728293031// 停止标记用 volatile 是为了保证该变量在多个线程之间的可见性// 我们的例子中，即主线程把它修改为 true 对 t1 线程可见class TPTVolatile &#123; private Thread thread; private volatile boolean stop = false; public void start()&#123; thread = new Thread(() -&gt; &#123; while(true) &#123; //Thread current = Thread.currentThread(); if(stop) &#123; log.debug(&quot;料理后事&quot;); break; &#125; try &#123; Thread.sleep(1000); log.debug(&quot;将结果保存&quot;); &#125; catch (InterruptedException e) &#123; &#125; // 执行监控操作 &#125; &#125;,&quot;监控线程&quot;); thread.start(); &#125; public void stop() &#123; stop = true; thread.interrupt(); &#125;&#125; main线程new了一个TPTVolatile t t.start();new了一个线程thread执行操作，该线程可以访问到t对象的停止标记stop t.stop();修改了t对象的停止标记，这是在main线程中执行的 main线程对t对象的修改，要想让thread线程看到，需要设置private volatile boolean stop &#x3D; false; 意味着t对象的stop被一个线程修改了，其它线程是可见的 为什么还要调用thread.interrupt();？ 因为while中 Thread.sleep(1000);，在线程thread睡眠过程中，即使修改了stop也不能马上看到 可以通过interrupt来打断，然后进入catch块，然后继续判断是否while循环 调用： 123456TPTVolatile t = new TPTVolatile();t.start();Thread.sleep(3500);log.debug(&quot;stop&quot;);t.stop(); 结果： 1234511:54:52.003 c.TPTVolatile [监控线程] - 将结果保存11:54:53.006 c.TPTVolatile [监控线程] - 将结果保存11:54:54.007 c.TPTVolatile [监控线程] - 将结果保存11:54:54.502 c.TestTwoPhaseTermination [main] - stop 11:54:54.502 c.TPTVolatile [监控线程] - 料理后事 4） (同步)模式之 Balking(犹豫)Balking （犹豫）模式用在一个线程发现另一个线程或本线程已经做了某一件相同的事，那么本线程就无需再做了，直接结束返回 实现： 1234567891011121314151617public class MonitorService &#123; // 用来表示是否已经有线程已经在执行启动了 private volatile boolean starting; public void start() &#123; log.info(&quot;尝试启动监控线程...&quot;); synchronized (this) &#123; if (starting) &#123; return; &#125; starting = true; &#125; // 真正启动监控线程... &#125;&#125; 注意：因为检查锁，修改锁是两个动作，为了保证原子性，需要加上synchronized 输出： 12345[http-nio-8080-exec-1] cn.itcast.monitor.service.MonitorService - 该监控线程已启动?(false)[http-nio-8080-exec-1] cn.itcast.monitor.service.MonitorService - 监控线程已启动...[http-nio-8080-exec-2] cn.itcast.monitor.service.MonitorService - 该监控线程已启动?(true)[http-nio-8080-exec-3] cn.itcast.monitor.service.MonitorService - 该监控线程已启动?(true)[http-nio-8080-exec-4] cn.itcast.monitor.service.MonitorService - 该监控线程已启动?(true) 它还经常用来实现线程安全的单例 1234567891011121314public final class Singleton &#123; private Singleton() &#123;&#125; private static Singleton INSTANCE = null; public static synchronized Singleton getInstance() &#123; if (INSTANCE != null) &#123; return INSTANCE; &#125; INSTANCE = new Singleton(); return INSTANCE; &#125;&#125; 对比一下保护性暂停模式：保护性暂停模式用在一个线程等待另一个线程的执行结果，当条件不满足时线程等待。 单例还有懒惰单例加载：https://f1ashades.github.io/2022/04/13/JVM/#more 三、5. 4）② 3.有序性1）指令重排特性JVM 会在不影响正确性的前提下，可以调整语句的执行顺序 思考下面一段代码 123456static int i;static int j;// 在某个线程内执行如下赋值操作i = ...; j = ...; 可以看到，至于是先执行 i 还是 先执行 j ，对最终的结果不会产生影响。所以，上面代码真正执行时，既可以是 12i = ...; j = ...; 也可以是 12j = ...;i = ...; 这种特性称之为『指令重排』，多线程下『指令重排』会影响正确性。 为什么要有重排指令这项优化呢？ 涉及到CPU 执行指令的原理 2）指令重排序①example123456789101112131415161718int num = 0;boolean ready = false;// 线程1 执行此方法public void actor1(I_Result r) &#123; if(ready) &#123; r.r1 = num + num; &#125; else &#123; r.r1 = 1; &#125;&#125;// 线程2 执行此方法public void actor2(I_Result r) &#123; //这里可能发生指令重排序 num = 2; ready = true; &#125; I_Result 是一个对象，有一个属性 r1 用来保存结果，问，可能的结果有几种？ 有以下分析： 情况1：线程1 先执行，这时 ready &#x3D; false，所以进入 else 分支结果为 1 情况2：线程2 先执行 num &#x3D; 2，但没来得及执行 ready &#x3D; true，线程1 执行，还是进入 else 分支,结果为1 情况3：线程2 执行到 ready &#x3D; true，线程1 执行，这回进入 if 分支，结果为 4（因为 num 已经执行过了） 但我告诉你，结果还有可能是 0 ： ​ 这种情况下是：线程2 执行 ready &#x3D; true，切换到线程1，进入 if 分支，相加为 0，再切回线程2 执行 num &#x3D; 2 ②解决方法volatile 修饰的变量，可以禁用指令重排 1234567891011121314151617public class ConcurrencyTest &#123;int num = 0;volatile boolean ready = false; //申明为volatile变量 public void actor1(I_Result r) &#123; if(ready) &#123; r.r1 = num + num; &#125; else &#123; r.r1 = 1; &#125; &#125; public void actor2(I_Result r) &#123; num = 2; ready = true; &#125;&#125; 4.volatile原理volatile 的底层实现原理是内存屏障，Memory Barrier（Memory Fence） 对 volatile 变量的写指令后会加入写屏障 对 volatile 变量的读指令前会加入读屏障 1）如何保证可见性 写屏障（sfence）保证在该屏障之前的，对共享变量的改动，都同步到主存当中 12345public void actor2(I_Result r) &#123; num = 2; ready = true; // ready 是 volatile 赋值带写屏障 // 写屏障&#125; 而读屏障（lfence）保证在该屏障之后，对共享变量的读取，加载的是主存中最新数据 123456789public void actor1(I_Result r) &#123; // 读屏障 // ready 是 volatile 读取值带读屏障 if(ready) &#123; r.r1 = num + num; &#125; else &#123; r.r1 = 1; &#125;&#125; 2）如何保证有序性 写屏障会确保指令重排序时，不会将写屏障之前的代码排在写屏障之后 123456public void actor2(I_Result r) &#123; num = 2; //读屏障 ready = true; // 写屏障&#125; 读屏障会确保指令重排序时，不会将读屏障之后的代码排在读屏障之前 123456789public void actor1(I_Result r) &#123; // 读屏障 // ready 是 volatile 赋值带写屏障 // ready 是 volatile 读取值带读屏障 if(ready) &#123; r.r1 = num + num; &#125; else &#123; r.r1 = 1; &#125;&#125; 还是那句话，不能解决指令交错（++i，–i）： 写屏障仅仅是保证之后的读能够读到最新的结果，但不能保证读跑到它前面去 而有序性的保证也只是保证了本线程内相关代码不被重排序 总结： volatile会在ready = true;下方设置一个写屏障，上方设置一个读屏障 写屏障 保证写屏障之前对共享变量的修改都同步到缓存都同步到主存当中 可以保证指令重排后，其上方指令不会跑到写屏障下面， 读屏障 保证读屏障后的代码都会从主存中读取数据，而不是使用缓存中的数据 可以保证指令重排后，其下方代码跑到读屏障之前 3）volatile不能和final同时使用 volatile是一个类型修饰符，它是被设计用来修饰被不同线程访问和修改的变量，可以被异步的线程所修改。 volatile修饰的变量是放到共享内存中的，可以让所有的线程获取和修改。 final必须对它赋予初值并且不能修改它。 4）double-checked locking 问题①问题 double-checked locking 单例模式为例 12345678910111213141516public final class Singleton &#123; private Singleton() &#123; &#125; private static Singleton INSTANCE = null; public static Singleton getInstance() &#123; if(INSTANCE == null) &#123; // 这段代码是不受synchronized管理的 // 首次访问会同步，而之后的使用没有 synchronized synchronized(Singleton.class) &#123; if (INSTANCE == null) &#123; // t1 INSTANCE = new Singleton(); &#125; &#125; &#125; return INSTANCE; &#125;&#125; 以上的实现特点是： 懒惰实例化 首次使用 getInstance() 才使用 synchronized 加锁，后续使用时无需加锁 这样就可以避免已经单例后，后续还需synchronized获得锁后在判断，浪费资源 有隐含的，但很关键的一点：第一个 if 使用了 INSTANCE 变量，是在同步块之外 但在多线程环境下，上面的代码是有问题的，getInstance 方法对应的字节码为： 其中 17 表示创建对象，将对象引用入栈 &#x2F;&#x2F; new Singleton 20 表示复制一份对象引用 &#x2F;&#x2F; 引用地址 21 表示利用一个对象引用，调用构造方法 24 表示利用一个对象引用，赋值给 static INSTANCE jvm可能会优化为：先执行 24，再执行 21，这意味给了instance一个引用，然后再执行其初始化方法 如果两个线程 t1，t2 按如下时间序列执行： 关键在于 0: getstatic 这行代码在 monitor 控制之外，它就像之前举例中不守规则的人，可以越过 monitor 读取INSTANCE 变量的值 因为这时 t1 还未完全将构造方法执行完毕，如果在构造方法中要执行很多初始化操作，那么 t2 拿到的是将是一个未初始化完毕的单例 synchronized不是可以保证：原子、可见、有序吗？ 如果instance是完全在synchronized中，是可以保证这些特性的，但是instance并不是完全在synchronized内，所以并不能保证其这些特性 ②解决对 INSTANCE 使用 volatile 修饰即可，可以禁用指令重排，但要注意在 JDK 5 以上的版本的 volatile 才会真正有效 1234567891011121314151617public final class Singleton &#123; private Singleton() &#123; &#125; private static volatile Singleton INSTANCE = null;//注意volatile修饰 public static Singleton getInstance() &#123; // 实例没创建，才会进入内部的 synchronized代码块 if (INSTANCE == null) &#123; synchronized (Singleton.class) &#123; // t2 // 也许有其它线程已经创建实例，所以再判断一次 if (INSTANCE == null) &#123; // t1 INSTANCE = new Singleton(); &#125; &#125; &#125; return INSTANCE; &#125;&#125; 由于instance加了写屏障，所以 putstatic给局部变量表中的instance赋值的代码之前保证进行了构造方法 写屏障之前代码不能排序 如上面的注释内容所示，读写 volatile 变量时会加入内存屏障（Memory Barrier（Memory Fence）），保证下面两点： 可见性 写屏障（sfence）保证在该屏障之前的 t1 对共享变量的改动，都同步到主存当中 而读屏障（lfence）保证在该屏障之后 t2 对共享变量的读取，加载的是主存中最新数据 有序性 写屏障会确保指令重排序时，不会将写屏障之前的代码排在写屏障之后 读屏障会确保指令重排序时，不会将读屏障之后的代码排在读屏障之前 更底层是读写变量时使用 lock 指令来多核 CPU 之间的可见性与有序性 5.happens-before规则情况1：线程解锁 m 之前对变量的写，对于接下来对 m 加锁的其它线程对该变量的读可见： 1234567891011121314static int x;static Object m = new Object();new Thread(()-&gt;&#123; synchronized(m) &#123; x = 10; &#125;&#125;,&quot;t1&quot;).start();new Thread(()-&gt;&#123; synchronized(m) &#123; System.out.println(x); &#125;&#125;,&quot;t2&quot;).start(); 情况2：线程对 volatile 变量的写，对接下来其它线程对该变量的读可见 123456789volatile static int x;new Thread(()-&gt;&#123; x = 10;&#125;,&quot;t1&quot;).start();new Thread(()-&gt;&#123; System.out.println(x);&#125;,&quot;t2&quot;).start(); 情况3：线程 start 前对变量的写，对该线程开始后对该变量的读可见 123456static int x; x = 10;new Thread(()-&gt;&#123; System.out.println(x);&#125;,&quot;t2&quot;).start(); 情况4：线程结束前对变量的写，对其它线程得知它结束后的读可见（比如其它线程调用 t1.isAlive() 或 t1.join()等待它结束） 123456789static int x;Thread t1 = new Thread(()-&gt;&#123; x = 10;&#125;,&quot;t1&quot;);t1.start();t1.join();System.out.println(x); 退不出的循环中，main线程虽然结束并且修改了变量，但是t线程并没有得知main线程结束了，除非t调用了main.join 情况5：线程 t1 打断 t2（interrupt）前对变量的写，对于其他线程得知 t2 被打断后对变量的读可见（通过t2.interrupted 或 t2.isInterrupted） 123456789101112131415161718192021222324static int x;public static void main(String[] args) &#123; Thread t2 = new Thread(()-&gt;&#123; while(true) &#123; if(Thread.currentThread().isInterrupted()) &#123; System.out.println(x); break; &#125; &#125; &#125;,&quot;t2&quot;); t2.start(); new Thread(()-&gt;&#123; sleep(1); x = 10; t2.interrupt(); &#125;,&quot;t1&quot;).start(); while(!t2.isInterrupted()) &#123; Thread.yield(); &#125; System.out.println(x);&#125; t1线程修改了变量，然后t1打断了t2，那么t2也能得知该被修改的变量 情况6: 对变量默认值（int是0，boolean是false，引用是null）的写，对其它线程对该变量的读可见 情况7: 具有传递性，如果 x hb-&gt; y 并且 y hb-&gt; z 那么有 x hb-&gt; z ，配合 volatile 的防指令重排，有下面的例子 123456789101112volatile static int x;static int y;new Thread(()-&gt;&#123; y = 10; x = 20;&#125;,&quot;t1&quot;).start();new Thread(()-&gt;&#123; // x=20 对 t2 可见, 同时 y=10 也对 t2 可见 System.out.println(x); &#125;,&quot;t2&quot;).start(); t1中volatile会对在x下面添加一个写屏障，写屏障之前的代码都会同步到主存中 t2中volatile会对在x上面添加一个读屏障，后续读操作都会去主存中读 6.balking和线程单例test1）balking 模式习题希望 doInit() 方法仅被调用一次，下面的实现是否有问题，为什么？ 123456789101112131415public class TestVolatile &#123; volatile boolean initialized = false; void init() &#123; if (initialized) &#123; //这里读取了共享变量 return; &#125; doInit(); initialized = true;//这里修改了共享变量 &#125; private void doInit() &#123; ........ &#125;&#125; volatile只能保证可见性和有序性，不能保证原子性 修改： 1234567891011121314151617public class TestVolatile &#123; volatile boolean initialized = false; void init() &#123; synchronized(this)&#123; if (initialized) &#123; return; &#125; doInit(); initialized = true; &#125; &#125; private void doInit() &#123; ......... &#125;&#125; 2）线程安全单例习题单例模式有很多实现方法，饿汉、懒汉、静态内部类、枚举类，试分析每种实现下获取单例对象（即调用getInstance）时的线程安全，并思考注释中的问题 饿汉式：类加载就会导致该单实例对象被创建 懒汉式：类加载不会导致该单实例对象被创建，而是首次使用该对象时才会创建 实现1： 123456789101112131415// 问题1：为什么加 final// 问题2：如果实现了序列化接口, 还要做什么来防止反序列化破坏单例public final class Singleton implements Serializable &#123; // 问题3：为什么设置为私有? 是否能防止反射创建新的实例? private Singleton() &#123;&#125; // 问题4：这样初始化是否能保证单例对象创建时的线程安全? private static final Singleton INSTANCE = new Singleton(); // 问题5：为什么提供静态方法而不是直接将 INSTANCE 设置为 public, 说出你知道的理由 public static Singleton getInstance() &#123; return INSTANCE; &#125; public Object readResolve() &#123; //反序列化 return INSTANCE; &#125;&#125; 问题1 防止有子类对父类方法有不当的实现 问题2 实现一个public Object readResolve() （写法也是固定的），反序列化的时候就会返回该方法return的对象 问题3 防止随意通过构造方法随意创建对象 不能，反射可以获得构造器，设置setAccessible为true，然后暴力反射 问题4 这种实现是饿汉式，因为其在类加载阶段中的初始化阶段就把对象创建了 可以，类加载器会保证单例对象创建时的安全 问题5 方法可以支持懒惰加载 可以做一些控制 实现2： 123456789// 问题1：枚举单例是如何限制实例个数的 反编译可以看到枚举变量就是 private final static enum INSTANCE; // 问题2：枚举单例在创建时是否有并发问题 不会，同样是类加载器保证// 问题3：枚举单例能否被反射破坏单例 不会// 问题4：枚举单例能否被反序列化破坏单例 枚举类默认实现serializable，内部实现了不会被反序列化破坏单例// 问题5：枚举单例属于懒汉式还是饿汉式 饿汉式，因为类加载阶段链接阶段就会把对象创建// 问题6：枚举单例如果希望加入一些单例创建时的初始化逻辑该如何做 构造方法里写逻辑enum Singleton &#123; INSTANCE; &#125; 实现3： 123456789101112public final class Singleton &#123; private Singleton() &#123; &#125; private static Singleton INSTANCE = null; // 分析这里的线程安全, 并说明有什么缺点 public static synchronized Singleton getInstance() &#123; if( INSTANCE != null )&#123; return INSTANCE; &#125; INSTANCE = new Singleton(); return INSTANCE; &#125;&#125; synchronized不能加在INSTANCE上，因为一开始instance是null instance可能是null instance不是final，是一个变量 缺点是粒度太大，synchronized范围太大了 实现4：DCL 123456789101112131415161718192021public final class Singleton &#123; private Singleton() &#123; &#125; // 问题1：解释为什么要加 volatile ? 保证问题3处有序，先调构造，再赋值引用 private static volatile Singleton INSTANCE = null; // 问题2：对比实现3, 说出这样做的意义 如果已经有示例，不会再synchronized获取锁 public static Singleton getInstance() &#123; if (INSTANCE != null) &#123; return INSTANCE; &#125; synchronized (Singleton.class) &#123; //t1来到了这儿，下面如果不判空，又会有新的对象 // 问题3：为什么还要在这里加为空判断, 之前不是判断过了吗 if (INSTANCE != null) &#123; return INSTANCE; &#125; INSTANCE = new Singleton(); //t2还没执行这一句，INSTANCE还是null，导致t1到了上面的位置 return INSTANCE; &#125; &#125;&#125; (推荐的)实现5： 1234567891011public final class Singleton &#123; private Singleton() &#123; &#125; // 问题1：属于懒汉式还是饿汉式 懒汉式 private static class LazyHolder &#123; static final Singleton INSTANCE = new Singleton(); &#125; // 问题2：在创建时是否有并发问题 JVM保证其安全性 public static Singleton getInstance() &#123; return LazyHolder.INSTANCE; &#125;&#125; 只用到Singleton，是不会加载LazyHolder的，也不会new实例 四、无锁-乐观锁1.问题引出1）问题保证 account.withdraw 取款方法的线程安全 123456789101112131415161718192021222324252627282930313233343536373839import java.util.ArrayList;import java.util.List;interface Account &#123; // 获取余额 Integer getBalance(); // 取款 void withdraw(Integer amount); /** * 方法内会启动 1000 个线程，每个线程做 -10 元 的操作 * 如果初始余额为 10000 那么正确的结果应当是 0 */ static void demo(Account account) &#123; List&lt;Thread&gt; ts = new ArrayList&lt;&gt;(); long start = System.nanoTime(); for (int i = 0; i &lt; 1000; i++) &#123; ts.add(new Thread(() -&gt; &#123; account.withdraw(10); &#125;)); &#125; ts.forEach(Thread::start); ts.forEach(t -&gt; &#123; try &#123; t.join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); long end = System.nanoTime(); System.out.println(account.getBalance() + &quot; cost: &quot; + (end-start)/1000_000 + &quot; ms&quot;); &#125;&#125; 原有实现并不是线程安全的 1234567891011121314151617class AccountUnsafe implements Account &#123; private Integer balance; public AccountUnsafe(Integer balance) &#123; this.balance = balance; &#125; @Override public Integer getBalance() &#123; return balance; &#125; @Override public void withdraw(Integer amount) &#123; balance -= amount; &#125;&#125; 执行测试代码 123public static void main(String[] args) &#123; Account.demo(new AccountUnsafe(10000));&#125; 某次的执行结果 330 cost: 306 ms 可以看到结果是错误的，因为balance -= amount;并不是原子操作，多线程下可能会产生如下问题 线程1执行完运算：500-10&#x3D;490 线程2执行完运算：500-10&#x3D;490，并且已经修改了对象的该值 线程1继续执行，将490赋值给该对象，这样减了2次，结果仍然是490 2）synchronized解决给 Account 对象加锁 123456789101112131415161718class AccountUnsafe implements Account &#123; private Integer balance; public AccountUnsafe(Integer balance) &#123; this.balance = balance; &#125; @Override public synchronized Integer getBalance() &#123; return balance; &#125; @Override public synchronized void withdraw(Integer amount) &#123; balance -= amount; &#125; &#125; 结果为 0 cost: 399 ms 这样多线程修该下，一个时刻只有一个线程能进行withdraw修改，其它线程阻塞 3）无锁(AtomicInteger)123456789101112131415161718192021222324252627class AccountSafe implements Account &#123; private AtomicInteger balance; //原子整数 public AccountSafe(Integer balance) &#123; this.balance = new AtomicInteger(balance);//接收普通Integer，然后得到AtomicInteger &#125; @Override public Integer getBalance() &#123; return balance.get();//AtomicInteger对象get方法可以得到其值 &#125; @Override public void withdraw(Integer amount) &#123; while (true) &#123; int prev = balance.get(); //获取余额最新值 int next = prev - amount; //修改后的余额 if (balance.compareAndSet(prev, next)) &#123; //同步到主存，也就是实际对象中的值 break; &#125; //成功了就退出循环，没成功就到下次循环继续尝试 &#125; // 可以简化为下面的方法 // balance.addAndGet(-1 * amount); &#125;&#125; 执行测试代码 123public static void main(String[] args) &#123; Account.demo(new AccountSafe(10000));&#125; 某次的执行结果 0 cost: 302 ms 可以看到无锁也保证了线程并发安全，并且耗时要少一些 2.CAS和volatile1）基本思路前面看到的 AtomicInteger 的解决方法，内部并没有用锁来保护共享变量的线程安全。那么它是如何实现的呢？ 123456789101112131415161718192021public void withdraw(Integer amount) &#123; while(true) &#123; // 需要不断尝试，直到成功为止 while (true) &#123; // 比如拿到了旧值 1000 int prev = balance.get(); // 在这个基础上 1000-10 = 990 int next = prev - amount; /* compareAndSet 正是做这个检查，在 set 前，先比较 prev 与当前值 - 不一致了，next 作废，返回 false 表示失败 比如，别的线程已经做了减法，当前值已经被减成了 990 那么本线程的这次 990 就作废了，进入 while 下次循环重试 - 一致，以 next 设置为新值，返回 true 表示成功 */ if (balance.compareAndSet(prev, next)) &#123; break; &#125; &#125; &#125;&#125; 其中的关键是 compareAndSet，它的简称就是 CAS （也有 Compare And Swap 的说法），它必须是原子操作。 注意 其实 CAS 的底层是 lock cmpxchg 指令（X86 架构），在单核 CPU 和多核 CPU 下都能够保证【比较-交换】的原子性。 在多核状态下，某个核执行到带 lock 的指令时，CPU 会让总线锁住，当这个核把此指令执行完毕，再开启总线。这个过程中不会被线程的调度机制所打断，保证了多个线程对内存操作的准确性，是原子的。 2）volatile获取共享变量时，为了保证该变量线程间的可见性，需要使用 volatile 修饰 AtomicInteger源码： 12345678910111213141516public class AtomicInteger extends Number implements java.io.Serializable &#123; private static final long serialVersionUID = 6214790243416807050L; // setup to use Unsafe.compareAndSwapInt for updates private static final Unsafe unsafe = Unsafe.getUnsafe(); private static final long valueOffset; static &#123; try &#123; valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField(&quot;value&quot;)); &#125; catch (Exception ex) &#123; throw new Error(ex); &#125; &#125; private volatile int value; //volatile修饰 ..... 可以看到AtomicInteger中用来存储Integer的值使用volatile修饰的 volatile 可以用来修饰成员变量和静态成员变量，它可以避免线程从自己的工作缓存中查找变量的值，必须到主存中获取它的值，线程操作 volatile 变量都是直接操作主存。即一个线程对 volatile 变量的修改，对另一个线程可见。 注意 volatile 仅仅保证了共享变量的可见性，让其它线程能够看到最新值，但不能解决指令交错问题（不能保证原子性） CAS 必须借助 volatile 才能读取到共享变量的最新值来实现【比较并交换】的效果 保证cas中compare的值永远都是最新的 3）无锁效率高的原因 无锁情况下，即使重试失败，线程始终在高速运行，没有停歇，而 synchronized 会让线程在没有获得锁的时候，发生上下文切换，进入阻塞。 线程就好像高速跑道上的赛车，高速运行时，速度超快，一旦发生上下文切换，就好比赛车要减速、熄火,等被唤醒又得重新打火、启动、加速… 恢复到高速运行，代价比较大 但无锁情况下，因为线程要保持运行，需要额外 CPU 的支持，CPU 在这里就好比高速跑道，没有额外的跑道，线程想高速运行也无从谈起 虽然不会进入阻塞，但可能线程时间片到期，会进入可运行状态，从CPU上下来，还是会导致上下文切换。 ​ 4）CAS 的特点结合 CAS 和 volatile 可以实现无锁并发，适用于线程数少、多核 CPU 的场景下。 CAS 是基于乐观锁的思想：最乐观的估计，不怕别的线程来修改共享变量，就算改了也没关系，我吃亏点再重试呗。 synchronized 是基于悲观锁的思想：最悲观的估计，得防着其它线程来修改共享变量，我上了锁你们都别想改，我改完了解开锁，你们才有机会。 CAS 体现的是无锁并发、无阻塞并发，请仔细体会这两句话的意思 因为没有使用 synchronized，所以线程不会陷入阻塞，这是效率提升的因素之一 但如果竞争激烈，可以想到重试必然频繁发生，反而效率会受影响 3.原子类1）原子整数 J.U.C 并发包提供了： AtomicBoolean AtomicInteger AtomicLong 以 AtomicInteger 为例： 12345678910111213141516171819202122232425262728293031323334353637AtomicInteger i = new AtomicInteger(0);// 获取并自增（i = 0, 结果 i = 1, 返回 0），类似于 i++System.out.println(i.getAndIncrement());// 自增并获取（i = 1, 结果 i = 2, 返回 2），类似于 ++iSystem.out.println(i.incrementAndGet());// 自减并获取（i = 2, 结果 i = 1, 返回 1），类似于 --iSystem.out.println(i.decrementAndGet());// 获取并自减（i = 1, 结果 i = 0, 返回 1），类似于 i--System.out.println(i.getAndDecrement());// 获取并加值（i = 0, 结果 i = 5, 返回 0）System.out.println(i.getAndAdd(5));// 加值并获取（i = 5, 结果 i = 0, 返回 0）System.out.println(i.addAndGet(-5));// 获取并更新（i = 0, p 为 i 的当前值, 结果 i = -2, 返回 0）// 其中函数中的操作能保证原子，但函数需要无副作用System.out.println(i.getAndUpdate(p -&gt; p - 2));// 更新并获取（i = -2, p 为 i 的当前值, 结果 i = 0, 返回 0）// 其中函数中的操作能保证原子，但函数需要无副作用System.out.println(i.updateAndGet(p -&gt; p + 2));// 获取并计算（i = 0, p 为 i 的当前值, x 为参数1, 结果 i = 10, 返回 0）// 其中函数中的操作能保证原子，但函数需要无副作用// getAndUpdate 如果在 lambda 中引用了外部的局部变量，要保证该局部变量是 final 的// getAndAccumulate 可以通过 参数1 来引用外部的局部变量，但因为其不在 lambda 中因此不必是 finalSystem.out.println(i.getAndAccumulate(10, (p, x) -&gt; p + x));// 计算并获取（i = 10, p 为 i 的当前值, x 为参数1, 结果 i = 0, 返回 0）// 其中函数中的操作能保证原子，但函数需要无副作用System.out.println(i.accumulateAndGet(-10, (p, x) -&gt; p + x)); 2）原子引用 为什么需要原子引用类型？ 除乐Integer我们，我们可能还需要一些其它类，如BigDecimal等 原子引用是针对引用的一个保护 AtomicReference AtomicStampedReference AtomicMarkableReference ①AtomicReference123456789101112131415161718192021222324class DecimalAccountSafeCas implements DecimalAccount &#123; AtomicReference&lt;BigDecimal&gt; ref; public DecimalAccountSafeCas(BigDecimal balance) &#123; ref = new AtomicReference&lt;&gt;(balance); //new的时候就需要进行包装 &#125; @Override public BigDecimal getBalance() &#123; return ref.get(); &#125; @Override public void withdraw(BigDecimal amount) &#123; while (true) &#123; BigDecimal prev = ref.get(); BigDecimal next = prev.subtract(amount); if (ref.compareAndSet(prev, next)) &#123; break; &#125; &#125; &#125; &#125; ②ABA问题12345678910111213141516171819202122232425262728static AtomicReference&lt;String&gt; ref = new AtomicReference&lt;&gt;(&quot;A&quot;);public static void main(String[] args) throws InterruptedException &#123; log.debug(&quot;main start...&quot;); // 获取值 A // 这个共享变量被它线程修改过？ String prev = ref.get(); other(); sleep(1); // 尝试改为 C log.debug(&quot;change A-&gt;C &#123;&#125;&quot;, ref.compareAndSet(prev, &quot;C&quot;));&#125;private static void other() &#123; new Thread(() -&gt; &#123; log.debug(&quot;change A-&gt;B &#123;&#125;&quot;, ref.compareAndSet(ref.get(), &quot;B&quot;)); &#125;, &quot;t1&quot;).start(); sleep(0.5); new Thread(() -&gt; &#123; log.debug(&quot;change B-&gt;A &#123;&#125;&quot;, ref.compareAndSet(ref.get(), &quot;A&quot;)); &#125;, &quot;t2&quot;).start(); &#125; 输出 123411:29:52.325 c.Test36 [main] - main start... 11:29:52.379 c.Test36 [t1] - change A-&gt;B true 11:29:52.879 c.Test36 [t2] - change B-&gt;A true 11:29:53.880 c.Test36 [main] - change A-&gt;C true 这里用了AtomicReference，出现了一个问题 main线程运行过程中，t1线程将A-&gt;B，t2线程将B-&gt;A，main线程在compare的时候发现确实是A 这时候main就会认为A变量没有被修改过，然后进行set main线程仅能判断出共享变量的值与最初值 A 是否相同，不能感知到这种从 A 改为 B 又 改回 A 的情况， 如果主线程希望，只要有其它线程【动过了】共享变量，那么自己的 cas 就算失败 这时，仅比较值是不够的，需要再加一个版本号 需要用到AtomicStampedReference ③AtomicStampedReference1234567891011121314151617181920212223242526272829303132static AtomicStampedReference&lt;String&gt; ref = new AtomicStampedReference&lt;&gt;(&quot;A&quot;, 0);public static void main(String[] args) throws InterruptedException &#123; log.debug(&quot;main start...&quot;); // 获取值 A String prev = ref.getReference(); // 获取版本号 int stamp = ref.getStamp(); log.debug(&quot;版本 &#123;&#125;&quot;, stamp); // 如果中间有其它线程干扰，发生了 ABA 现象 other(); sleep(1); // 尝试改为 C log.debug(&quot;change A-&gt;C &#123;&#125;&quot;, ref.compareAndSet(prev, &quot;C&quot;, stamp, stamp + 1));&#125;private static void other() &#123; new Thread(() -&gt; &#123; //在CAS的时候需要，判断版本号 log.debug(&quot;change A-&gt;B &#123;&#125;&quot;, ref.compareAndSet(ref.getReference(), &quot;B&quot;, ref.getStamp(), ref.getStamp() + 1)); log.debug(&quot;更新版本为 &#123;&#125;&quot;, ref.getStamp()); &#125;, &quot;t1&quot;).start(); sleep(0.5); new Thread(() -&gt; &#123; log.debug(&quot;change B-&gt;A &#123;&#125;&quot;, ref.compareAndSet(ref.getReference(), &quot;A&quot;, ref.getStamp(), ref.getStamp() + 1)); log.debug(&quot;更新版本为 &#123;&#125;&quot;, ref.getStamp()); &#125;, &quot;t2&quot;).start();&#125; 输出为 123456715:41:34.891 c.Test36 [main] - main start... 15:41:34.894 c.Test36 [main] - 版本 0 15:41:34.956 c.Test36 [t1] - change A-&gt;B true 15:41:34.956 c.Test36 [t1] - 更新版本为 1 15:41:35.457 c.Test36 [t2] - change B-&gt;A true 15:41:35.457 c.Test36 [t2] - 更新版本为 2 15:41:36.457 c.Test36 [main] - change A-&gt;C false AtomicStampedReference 可以给原子引用加上版本号，追踪原子引用整个的变化过程，如： A -&gt; B -&gt; A -&gt; C ，通过AtomicStampedReference，我们可以知道，引用变量中途被更改了几次。 AtomicStampedReference getStamp方法可以拿到版本号 compareAndSet(原来拿到的和现在做比较的值，要修改成的值， 拿到的版本号， 修改后的版本号); ④AtomicMarkableReference​ 但是有时候，并不关心引用变量更改了几次，只是单纯的关心是否更改过，所以只需要一个boolean来检查是否被更改过，所以就有了 AtomicMarkableReference 12345678910111213141516171819202122232425262728293031323334353637383940414243class GarbageBag &#123; String desc; public GarbageBag(String desc) &#123; this.desc = desc; &#125; public void setDesc(String desc) &#123; this.desc = desc; &#125; @Override public String toString() &#123; return super.toString() + &quot; &quot; + desc; &#125; &#125;@Slf4jpublic class TestABAAtomicMarkableReference &#123; public static void main(String[] args) throws InterruptedException &#123; GarbageBag bag = new GarbageBag(&quot;装满了垃圾&quot;); // 参数2 mark 可以看作一个标记，表示垃圾袋满了 AtomicMarkableReference&lt;GarbageBag&gt; ref = new AtomicMarkableReference&lt;&gt;(bag, true); log.debug(&quot;主线程 start...&quot;); GarbageBag prev = ref.getReference(); log.debug(prev.toString()); new Thread(() -&gt; &#123; log.debug(&quot;打扫卫生的线程 start...&quot;); bag.setDesc(&quot;空垃圾袋&quot;); while (!ref.compareAndSet(bag, bag, true, false)) &#123;&#125; log.debug(bag.toString()); &#125;).start(); Thread.sleep(1000); log.debug(&quot;主线程想换一只新垃圾袋？&quot;); boolean success = ref.compareAndSet(prev, new GarbageBag(&quot;空垃圾袋&quot;), true, false); log.debug(&quot;换了么？&quot; + success); log.debug(ref.getReference().toString()); &#125;&#125; 输出 12345672019-10-13 15:30:09.264 [main] 主线程 start... 2019-10-13 15:30:09.270 [main] cn.itcast.GarbageBag@5f0fd5a0 装满了垃圾2019-10-13 15:30:09.293 [Thread-1] 打扫卫生的线程 start... 2019-10-13 15:30:09.294 [Thread-1] cn.itcast.GarbageBag@5f0fd5a0 空垃圾袋2019-10-13 15:30:10.294 [main] 主线程想换一只新垃圾袋？2019-10-13 15:30:10.294 [main] 换了么？false 2019-10-13 15:30:10.294 [main] cn.itcast.GarbageBag@5f0fd5a0 空垃圾袋 垃圾袋原来的flag是一个true，需要更换状态 保洁阿姨先进行了更换，并把flag设置为false，是不更换 主人想更改，然后发现flag是false，就不更换 这里的flag是和AtomicMarkableReference绑定的，并不是和垃圾袋对象绑定的 compareAndSet(原来获得的要和现在对比的对象，要修改成的值 , 原来的flag, 执行操作后要修改成的flage); 3）原子数组 AtomicIntegerArray 若int[10] arr是共享变量，那么多个线程对其中一个数字arr[0]的修改也不是线程安全的 AtomicIntegerArray 可以保证数组中的每一个变量的线程安全 AtomicLongArray AtomicReferenceArray 有如下方法 12345678910111213141516171819202122232425262728293031323334353637/** 参数1，提供数组、可以是线程不安全数组或线程安全数组 参数2，获取数组长度的方法 参数3，自增方法，回传 array, index 参数4，打印数组的方法*/// supplier 提供者 无中生有 ()-&gt;结果// function 函数 一个参数一个结果 (参数)-&gt;结果 BiFunction 两个参数一个结果 (参数1,参数2)-&gt;结果 // consumer 消费者 一个参数没结果 (参数)-&gt;void BiConsumer 两个个参数没有 (参数1,参数2)-&gt; private static &lt;T&gt; void demo( Supplier&lt;T&gt; arraySupplier, Function&lt;T, Integer&gt; lengthFun, BiConsumer&lt;T, Integer&gt; putConsumer, Consumer&lt;T&gt; printConsumer ) &#123; List&lt;Thread&gt; ts = new ArrayList&lt;&gt;(); T array = arraySupplier.get(); int length = lengthFun.apply(array); for (int i = 0; i &lt; length; i++) &#123; // 每个线程对数组作 10000 次操作 ts.add(new Thread(() -&gt; &#123; for (int j = 0; j &lt; 10000; j++) &#123; putConsumer.accept(array, j%length); &#125; &#125;)); &#125; ts.forEach(t -&gt; t.start()); // 启动所有线程 ts.forEach(t -&gt; &#123; try &#123; t.join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); // 等所有线程结束 printConsumer.accept(array);&#125; 不安全的数组： 123456demo( ()-&gt;new int[10], (array)-&gt;array.length, (array, index) -&gt; array[index]++, array-&gt; System.out.println(Arrays.toString(array))); 结果 [9870, 9862, 9774, 9697, 9683, 9678, 9679, 9668, 9680, 9698] 安全的数组AtomicIntegerArray： 123456demo( ()-&gt; new AtomicIntegerArray(10), (array) -&gt; array.length(), (array, index) -&gt; array.getAndIncrement(index), array -&gt; System.out.println(array)); 结果 [10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000] 4）字段原子更新器 AtomicReferenceFieldUpdater &#x2F;&#x2F; 域字段 AtomicIntegerFieldUpdater AtomicLongFieldUpdater 利用字段更新器，可以针对对象的某个域（Field）进行原子操作 只能配合 volatile 修饰的字段使用， 否则会出现异常 Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: Must be volatile type 123456789101112131415public class Test5 &#123; public static void main(String[] args)&#123; Student stu = new Student(); AtomicReferenceFieldUpdater updater = AtomicReferenceFieldUpdater.newUpdater(Student.class,String.class,&quot;name&quot;); //这里希望是null，因为希望stu的name还没被赋过值 System.out.println(updater.compareAndSet(stu,null,&quot;张三&quot;); System.out.println(stu); &#125; class Student &#123; volatile String name; &#125;&#125; 获得AtomicIntegerFieldUpdater需要通过AtomicReferenceFieldUpdater.newUpdater()方法 需要传入3个参数（操作的类.class，操作的field.class，操作的field的name） Field需要被volatile修饰，保证该字段在线程间可见 之前的原子整数、原子引用、原子数组等都在自身用于接收值的value中添加了volatile修饰 如AtomicInteger中的value就是被volatile修饰 5）原子累加器 AtomicInteger可以做累加，为什么还需要原子类器？ 原子类型累加器是JDK1.8引进的并发新技术，它可以看做AtomicLong和AtomicDouble的部分加强类型。 为什么叫部分呢？是因为原子类型累加器适用于数据统计，并不适用于其他粒度的应用。 主要有以下四类： DoubleAccumulator DoubleAdder LongAccumulator LongAdder 1234567891011121314151617181920212223242526private static &lt;T&gt; void demo(Supplier&lt;T&gt; adderSupplier, Consumer&lt;T&gt; action) &#123; T adder = adderSupplier.get(); long start = System.nanoTime(); List&lt;Thread&gt; ts = new ArrayList&lt;&gt;(); // 4 个线程，每人累加 50 万 for (int i = 0; i &lt; 40; i++) &#123; ts.add(new Thread(() -&gt; &#123; for (int j = 0; j &lt; 500000; j++) &#123; action.accept(adder); &#125; &#125;)); &#125; ts.forEach(t -&gt; t.start()); ts.forEach(t -&gt; &#123; try &#123; t.join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); long end = System.nanoTime(); System.out.println(adder + &quot; cost:&quot; + (end - start)/1000_000);&#125; 比较 AtomicLong 与 LongAdder 123456for (int i = 0; i &lt; 5; i++) &#123; demo(() -&gt; new LongAdder(), adder -&gt; adder.increment());&#125;for (int i = 0; i &lt; 5; i++) &#123; demo(() -&gt; new AtomicLong(), adder -&gt; adder.getAndIncrement());&#125; 输出 12345678910111000000 cost:43 1000000 cost:9 1000000 cost:7 1000000 cost:7 1000000 cost:7 1000000 cost:31 1000000 cost:27 1000000 cost:28 1000000 cost:24 1000000 cost:22 性能提升的原因很简单，就是在有竞争时，设置多个累加单元，Therad-0 累加 Cell[0]，而 Thread-1 累加Cell[1]… 最后将结果汇总。 这样它们在累加时操作的不同的 Cell 变量，因此减少了 CAS 重试失败，从而提高性能。 6）源码详解LongAdderLongAdder 是并发大师 @author Doug Lea （大哥李）的作品，设计的非常精巧 LongAdder 类有几个关键域 12345678// 累加单元数组, 懒惰初始化transient volatile Cell[] cells;// 基础值, 如果没有竞争, 则用 cas 累加这个域transient volatile long base;// 在 cells 创建或扩容时, 置为 1, 表示加锁transient volatile int cellsBusy; 基本思路 比如要针对500累加，如果2个线程t1，t2同时来查以500位基准增加，势必会造成冲突，从而cas重试 可以将500分成cells[0]&#x3D;250和cells[1]&#x3D;250 t1可以用cells[0]++得到251，t2可以用cells[1]++得到251 最后t1和t2结果相加得到502 当让，如果没有冲突，那么直接就可以用500作为基准，也就是base字段 cellsBusy主要是在创建cells或扩容cells时起作用的 并不是针对每个cell的自身累加+1起作用 现在是2个线程t1和t2，这时候t3来了，目前有cell[0]和cell[1]，需要再加一个cell[2] 这时候就需要针对这个扩容操作加锁 ①(cellsBusy作为)cas 锁(实现示例)1234567891011121314151617// 不要用于实践！！！public class LockCas &#123; private AtomicInteger state = new AtomicInteger(0); public void lock() &#123; while (true) &#123; if (state.compareAndSet(0, 1)) &#123; break; &#125; &#125; &#125; public void unlock() &#123; log.debug(&quot;unlock...&quot;); state.set(0); &#125;&#125; 测试 12345678910111213141516171819202122LockCas lock = new LockCas();new Thread(() -&gt; &#123; log.debug(&quot;begin...&quot;); lock.lock(); try &#123; log.debug(&quot;lock...&quot;); sleep(1); &#125; finally &#123; lock.unlock(); &#125;&#125;).start();new Thread(() -&gt; &#123; log.debug(&quot;begin...&quot;); lock.lock(); try &#123; log.debug(&quot;lock...&quot;); &#125; finally &#123; lock.unlock(); &#125;&#125;).start(); 输出 12345618:27:07.198 c.Test42 [Thread-0] - begin... 18:27:07.202 c.Test42 [Thread-0] - lock... 18:27:07.198 c.Test42 [Thread-1] - begin... 18:27:08.204 c.Test42 [Thread-0] - unlock... 18:27:08.204 c.Test42 [Thread-1] - lock... 18:27:08.204 c.Test42 [Thread-1] - unlock... cellsBusy主要是在创建cells或扩容cells时起作用的 并不是针对每次的累加+1起作用 ②原理之伪共享其中 Cell 即为累加单元 1234567891011121314151617// 防止缓存行伪共享 // Contended: v.(尤指在争论中)声称，主张，认为;竞争;争夺 ,contend的过去分词和过去式@sun.misc.Contendedstatic final class Cell &#123; volatile long value; Cell(long x) &#123; value = x; &#125; // 最重要的方法, 用来 cas 方式进行累加, prev 表示旧值, next 表示新值 final boolean cas(long prev, long next) &#123; return UNSAFE.compareAndSwapLong(this, valueOffset, prev, next); &#125;// 省略不重要代码&#125; 注意@sun.misc.Contended注解，它是用来防止防止缓存行伪共享 关于缓存行伪共享问题 缓存与内存的速度比较： 因为 CPU 与 内存的速度差异很大，需要靠预读数据至缓存来提升效率。 而缓存以缓存行为单位，每个缓存行对应着一块内存，一般是 64 byte（8 个 long） 缓存的加入会造成数据副本的产生，即同一份数据会缓存在不同核心的缓存行中 CPU 要保证数据的一致性，如果某个 CPU 核心更改了数据，其它 CPU 核心对应的整个缓存行必须失效 因为 Cell 是数组形式，在内存中是连续存储的，一个 Cell 为 24 字节（16 字节的对象头和 8 字节的 value），因此缓存行可以存下 2 个的 Cell 对象。 这样问题来了： Core-0 要修改 Cell[0] Core-1 要修改 Cell[1] 无论谁修改成功，都会导致对方 Core 的缓存行失效， 比如 Core-0 中 Cell[0]=6000, Cell[1]=8000 要累加Cell[0]&#x3D;6001, Cell[1]&#x3D;8000 ，这时会让 Core-1 的缓存行失效 ; 同理 Core-1修改Cell[1]也会让 Core-0 的缓存行失效 解决方法: @sun.misc.Contended @sun.misc.Contended 用来解决这个问题，它的原理是在使用此注解的对象或字段的前后各增加 128 字节大小的padding，从而让 CPU 将对象预读至缓存时占用不同的缓存行，这样，不会造成对方缓存行的失效 ③add源码累加主要调用下面的方法 12345678910111213141516171819202122232425public void add(long x) &#123; // as 为累加单元数组 // b 为基础值 // x 为累加值 Cell[] as; long b, v; int m; Cell a; // 进入 if 的两个条件 // 1. as 有值, 表示已经发生过竞争, 进入 if // 2. cas 给 base 累加时失败了, 表示 base 发生了竞争, 进入 if if ((as = cells) != null || !casBase(b = base, b + x)) &#123; // uncontended 表示 cell 没有竞争 boolean uncontended = true; if ( // as 还没有创建 as == null || (m = as.length - 1) &lt; 0 || // 当前线程对应的 cell 还没有 (a = as[getProbe() &amp; m]) == null || // cas 给当前线程的 cell 累加失败 uncontended=false ( a 为当前线程的 cell ) !(uncontended = a.cas(v = a.value, v + x)) ) &#123; // 进入 cell 数组创建、cell 创建的流程 longAccumulate(x, null, uncontended); &#125; &#125;&#125; add 流程图： ④longAccumulate源码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051final void longAccumulate(long x, LongBinaryOperator fn,boolean wasUncontended) &#123; int h; // 当前线程还没有对应的 cell, 需要随机生成一个 h 值用来将当前线程绑定到 cell if ((h = getProbe()) == 0) &#123; // 初始化 probe ThreadLocalRandom.current(); // h 对应新的 probe 值, 用来对应 cell h = getProbe(); wasUncontended = true; &#125; // collide 为 true 表示需要扩容 boolean collide = false; for (;;) &#123; Cell[] as; Cell a; int n; long v; // 已经有了 cells if ((as = cells) != null &amp;&amp; (n = as.length) &gt; 0) &#123; // 还没有 cell if ((a = as[(n - 1) &amp; h]) == null) &#123; // 为 cellsBusy 加锁, 创建 cell, cell 的初始累加值为 x // 成功则 break, 否则继续 continue 循环 &#125; // 有竞争, 改变线程对应的 cell 来重试 cas else if (!wasUncontended) wasUncontended = true; // cas 尝试累加, fn 配合 LongAccumulator 不为 null, 配合 LongAdder 为 null else if (a.cas(v = a.value, ((fn == null) ? v + x : fn.applyAsLong(v, x)))) break; // 如果 cells 长度已经超过了最大长度, 或者已经扩容, 改变线程对应的 cell 来重试 cas else if (n &gt;= NCPU || cells != as) collide = false; // 确保 collide 为 false 进入此分支, 就不会进入下面的 else if 进行扩容了 else if (!collide) collide = true; // 加锁 else if (cellsBusy == 0 &amp;&amp; casCellsBusy()) &#123; // 加锁成功, 扩容 continue; &#125; // 改变线程对应的 cell h = advanceProbe(h); &#125; // 还没有 cells, 尝试给 cellsBusy 加锁 else if (cellsBusy == 0 &amp;&amp; cells == as &amp;&amp; casCellsBusy()) &#123; // 加锁成功, 初始化 cells, 最开始长度为 2, 并填充一个 cell // 成功则 break; &#125; // 上两种情况失败, 尝试给 base 累加 else if (casBase(v = base, ((fn == null) ? v + x : fn.applyAsLong(v, x)))) break; &#125;&#125; longAccumulate 流程图： 每个线程刚进入 longAccumulate 时，会尝试对应一个 cell 对象（找到一个坑位）： ⑤sum源码获取最终结果通过 sum 方法 1234567891011public long sum() &#123; Cell[] as = cells; Cell a; long sum = base; if (as != null) &#123; for (int i = 0; i &lt; as.length; ++i) &#123; if ((a = as[i]) != null) sum += a.value; &#125; &#125; return sum; &#125; 4.底层类Unsafe1）获得UnsafeUnsafe 对象提供了非常底层的，操作内存、线程的方法，Unsafe 对象不能直接调用，只能通过反射获得 1234567891011121314151617public class UnsafeAccessor &#123; static Unsafe unsafe; static &#123; try &#123; //getDeclaredField可以获得private成员变量，getField只能获得普通成员变量 Field theUnsafe = Unsafe.class.getDeclaredField(&quot;theUnsafe&quot;); //私有需要先设置允许访问私有成员变量 theUnsafe.setAccessible(true); unsafe = (Unsafe) theUnsafe.get(null); &#125; catch (NoSuchFieldException | IllegalAccessException e) &#123; throw new Error(e); &#125; &#125; static Unsafe getUnsafe() &#123; return unsafe; &#125;&#125; 2）Unsafe CAS 操作UnsafeAccessor工具类： 12345678910111213public class UnsafeAccessor &#123; private static final Unsafe unsafe; static &#123; try &#123; Field theUnsafe = Unsafe.class.getDeclaredField( &quot;theUnsafe&quot;); theUnsafe.setAccessible(true); unsafe = (Unsafe) theUnsafe.get(nul1); &#125;catch (NoSuchFieldException | IllegalAccessException e) &#123; throw new Error(e); &#125; &#125; public static Unsafe getUnsafe()&#123; return unsafe; &#125;&#125; 具体操作： 12345678910111213141516171819202122232425262728293031323334353637import lombok.Data;import sun.misc.Unsafe;import java.lang.reflect.Field;public class TestUnsafeCAS &#123; public static void main(String[] args) throws NoSuchFieldException, IllegalAccessException &#123; //暴力反射获得Unsafe Field theUnsafe = Unsafe.class.getDeclaredField(&quot;theUnsafe&quot;); theUnsafe.setAccessible(true); Unsafe unsafe = (Unsafe) theUnsafe.get(null); // Unsafe unsafe = UnsafeAccessor.getUnsafe(); // System.out.println(unsafe); // 1. 获取域的偏移地址 long idOffset = unsafe.objectFieldOffset(Teacher.class.getDeclaredField(&quot;id&quot;)); long nameOffset = unsafe.objectFieldOffset(Teacher.class.getDeclaredField(&quot;name&quot;)); Teacher t = new Teacher(); System.out.println(t); // 2. 执行 cas 操作 unsafe.compareAndSwapInt(t, idOffset, 0, 1);//for int unsafe.compareAndSwapObject(t, nameOffset, null, &quot;张三&quot;); //for object // 3. 验证 System.out.println(t); &#125;&#125;@Dataclass Teacher &#123; volatile int id; volatile String name;&#125; 如果不使用unsafe，还可以用字段原子更新器中的AtomicIntegerUpdater来进行包装并修改 unsafe是根据偏移地址来定位到对象的某个字段的 unsafe.compareAndSwapInt(对应的对象，要修改字段的偏移量，原来拿到的要进行比较的旧值，要修改成的值); 输出 123sun.misc.Unsafe@77556fdTeacher(id=0, name=null)Teacher(id=1, name=张三) 3）模拟实现原子整数使用自定义的 AtomicData 实现之前线程安全的原子整数 Account 实现 UnsafeAccessor工具类： 12345678910111213public class UnsafeAccessor &#123; private static final Unsafe unsafe; static &#123; try &#123; Field theUnsafe = Unsafe.class.getDeclaredField( &quot;theUnsafe&quot;); theUnsafe.setAccessible(true); unsafe = (Unsafe) theUnsafe.get(nul1); &#125;catch (NoSuchFieldException | IllegalAccessException e) &#123; throw new Error(e); &#125;&#125;public static Unsafe getUnsafe()&#123; return unsafe; &#125;&#125; 具体操作： 1234567891011121314151617181920212223242526272829303132333435363738class AtomicData &#123; // 保护的是一个Integer，需要配合cas使用，需要是volatile private volatile int data; static final Unsafe unsafe; static final long DATA_OFFSET; static &#123; unsafe = UnsafeAccessor.getUnsafe(); try &#123; // data 属性在 DataContainer 对象中的偏移量，用于 Unsafe 直接访问该属性 DATA_OFFSET = unsafe.objectFieldOffset(AtomicData.class.getDeclaredField(&quot;data&quot;)); &#125; catch (NoSuchFieldException e) &#123; //肯定有data域，不会出错 //这里不能抛出去，只能抛运行时异常 throw new Error(e); &#125; &#125; public AtomicData(int data) &#123; this.data = data; &#125; public void decrease(int amount) &#123; int oldValue; while(true) &#123; // 获取共享变量旧值，可以在这一行加入断点，修改 data 调试来加深理解 oldValue = data; // cas 尝试修改 data 为 旧值 + amount，如果期间旧值被别的线程改了，返回 false if (unsafe.compareAndSwapInt(this, DATA_OFFSET, oldValue, oldValue - amount)) &#123; return; &#125; &#125; &#125; public int getData() &#123; return data; &#125;&#125; 注意，这里是int的value，所以可以使用compareAndSwapInt 如果是Integer，需要使用compareAndSwapObject Account 实现： 1234567891011121314Account.demo(new Account() &#123; AtomicData atomicData = new AtomicData(10000); @Override public Integer getBalance() &#123; return atomicData.getData(); &#125; @Override public void withdraw(Integer amount) &#123; atomicData.decrease(amount); &#125; &#125;); 五、不可变类1.日期转换1）日期转换的问题问题提出—线程不安全类SimpleDateFormat 下面的代码在运行时，由于 SimpleDateFormat 不是线程安全的, 有很大几率出现 java.lang.NumberFormatException 或者出现不正确的日期解析结果， 1234567891011SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;);for (int i = 0; i &lt; 10; i++) &#123; new Thread(() -&gt; &#123; try &#123; log.debug(&quot;&#123;&#125;&quot;, sdf.parse(&quot;1951-04-21&quot;)); &#125; catch (Exception e) &#123; log.error(&quot;&#123;&#125;&quot;, e); &#125; &#125;).start();&#125; 2）解决-synchronized同步锁这样虽能解决问题，但带来的是性能上的损失，并不算很好： 12345678910111213SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;);for (int i = 0; i &lt; 50; i++) &#123; new Thread(() -&gt; &#123; synchronized (sdf) &#123; try &#123; log.debug(&quot;&#123;&#125;&quot;, sdf.parse(&quot;1951-04-21&quot;)); &#125; catch (Exception e) &#123; log.error(&quot;&#123;&#125;&quot;, e); &#125; &#125; &#125;).start();&#125; 3）解决 - 不可变 DateTimeFormatter如果一个对象在不能够修改其内部状态（属性），那么它就是线程安全的，因为不存在并发修改啊！ 这样的对象在Java 中有很多，例如在 Java 8 后，提供了一个新的日期格式化类： 12345678DateTimeFormatter dtf = DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd&quot;);for (int i = 0; i &lt; 10; i++) &#123; new Thread(() -&gt; &#123; LocalDate date = dtf.parse(&quot;2018-10-01&quot;, LocalDate::from); log.debug(&quot;&#123;&#125;&quot;, date); &#125;).start();&#125; 注意是通过DateTimeFormatter.ofPattern(日期格式)来获取 可以看 DateTimeFormatter 的文档： 12@implSpecThis class is immutable and thread-safe. // 不可变，并且是线程安全的 不可变对象，实际是另一种避免竞争的方式。 2. 不可变设计另一个大家更为熟悉的 String 类也是不可变的，以它为例，说明一下不可变设计的要素 123456789public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence &#123; /** The value is used for character storage. */ private final char value[]; /** Cache the hash code for the string */ private int hash; // Default to 0 // ... &#125; 1）final 的使用发现该类、类中所有属性都是final的 属性用 final 修饰保证了该属性是只读的，不能修改 private final char value[];，只能在构造的时候赋值，保证了对象引用不能改变 类用 final 修饰保证了该类中的方法不能被覆盖，防止子类无意间破坏不可变性 2）保护性拷贝 但有同学会说，使用字符串时，也有一些跟修改相关的方法啊，比如 substring 等， 在实现这些方法的时候使用了保护性拷贝思想 保护性拷贝——defensive copy ①构造方法12345678910111213public String() &#123; this.value = &quot;&quot;.value;&#125;public String(String original) &#123; this.value = original.value; this.hash = original.hash;&#125;public String(char value[]) &#123; this.value = Arrays.copyOf(value, value.length);&#125; 当传入的是一个String 的时候，两个对象都会指向同一个char[]数组 而当传入的是一个char value[]时候，String不会直接指向该char[]，而是copy了一个 这是防止将来在其它地方修改了该char[]，若String指向该char[]，那么其value也会被改变 ②substring12345678910public String substring(int beginIndex) &#123; if (beginIndex &lt; 0) &#123; throw new StringIndexOutOfBoundsException(beginIndex); &#125; int subLen = value.length - beginIndex; if (subLen &lt; 0) &#123; throw new StringIndexOutOfBoundsException(subLen); &#125; return (beginIndex == 0) ? this : new String(value, beginIndex, subLen);&#125; 发现其内部是调用 String 的构造方法创建了一个新字符串， 再进入这个构造看看，是否对 final char[] value 做出了修改： 123456789101112131415161718public String(char value[], int offset, int count) &#123; if (offset &lt; 0) &#123; throw new StringIndexOutOfBoundsException(offset); &#125; if (count &lt;= 0) &#123; if (count &lt; 0) &#123; throw new StringIndexOutOfBoundsException(count); &#125; if (offset &lt;= value.length) &#123; this.value = &quot;&quot;.value; return; &#125; &#125; if (offset &gt; value.length - count) &#123; throw new StringIndexOutOfBoundsException(offset + count); &#125; this.value = Arrays.copyOfRange(value, offset, offset+count);&#125; 结果发现也没有，构造新字符串对象时，会生成新的 char[] value，对内容进行复制 。 这种通过创建副本对象来避免共享的手段称之为【保护性拷贝（defensive copy）】 3.模式之享元 (池)1）简介定义 英文名称：Flyweight pattern. 当需要重用数量有限的同一类对象时 . wikipedia： A flyweight is an object that minimizes memory usage by sharing as much data as possible with other similar objects flyweight是一种通过与其他类似对象共享尽可能多的数据来最小化内存使用的对象 出自 “Gang of Four” design patterns 归类 Structual patterns 2）体现①包装类在JDK中 Boolean，Byte，Short，Integer，Long，Character 等包装类提供了 valueOf 方法， 例如 Long 的valueOf 会缓存 -128~127 之间的 Long 对象，在这个范围之间会重用对象，大于这个范围，才会新建 Long 对象： 1234567public static Long valueOf(long l) &#123; final int offset = 128; if (l &gt;= -128 &amp;&amp; l &lt;= 127) &#123; // will cache return LongCache.cache[(int)l + offset]; &#125; return new Long(l);&#125; 注意： Byte, Short, Long 缓存的范围都是 -128~127 Character 缓存的范围是 0~127 Integer的默认范围是 -128~127 最小值不能变 但最大值可以通过调整虚拟机参数 -Djava.lang.Integer.IntegerCache.high 来改变 Boolean 缓存了 TRUE 和 FALSE ②String 串池https://f1ashades.github.io/2022/04/13/JVM/#more ③BigDecimal BigInteger 单个方法是如.subtract是线程安全的 但是多个方法组合就不一定了 这些类的单个方法是线程安全的,但多个方法的组合使用如果也要保证线程安全就需要使用锁来保护了 3）应用-DIY 自定义数据库连接池例如：一个线上商城应用，QPS 达到数千，如果每次都重新创建和关闭数据库连接，性能会受到极大影响。 这时预先创建好一批连接，放入连接池。一次请求到达后，从连接池获取连接，使用完毕后再还回连接池， 这样既节约了连接的创建和关闭时间，也实现了连接的重用，不至于让庞大的连接数压垮数据库。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364class Pool &#123; // 1. 连接池大小 private final int poolSize; // 2. 连接对象数组 private Connection[] connections; // 3. 连接状态数组 0 表示空闲， 1 表示繁忙 private AtomicIntegerArray states; // 4. 构造方法初始化 public Pool(int poolSize) &#123; this.poolSize = poolSize; this.connections = new Connection[poolSize]; this.states = new AtomicIntegerArray(new int[poolSize]); for (int i = 0; i &lt; poolSize; i++) &#123; connections[i] = new MockConnection(&quot;连接&quot; + (i+1)); &#125; &#125; // 5. 借连接 public Connection borrow() &#123; while(true) &#123; for (int i = 0; i &lt; poolSize; i++) &#123; // 获取空闲连接 if(states.get(i) == 0) &#123; if (states.compareAndSet(i, 0, 1)) &#123;//cas保证线程安全性 log.debug(&quot;borrow &#123;&#125;&quot;, connections[i]); return connections[i]; &#125; &#125; &#125; // 如果没有空闲连接，当前线程进入等待 synchronized (this) &#123; try &#123; log.debug(&quot;wait...&quot;); this.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; // 6. 归还连接 public void free(Connection conn) &#123; for (int i = 0; i &lt; poolSize; i++) &#123; if (connections[i] == conn) &#123; states.set(i, 0); synchronized (this) &#123;//因为要唤醒其它因为没有连接而阻塞的线程，所以需要先获得锁 log.debug(&quot;free &#123;&#125;&quot;, conn); this.notifyAll(); &#125; break; &#125; &#125; &#125; &#125;class MockConnection implements Connection &#123; // 实现略，主要就是数据库的一些连接信息&#125; 使用连接池： 12345678910111213Pool pool = new Pool(2);for (int i = 0; i &lt; 5; i++) &#123; new Thread(() -&gt; &#123; Connection conn = pool.borrow(); try &#123; Thread.sleep(new Random().nextInt(1000)); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; pool.free(conn); &#125;).start();&#125; 以上实现没有考虑： 连接的动态增长与收缩 连接保活（可用性检测） 等待超时处理 分布式 hash 对于关系型数据库，有比较成熟的连接池实现，例如c3p0, druid等 对于更通用的对象池，可以考虑使用apache commons pool，例如redis连接池可以参考jedis中关于连接池的实现 4.原理之 final1）设置 final 变量的原理理解了 volatile 原理，再对比 final 的实现就比较简单了 123public class TestFinal &#123; final int a = 20; &#125; 字节码： 发现 final 变量的赋值也会通过 putfifield 指令来完成，同样在这条指令之后也会加入写屏障，保证在其它线程读到 它的值时不会出现为 0 的情况 2） 获取 final 变量的原理12345678910111213141516171819202122232425262728293031323334353637383940414243public class TestFinal &#123; static int A = 10; static int B = Short.MAX_VALUE+1; final int a = 20; final int b = Integer.MAX_VALUE; final void test1() &#123; final int c = 30; new Thread(()-&gt;&#123; System.out.println(c); &#125;).start(); final int d = 30; class Task implements Runnable &#123; @Override public void run() &#123; System.out.println(d); &#125; &#125; new Thread(new Task()).start(); &#125; &#125;class UseFinal1 &#123; public void test() &#123; System.out.println(TestFinal.A); System.out.println(TestFinal.B); System.out.println(new TestFinal().a); System.out.println(new TestFinal().b); new TestFinal().test1(); &#125;&#125;class UseFinal2 &#123; public void test() &#123; System.out.println(TestFinal.A); &#125;&#125; 需要从字节码层面看这段代码 final定义的变量，会在获取到值以后复制一份在栈中 数字没超过Short.MAX_VALUE，直接就是和字节码指令一起的 数字超过Short.MAX_VALUE，即Short.MAX_VALUE + 1，会在常量池中获取 如果没有final static从堆中的.class中获取 非static从堆中对应的对象获取 3）匿名内部类问题匿名内部类访问的局部变量为什么必须要用final修饰？ 参考 https://blog.csdn.net/tianjindong0804/article/details/81710268 匿名内部类之所以可以访问局部变量，是因为在底层将这个局部变量的值传入到了匿名内部类中 并且以匿名内部类的成员变量的形式存在，这个值的传递过程是通过匿名内部类的构造器完成的。 六、共享模型之工具1.自定义线程池 1）自定义拒绝策略接口 RejectPolicy1234567891011package com.tobestronger.n8._8_1;/** * 任务拒绝策略 * * @param &lt;T&gt; */@FunctionalInterfaceinterface RejectPolicy&lt;T&gt; &#123; void reject(BlockingQueue&lt;T&gt; queue, T task);&#125; 2）自定义任务队列 BlockingQueueBlockingQueue： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157package com.tobestronger.n8._8_1;import lombok.extern.slf4j.Slf4j;import java.util.ArrayDeque;import java.util.Deque;import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.ReentrantLock;/** * 任务队列 * * @param &lt;T&gt; */@Slf4j(topic = &quot;c.BlockingQueue&quot;)class BlockingQueue&lt;T&gt; &#123; // 1. 任务队列 private Deque&lt;T&gt; queue = new ArrayDeque&lt;&gt;(); // 2. 锁 private ReentrantLock lock = new ReentrantLock(); // 3. 生产者条件变量，容量满了进入fullWaitSet等待 private Condition fullWaitSet = lock.newCondition(); // 4. 消费者条件变量，没有线程可用进入emptyWaitSet等待 private Condition emptyWaitSet = lock.newCondition(); // 5. 容量，构造方法传递 private int capcity; public BlockingQueue(int capcity) &#123; log.info(&quot;构造BlockingQueue&quot;); this.capcity = capcity; &#125; // 带超时阻塞获取 public T poll(long timeout, TimeUnit unit) &#123; lock.lock(); try &#123; // 将 timeout 统一转换为 纳秒 long nanos = unit.toNanos(timeout); while (queue.isEmpty()) &#123; try &#123; // 返回值是剩余时间，第一次没获取到，第二次走到这里就会停止 if (nanos &lt;= 0) &#123; return null; &#125; //第一次到时间后重新获得锁然后继续执行，然后再进入while循环判断 //返回值=等待时间-已经经过的时间 //比如等了0.5秒被唤醒然后竞争锁成功，发现还是空的，那么下一次就只需要等0.5秒了 nanos = emptyWaitSet.awaitNanos(nanos); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; T t = queue.removeFirst(); fullWaitSet.signal(); return t; &#125; finally &#123; lock.unlock(); &#125; &#125; // 阻塞获取 public T take() &#123; lock.lock(); try &#123; while (queue.isEmpty()) &#123; try &#123; emptyWaitSet.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; T t = queue.removeFirst(); fullWaitSet.signal(); return t; &#125; finally &#123; lock.unlock(); &#125; &#125; // 阻塞添加 public void put(T task) &#123; lock.lock(); try &#123; while (queue.size() == capcity) &#123; try &#123; log.debug(&quot;等待加入任务队列 &#123;&#125; ...&quot;, task); fullWaitSet.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; log.debug(&quot;加入任务队列 &#123;&#125;&quot;, task); queue.addLast(task); emptyWaitSet.signal(); &#125; finally &#123; lock.unlock(); &#125; &#125; // 带超时时间阻塞添加 public boolean offer(T task, long timeout, TimeUnit timeUnit) &#123; lock.lock(); try &#123; long nanos = timeUnit.toNanos(timeout); while (queue.size() == capcity) &#123; try &#123; if(nanos &lt;= 0) &#123; return false; &#125; log.debug(&quot;等待加入任务队列 &#123;&#125; ...&quot;, task); nanos = fullWaitSet.awaitNanos(nanos); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; log.debug(&quot;加入任务队列 &#123;&#125;&quot;, task); queue.addLast(task); emptyWaitSet.signal();//唤醒take的消费者waitSet return true; &#125; finally &#123; lock.unlock(); &#125; &#125; public int size() &#123; lock.lock(); try &#123; return queue.size(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void tryPut(RejectPolicy&lt;T&gt; rejectPolicy, T task) &#123; lock.lock(); try &#123; // 判断队列是否满 if(queue.size() == capcity) &#123; log.info(&quot;队列已满,按照拒绝策略处理任务 &#123;&#125;&quot;,task); rejectPolicy.reject(this, task); &#125; else &#123; // 有空闲 log.debug(&quot;队列未满,加入任务队列 &#123;&#125;&quot;, task); queue.addLast(task); emptyWaitSet.signal(); &#125; &#125; finally &#123; lock.unlock(); &#125; &#125;&#125; 3）自定义线程池 ThreadPool123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112package com.tobestronger.n8._8_1;import lombok.extern.slf4j.Slf4j;import java.util.HashSet;import java.util.concurrent.TimeUnit;@Slf4j(topic = &quot;c.ThreadPool&quot;)class ThreadPool &#123; // 任务队列 private BlockingQueue&lt;Runnable&gt; taskQueue; // 线程集合 private HashSet&lt;Worker&gt; workers = new HashSet&lt;&gt;(); // 核心线程数 private int coreSize; // 获取任务时的超时时间 private long timeout; private TimeUnit timeUnit; /** * 拒绝策略 */ private RejectPolicy&lt;Runnable&gt; rejectPolicy; // 执行任务 public void execute(Runnable task) &#123; log.info(&quot;接收到任务需要执行: &quot;+task); // 当任务数没有超过 coreSize 时，直接交给 worker 对象执行 // 如果任务数超过 coreSize 时，加入任务队列暂存 synchronized (workers) &#123; if(workers.size() &lt; coreSize) &#123; log.info(&quot;coreSize未满&quot;); Worker worker = new Worker(task); log.debug(&quot;新增 worker &#123;&#125; 来执行任务 &#123;&#125;&quot;, worker, task); workers.add(worker); worker.start(); &#125; else &#123; log.info(&quot;coreSize已经满了!!!!!,尝试先将任务放入队列 &#123;&#125;&quot;,task); // taskQueue.put(task); // 1) 死等 // 2) 带超时等待 // 3) 让调用者放弃任务执行 // 4) 让调用者抛出异常 // 5) 让调用者自己执行任务 taskQueue.tryPut(rejectPolicy, task); &#125; &#125; &#125; public ThreadPool(int coreSize, long timeout, TimeUnit timeUnit, int queueCapcity, RejectPolicy&lt;Runnable&gt; rejectPolicy) &#123; log.info(&quot;构造ThreadPool&quot;); this.coreSize = coreSize; this.timeout = timeout; this.timeUnit = timeUnit; this.taskQueue = new BlockingQueue&lt;&gt;(queueCapcity); this.rejectPolicy = rejectPolicy; &#125; /** * 工作线程 */ class Worker extends Thread&#123; /** * 执行任务主体 */ private Runnable task; public Worker(Runnable task) &#123; this.task = task; &#125; /** * 执行已有任务或从队列中获取一个任务执行. * 如果都执行完了,就结束线程 */ @Override public void run() &#123; log.info(&quot;跑起来了,让我看看有没有task来做&quot;); // 执行任务 // 1) 当 task 不为空，执行任务 // 2) 当 task 执行完毕，再接着从任务队列获取任务并执行// while(task != null || (task = taskQueue.take()) != null) &#123; while(task != null || (task = taskQueue.poll(timeout, timeUnit)) != null) &#123; try &#123; log.debug(&quot;获取到任务了,正在执行...&#123;&#125;&quot;, task); task.run(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; log.info(&quot;搞定一个任务 &#123;&#125;,尝试获取新任务执行&quot;,task); task = null; &#125; &#125; synchronized (workers) &#123; log.debug(&quot;worker 因长时间没有可执行任务 将被释放 &#123;&#125;&quot;, this); workers.remove(this); &#125; &#125; &#125;&#125; 4）测试12345678910111213141516171819202122232425262728293031323334353637383940package com.tobestronger.n8._8_1;import lombok.extern.slf4j.Slf4j;import java.util.concurrent.TimeUnit;@Slf4j(topic = &quot;c.TestCustomThreadPool&quot;)public class TestCustomThreadPool &#123; public static void main(String[] args) &#123; ThreadPool threadPool = new ThreadPool(1, 3000, TimeUnit.MILLISECONDS, 1, (queue, task)-&gt;&#123; // 1. 死等// queue.put(task); // 2) 带超时等待// queue.offer(task, 1500, TimeUnit.MILLISECONDS); // 3) 让调用者放弃任务执行// log.debug(&quot;放弃&#123;&#125;&quot;, task); // 4) 让调用者抛出异常// throw new RuntimeException(&quot;任务执行失败 &quot; + task); // 5) 让调用者自己执行任务 log.info(&quot;当前拒绝策略: 让调用者自己执行任务,没有开新线程,直接调用的run()&quot;); task.run(); &#125;); for (int i = 0; i &lt; 4; i++) &#123; int j = i; threadPool.execute(() -&gt; &#123; try &#123; log.info(&quot;我先睡1s&quot;); Thread.sleep(1000L); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; log.debug(&quot;我是第 &#123;&#125; 个任务,我马上执行完了&quot;, j); &#125;); &#125; &#125;&#125; 某次的执行结果 1234567891011121314151617181920212223242526272829303132333435Connected to the target VM, address: &#x27;127.0.0.1:49956&#x27;, transport: &#x27;socket&#x27;18:25:21.216 c.ThreadPool [main] - 构造ThreadPool18:25:21.225 c.BlockingQueue [main] - 构造BlockingQueue18:25:21.228 c.ThreadPool [main] - 接收到任务需要执行: com.tobestronger.n8._8_1.TestCustomThreadPool$$Lambda$2/626742236@1b68b9a418:25:21.229 c.ThreadPool [main] - coreSize未满18:25:21.231 c.ThreadPool [main] - 新增 worker Thread[Thread-0,5,main] 来执行任务 com.tobestronger.n8._8_1.TestCustomThreadPool$$Lambda$2/626742236@1b68b9a418:25:21.235 c.ThreadPool [main] - 接收到任务需要执行: com.tobestronger.n8._8_1.TestCustomThreadPool$$Lambda$2/626742236@51b7e5df18:25:21.236 c.ThreadPool [main] - coreSize已经满了!!!!!,尝试先将任务放入队列 com.tobestronger.n8._8_1.TestCustomThreadPool$$Lambda$2/626742236@51b7e5df18:25:21.236 c.BlockingQueue [main] - 队列未满,加入任务队列 com.tobestronger.n8._8_1.TestCustomThreadPool$$Lambda$2/626742236@51b7e5df18:25:21.236 c.ThreadPool [Thread-0] - 跑起来了,让我看看有没有task来做18:25:21.236 c.ThreadPool [main] - 接收到任务需要执行: com.tobestronger.n8._8_1.TestCustomThreadPool$$Lambda$2/626742236@18a70f1618:25:21.236 c.ThreadPool [main] - coreSize已经满了!!!!!,尝试先将任务放入队列 com.tobestronger.n8._8_1.TestCustomThreadPool$$Lambda$2/626742236@18a70f1618:25:21.236 c.BlockingQueue [main] - 队列已满,按照拒绝策略处理任务 com.tobestronger.n8._8_1.TestCustomThreadPool$$Lambda$2/626742236@18a70f1618:25:21.236 c.TestCustomThreadPool [main] - 当前拒绝策略: 让调用者自己执行任务,没有开新线程,直接调用的run()18:25:21.236 c.ThreadPool [Thread-0] - 获取到任务了,正在执行...com.tobestronger.n8._8_1.TestCustomThreadPool$$Lambda$2/626742236@1b68b9a418:25:21.236 c.TestCustomThreadPool [main] - 我先睡1s18:25:21.236 c.TestCustomThreadPool [Thread-0] - 我先睡1s18:25:22.236 c.TestCustomThreadPool [Thread-0] - 我是第 0 个任务,我马上执行完了18:25:22.236 c.ThreadPool [Thread-0] - 搞定一个任务 com.tobestronger.n8._8_1.TestCustomThreadPool$$Lambda$2/626742236@1b68b9a4,尝试获取新任务执行18:25:22.236 c.TestCustomThreadPool [main] - 我是第 2 个任务,我马上执行完了18:25:22.237 c.ThreadPool [Thread-0] - 获取到任务了,正在执行...com.tobestronger.n8._8_1.TestCustomThreadPool$$Lambda$2/626742236@51b7e5df18:25:22.237 c.TestCustomThreadPool [Thread-0] - 我先睡1s18:25:22.237 c.ThreadPool [main] - 接收到任务需要执行: com.tobestronger.n8._8_1.TestCustomThreadPool$$Lambda$2/626742236@62e136d318:25:22.237 c.ThreadPool [main] - coreSize已经满了!!!!!,尝试先将任务放入队列 com.tobestronger.n8._8_1.TestCustomThreadPool$$Lambda$2/626742236@62e136d318:25:22.237 c.BlockingQueue [main] - 队列未满,加入任务队列 com.tobestronger.n8._8_1.TestCustomThreadPool$$Lambda$2/626742236@62e136d318:25:23.238 c.TestCustomThreadPool [Thread-0] - 我是第 1 个任务,我马上执行完了18:25:23.238 c.ThreadPool [Thread-0] - 搞定一个任务 com.tobestronger.n8._8_1.TestCustomThreadPool$$Lambda$2/626742236@51b7e5df,尝试获取新任务执行18:25:23.238 c.ThreadPool [Thread-0] - 获取到任务了,正在执行...com.tobestronger.n8._8_1.TestCustomThreadPool$$Lambda$2/626742236@62e136d318:25:23.238 c.TestCustomThreadPool [Thread-0] - 我先睡1s18:25:24.239 c.TestCustomThreadPool [Thread-0] - 我是第 3 个任务,我马上执行完了18:25:24.239 c.ThreadPool [Thread-0] - 搞定一个任务 com.tobestronger.n8._8_1.TestCustomThreadPool$$Lambda$2/626742236@62e136d3,尝试获取新任务执行18:25:27.241 c.ThreadPool [Thread-0] - worker 因长时间没有可执行任务 将被释放 Thread[Thread-0,5,main]Disconnected from the target VM, address: &#x27;127.0.0.1:49956&#x27;, transport: &#x27;socket&#x27;Process finished with exit code 0 2.ThreadPoolExecutor 1）线程池状态ThreadPoolExecutor 使用 int 的高 3 位来表示线程池状态，低 29 位表示线程数量 从数字上比较，TERMINATED &gt; TIDYING &gt; STOP &gt; SHUTDOWN &gt; RUNNING . 因为第一位是符号位,RUNNING 是负数,所以最小. 这些信息存储在一个原子变量 ctl 中，目的是将线程池状态与线程个数合二为一，这样就可以用一次 cas 原子操作进行赋值 12345// c 为旧值， ctlOf 返回结果为新值ctl.compareAndSet(c, ctlOf(targetState, workerCountOf(c))));// rs 为高 3 位代表线程池状态， wc 为低 29 位代表线程个数，ctl 是合并它们private static int ctlOf(int rs, int wc) &#123; return rs | wc; &#125; 2) 构造方法这是参数最多的一个构造 1234567public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) corePoolSize 核心线程数目 (最多保留的线程数) maximumPoolSize 最大线程数目 最大线程数目-核心线程数目&#x3D;救急线程数目 keepAliveTime 生存时间 - 针对救急线程 救急线程是有生存时间的，如果救急线程没有执行任务，会在设置的时间关闭 核心线程是没有是生存时间的，同时也不是守护线程，所以main结束后，核心线程还会等待执行任务 unit 时间单位 - 针对救急线程 workQueue 阻塞队列 threadFactory 线程工厂 - 可以为线程创建时起个好名字 可以重写，手动规定线程的名字 handler 拒绝策略 当阻塞队列满了，核心线程和救急线程都在执行任务 那么多余的任务应该以什么策略处理 工作方式： 线程池中刚开始没有线程，当一个任务提交给线程池后，线程池会创建一个新线程来执行任务。 当线程数达到 corePoolSize 并没有线程空闲，这时再加入任务，新加的任务会被加入workQueue 队列排队，直到有空闲的线程。 如果队列选择了有界队列，那么任务超过了队列大小时，会创建 maximumPoolSize - corePoolSize 数目的线程来救急。 如果线程到达 maximumPoolSize 仍然有新任务这时会执行拒绝策略。拒绝策略 jdk 提供了 4 种实现： AbortPolicy 让调用者抛出 RejectedExecutionException 异常，这是默认策略 CallerRunsPolicy 让调用者运行任务 DiscardPolicy 放弃本次任务 DiscardOldestPolicy 放弃队列中最早的任务，本任务取而代之 其它著名框架也提供了实现： Dubbo 的实现，在抛出 RejectedExecutionException 异常之前会记录日志，并 dump 线程栈信息，方便定位问题 Netty 的实现，是创建一个新线程来执行任务 ActiveMQ 的实现，带超时等待（60s）尝试放入队列，类似我们之前自定义的拒绝策略 PinPoint 的实现，它使用了一个拒绝策略链，会逐一尝试策略链中每种拒绝策略 当高峰过去后，超过corePoolSize 的救急线程如果一段时间没有任务做，需要结束节省资源，这个时间由keepAliveTime 和 unit 来控制。 3）JDK Executors类中提供的工厂方法​ ThreadPoolExecutor有众多的构造方法，JDK对这些ThreadPoolExecutor的构造进行了封装。针对不同的需求，我们可以通过工具类获得不同的ThreadPoolExecutor。 ①newFixedThreadPool12345public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads,//核心线程数 == 最大线程数 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125; 特点 核心线程数 &#x3D;&#x3D; 最大线程数（没有救急线程被创建），因此也无需超时时间 阻塞队列是无界的，可以放任意数量的任务 适用情况： 适用于任务量已知，相对耗时的任务 ②newCachedThreadPool12345public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125; 特点 核心线程数是 0，最大线程数是 Integer.MAX_VALUE，救急线程的空闲生存时间是 60s，意味着 全部都是救急线程（60s 后可以回收） 救急线程可以无限创建 队列采用了 SynchronousQueue 实现特点是，它没有容量，没有线程来取是放不进去的（一手交钱、一手交货） 123456789101112131415161718192021222324252627282930313233343536SynchronousQueue&lt;Integer&gt; integers = new SynchronousQueue&lt;&gt;();new Thread(() -&gt; &#123; try &#123; log.debug(&quot;putting &#123;&#125; &quot;, 1); integers.put(1); log.debug(&quot;&#123;&#125; putted...&quot;, 1); log.debug(&quot;putting...&#123;&#125; &quot;, 2); integers.put(2); log.debug(&quot;&#123;&#125; putted...&quot;, 2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;&#125;,&quot;t1&quot;).start();sleep(1);new Thread(() -&gt; &#123; try &#123; log.debug(&quot;taking &#123;&#125;&quot;, 1); integers.take(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;&#125;,&quot;t2&quot;).start();sleep(1);new Thread(() -&gt; &#123; try &#123; log.debug(&quot;taking &#123;&#125;&quot;, 2); integers.take(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;&#125;,&quot;t3&quot;).start(); 输出 12345611:48:15.500 c.TestSynchronousQueue [t1] - putting 1 11:48:16.500 c.TestSynchronousQueue [t2] - taking 1 11:48:16.500 c.TestSynchronousQueue [t1] - 1 putted... 11:48:16.500 c.TestSynchronousQueue [t1] - putting...2 11:48:17.502 c.TestSynchronousQueue [t3] - taking 2 11:48:17.503 c.TestSynchronousQueue [t1] - 2 putted... 可以看到，只有在有救急线程尝试从阻塞队列去取task的情况下，才能把task放进SynchronousQueue队列 这也就是一个同步的过程 适用情况： 整个线程池表现为线程数会根据任务量不断增长，没有上限，当任务执行完毕，空闲 1分钟后释放线程。 适合任务数比较密集，但每个任务执行时间较短的情况 ③newSingleThreadExecutor123456public static ExecutorService newSingleThreadExecutor() &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));&#125; 注意这里使用了装饰器模式，FinalizableDelegatedExecutorService中的方法其实就是调用了ThreadPoolExecutor的方法，但是类似setCorePoolSize等ThreadPoolExecutor 中特有的方法并没有写，这样就能保证返回的FinalizableDelegatedExecutorService中不能使用setCorePoolSize来对核心数进行修改 和自己创建一个线程来工作的区别: 自己创建一个单线程串行执行任务，如果任务执行失败而终止那么没有任何补救措施，而线程池还会新建一个线程，保证池的正常工作 和Executors.newFixedThreadPool(1)的区别 Executors.newSingleThreadExecutor() 线程个数始终为1，不能修改 FinalizableDelegatedExecutorService 应用的是装饰器模式，只对外暴露了 ExecutorService（FinalizableDelegatedExecutorService ） 接口，因此不能调用 ThreadPoolExecutor 中特有的方法 Executors.newFixedThreadPool(1) 初始时为1，以后还可以修改 对外暴露的是 ThreadPoolExecutor 对象，可以强转成ThreadPoolExecutor 后调用 setCorePoolSize 等方法进行修改 但这里即使强转成FinalizableDelegatedExecutorService，也没有对应的方法setCorePoolSize 适用场景： 希望多个任务排队执行。线程数固定为 1，任务数多于 1 时，会放入无界队列排队。 任务执行完毕，这唯一的线程也不会被释放。 4) 提交任务的方法1234567891011121314151617181920212223// 执行任务void execute(Runnable command);// 提交任务 task，会等待任务执行结束，用返回值 Future 获得任务执行结果，&lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task); //也可以接受Runnable&lt;T&gt; task// 提交 tasks 中所有任务，最后返回的是所有tasks执行完得到的结果&lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException;// 提交 tasks 中所有任务，带超时时间&lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException;// 提交 tasks 中所有任务，哪个任务先成功执行完毕，返回此任务执行结果，其它任务取消&lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException, ExecutionException;// 提交 tasks 中所有任务，哪个任务先成功执行完毕，返回此任务执行结果，其它任务取消，带超时时间&lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException; eg：invokeAny 123456789101112131415161718192021222324252627public static void main(String[] args) throws ExecutionException,InterruptedException &#123; ExecutorService pool = Executors.newFixedThreadPool(3);//有3个线程，下面3个任务会同时执行 //得到的只有一个最先执行成功的结果 String result = pool.invokeAny(Arrays.asList( () -&gt;&#123; Log.debug(&quot;begin&quot;); Thread.sleep(1000); Log.debug(&quot;end&quot;) ; return &quot;1&quot;; &#125;, ()-&gt;&#123; Log.debug(&quot;begin&quot;); Thread.sleep(500); Log.debug(&quot;end&quot;) ; return &quot;2&quot;; &#125;, ()-&gt;&#123; Log.debug(&quot;begin&quot;); Thread.sleep(2000); Log.debug(&quot;end&quot;) ; return &quot;3&quot;; &#125; )); Log.debug(&quot;0&quot;,result); &#125; 结果： 12345[pool-1-thread-2] - begin[pool-1-thread-3] - begin[poo1-1-thread-1] - begin[poo1-1-thread-2] - end //只有2执行完毕，因为最先执行完（sleep时间最少）[main]- 2 //main线程获得的结果也是2返回的结果 5）关闭线程池的方法①shutdown123456789101112131415161718192021222324/*线程池状态变为 SHUTDOWN- 不会接收新任务- 但已提交任务会执行完- 此方法不会阻塞调用线程的执行*/void shutdown();public void shutdown() &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; checkShutdownAccess(); // 修改线程池状态 advanceRunState(SHUTDOWN); // 仅会打断空闲线程 interruptIdleWorkers(); onShutdown(); // 扩展点 ScheduledThreadPoolExecutor &#125; finally &#123; mainLock.unlock(); &#125; // 尝试终结(没有运行的线程可以立刻终结，如果还有运行的线程也不会等) tryTerminate();&#125; eg： 12345678910111213141516171819202122232425262728public static void main(String[] args) throws InterruptedException &#123; ExecutorService pool = Executors.newFixedThreadPool(2); Future&lt;Integer&gt; result1 = pool.submit(() -&gt; &#123; Log.debug(&quot;task 1 running...&quot;); Thread.sleep( 1000); Log.debug(&quot;task 1 finish...&quot;); return 3; &#125;); Future&lt;Integer&gt; result2 = pool.submit(() -&gt; &#123; Log.debug(&quot;task 2 running...&quot;); Thread.sleep( 1000); Log.debug(&quot;task 2 finish...&quot;); return 3; &#125;); Future&lt;Integer&gt; result3 = pool.submit(() -&gt; &#123; Log.debug(&quot;task 3 running...&quot;); Thread.sleep( 1000); Log.debug(&quot;task 3 finish...&quot;); return 3; &#125;); Log.debug( &quot;shutdown&quot;); pool.shutdown(); pool.awaitTermination (3, TimeUnit.SECONDS); Log.debug( &quot;other....&quot;);&#125; 12345678[main] - shutdown //可以看到执行shutdown，并不会影响正在执行的方法和阻塞队列中的方法[poo1-1-thread-1] - task 1 running...[pool-1-thread-2] - task 2 running...[pool-1-thread-1] - task 1 finish...[poo1-1-thread-2] - task 2 finish[poo1-1-thread-1] - task 3 running...[pool-1-thread-1] - task 3 finish...[main] - other.... //因为执行了awaitTermination方法，在shutdown 3秒后才会打印other.... ②shutdownNow12345678910111213141516171819202122232425262728/*线程池状态变为 STOP- 不会接收新任务- 会将队列中的任务返回- 并用 interrupt 的方式中断正在执行的任务*/List&lt;Runnable&gt; shutdownNow();public List&lt;Runnable&gt; shutdownNow() &#123; List&lt;Runnable&gt; tasks; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; checkShutdownAccess(); // 修改线程池状态 advanceRunState(STOP); // 打断所有线程 interruptWorkers(); // 获取队列中剩余任务 tasks = drainQueue(); &#125; finally &#123; mainLock.unlock(); &#125; // 尝试终结 tryTerminate(); return tasks; &#125; 打断会终止正在执行的任务和阻塞队列中的任务，返回值就是阻塞队列中还没执行的任务 通过interruptWorkers();打断所有正在运行的线程 通过tasks = drainQueue();获取队列中剩余任务 ③其它方法12345678// 不在 RUNNING 状态的线程池，此方法就返回 trueboolean isShutdown();// 线程池状态是否是 TERMINATEDboolean isTerminated();// 调用 shutdown 后，由于调用线程并不会等待所有任务运行结束，因此如果它想在线程池 TERMINATED 后做些事情，可以利用此方法等待boolean awaitTermination(long timeout, TimeUnit unit) throws InterruptedException; 3.异步模式之Worker Thread (工作线程)1） 定义 让有限的工作线程（Worker Thread）来轮流异步处理无限多的任务。 也可以将其归类为分工模式，它的典型实现就是线程池，也体现了经典设计模式中的享元模式。 例如，海底捞的服务员（线程），轮流处理每位客人的点餐（任务），如果为每位客人都配一名专属的服务员，那么成本就太高了（对比另一种多线程设计模式：Thread-Per-Message） 注意：不同任务类型应该使用不同的线程池，这样能够避免饥饿，并能提升效率 例如，如果一个餐馆的工人既要招呼客人（任务类型A），又要到后厨做菜（任务类型B）显然效率不咋地，分成服务员（线程池A）与厨师（线程池B）更为合理，当然你能想到更细致的分工 2）饥饿固定大小线程池会有饥饿现象 两个工人是同一个线程池中的两个线程 他们要做的事情是：为客人点餐和到后厨做菜，这是两个阶段的工作 客人点餐：必须先点完餐，等菜做好，上菜，在此期间处理点餐的工人必须等待 后厨做菜：没啥说的，做就是了 比如工人A 处理了点餐任务，接下来它要等着 工人B 把菜做好，然后上菜，他俩也配合的蛮好 但现在同时来了两个客人，这个时候工人A 和工人B 都去处理点餐了，这时没人做饭了，饥饿 12345678910111213141516171819202122232425262728293031323334353637383940public class TestStarvation &#123; static final List&lt;String&gt; MENU = Arrays.asList(&quot;地三鲜&quot;, &quot;宫保鸡丁&quot;, &quot;辣子鸡丁&quot;, &quot;烤鸡翅&quot;); static Random RANDOM = new Random(); static String cooking() &#123; return MENU.get(RANDOM.nextInt(MENU.size())); &#125; public static void main(String[] args) &#123; ExecutorService executorService = Executors.newFixedThreadPool(2);//只有两个线程 executorService.execute(() -&gt; &#123; log.debug(&quot;处理点餐...&quot;); Future&lt;String&gt; f = executorService.submit(() -&gt; &#123; log.debug(&quot;做菜&quot;); return cooking(); &#125;); try &#123; log.debug(&quot;上菜: &#123;&#125;&quot;, f.get()); &#125; catch (InterruptedException | ExecutionException e) &#123; e.printStackTrace(); &#125; &#125;); /* executorService.execute(() -&gt; &#123; log.debug(&quot;处理点餐...&quot;); Future&lt;String&gt; f = executorService.submit(() -&gt; &#123; log.debug(&quot;做菜&quot;); return cooking(); &#125;); try &#123; log.debug(&quot;上菜: &#123;&#125;&quot;, f.get()); &#125; catch (InterruptedException | ExecutionException e) &#123; e.printStackTrace(); &#125; &#125;); */ &#125;&#125; 输出 12317:21:27.883 c.TestDeadLock [pool-1-thread-1] - 处理点餐...17:21:27.891 c.TestDeadLock [pool-1-thread-2] - 做菜17:21:27.891 c.TestDeadLock [pool-1-thread-1] - 上菜: 烤鸡翅 当注释取消后，可能的输出 1217:08:41.339 c.TestDeadLock [pool-1-thread-2] - 处理点餐... 17:08:41.339 c.TestDeadLock [pool-1-thread-1] - 处理点餐... 一共只有两个线程，两个线程都处理完点餐然后等待做菜的futureTask 但是已经没有线程可以能够做菜了，做菜的任务就在阻塞队列中等着 解决方法可以增加线程池的大小，不过不是根本解决方案 还是前面提到的，不同的任务类型，采用不同的线程 池，例如： 1234567891011121314151617181920212223242526272829303132333435363738394041public class TestStarvation &#123; static final List&lt;String&gt; MENU = Arrays.asList(&quot;地三鲜&quot;, &quot;宫保鸡丁&quot;, &quot;辣子鸡丁&quot;, &quot;烤鸡翅&quot;); static Random RANDOM = new Random(); static String cooking() &#123; return MENU.get(RANDOM.nextInt(MENU.size())); &#125; public static void main(String[] args) &#123; ExecutorService waiterPool = Executors.newFixedThreadPool(1); ExecutorService cookPool = Executors.newFixedThreadPool(1); waiterPool.execute(() -&gt; &#123; log.debug(&quot;处理点餐...&quot;); Future&lt;String&gt; f = cookPool.submit(() -&gt; &#123; log.debug(&quot;做菜&quot;); return cooking(); &#125;); try &#123; log.debug(&quot;上菜: &#123;&#125;&quot;, f.get()); &#125; catch (InterruptedException | ExecutionException e) &#123; e.printStackTrace(); &#125; &#125;); waiterPool.execute(() -&gt; &#123; log.debug(&quot;处理点餐...&quot;); Future&lt;String&gt; f = cookPool.submit(() -&gt; &#123; log.debug(&quot;做菜&quot;); return cooking(); &#125;); try &#123; log.debug(&quot;上菜: &#123;&#125;&quot;, f.get()); &#125; catch (InterruptedException | ExecutionException e) &#123; e.printStackTrace(); &#125; &#125;); &#125;&#125; 输出 12345617:25:14.626 c.TestDeadLock [pool-1-thread-1] - 处理点餐... 17:25:14.630 c.TestDeadLock [pool-2-thread-1] - 做菜17:25:14.631 c.TestDeadLock [pool-1-thread-1] - 上菜: 地三鲜17:25:14.632 c.TestDeadLock [pool-1-thread-1] - 处理点餐... 17:25:14.632 c.TestDeadLock [pool-2-thread-1] - 做菜17:25:14.632 c.TestDeadLock [pool-1-thread-1] - 上菜: 辣子鸡丁 3）创建多少线程池合适 过小会导致程序不能充分地利用系统资源、容易导致饥饿 过大会导致更多的线程上下文切换，占用更多内存 ①CPU 密集型运算 通常采用 cpu 核数 + 1 能够实现最优的 CPU 利用率， +1 是保证当线程由于页缺失故障（操作系统）或其它原因导致暂停时，额外的这个线程就能顶上去，保证 CPU 时钟周期不被浪费 如4核CPU，5个线程，t4发生缺页，需要从外存中进行页面置换，这时候CPU就会阻塞，那么t5就会顶替上去 ②I&#x2F;O 密集型运算 Web应用就是I&#x2F;O 密集型运算 CPU 不总是处于繁忙状态，在很多时刻CPU是空闲的 例如，当你执行业务计算时，这时候会使用 CPU 资源，但当你执行 I&#x2F;O 操作时、远程 RPC 调用时，包括进行数据库操作时，这时候 CPU 就闲下来了，你可以利用多线程提高它的利用率。 经验公式如下 1线程数 = 核数 * 期望 CPU 利用率 * 总时间(CPU计算时间+等待时间) / CPU 计算时间 例如 4 核 CPU 计算时间是 50% ，其它等待时间是 50%，期望 cpu 被 100% 利用，套用公式 14 * 100% * 100% / 50% = 8 例如 4 核 CPU 计算时间是 10% ，其它等待时间是 90%，期望 cpu 被 100% 利用，套用公式 14 * 100% * 100% / 10% = 40 4.ScheduledExecutorService1）Timerjava.util.Timer 在『任务调度线程池』功能加入之前，可以使用 java.util.Timer 来实现定时功能，Timer 的优点在于简单易用，但 由于所有任务都是由同一个线程来调度，因此所有任务都是串行执行的，同一时间只能有一个任务在执行，前一个任务的延迟或异常都将会影响到之后的任务。 12345678910111213141516171819202122232425public static void main(String[] args) &#123; Timer timer = new Timer(); TimerTask task1 = new TimerTask() &#123; @Override public void run() &#123; log.debug(&quot;task 1&quot;); sleep(2); &#125; &#125;; TimerTask task2 = new TimerTask() &#123; @Override public void run() &#123; log.debug(&quot;task 2&quot;); &#125; &#125;; log.debug(&quot;start...&quot;); // 使用 timer 添加两个任务，希望它们都在 1s 后执行 // 但由于 timer 内只有一个线程来顺序执行队列中的任务，因此『任务1』的延时，影响了『任务2』的执行 // 甚至如果task1出异常停止后,task2都不会执行 timer.schedule(task1, 1000); timer.schedule(task2, 1000);&#125; 输出 12320:46:09.444 c.TestTimer [main] - start... 20:46:10.447 c.TestTimer [Timer-0] - task 1 //两者都是延时1s执行20:46:12.448 c.TestTimer [Timer-0] - task 2 //串行执行，task1花了2s 只有一个线程来串行执行任务，task1执行完才能执行task2 task1出现异常，task2就无法执行 建议使用ScheduledExecutorService来改进 2）使用 ScheduledExecutorService任务调度线程池——ScheduledExecutorService ①使用 ScheduledExecutorService 改写Timer1234567891011121314ScheduledExecutorService executor = Executors.newScheduledThreadPool(2);//两个线程来执行任务// 添加两个任务，希望它们都在 1s 后执行executor.schedule(() -&gt; &#123; System.out.println(&quot;任务1，执行时间：&quot; + new Date()); try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; &#125;&#125;, 1000, TimeUnit.MILLISECONDS);executor.schedule(() -&gt; &#123; System.out.println(&quot;任务2，执行时间：&quot; + new Date());&#125;, 1000, TimeUnit.MILLISECONDS); 输出 12任务1，执行时间：Thu Jan 03 12:45:17 CST 2019 任务2，执行时间：Thu Jan 03 12:45:17 CST 2019 ②scheduleAtFixedRatescheduleAtFixedRate（初始延迟时间，时间间隔，时间单位） 123456ScheduledExecutorService pool = Executors.newScheduledThreadPool(1);log.debug(&quot;start...&quot;);pool.scheduleAtFixedRate(() -&gt; &#123; log.debug(&quot;running...&quot;);&#125;, 1, 1, TimeUnit.SECONDS); 输出 12345621:45:43.167 c.TestTimer [main] - start... 21:45:44.215 c.TestTimer [pool-1-thread-1] - running... //初始延时1s21:45:45.215 c.TestTimer [pool-1-thread-1] - running... //执行间隔是1s21:45:46.215 c.TestTimer [pool-1-thread-1] - running... 21:45:47.215 c.TestTimer [pool-1-thread-1] - running............不断执行 scheduleAtFixedRate 例子（任务执行时间超过了间隔时间）： 1234567ScheduledExecutorService pool = Executors.newScheduledThreadPool(1);log.debug(&quot;start...&quot;);pool.scheduleAtFixedRate(() -&gt; &#123; log.debug(&quot;running...&quot;); sleep(2);//任务本身就有2s&#125;, 1, 1, TimeUnit.SECONDS);//执行间隔只有1s 输出分析：一开始，延时 1s，接下来，由于任务执行时间 &gt; 间隔时间，间隔被『撑』到了 2s 1234521:44:30.311 c.TestTimer [main] - start... 21:44:31.360 c.TestTimer [pool-1-thread-1] - running... //2s的间隔21:44:33.361 c.TestTimer [pool-1-thread-1] - running... 21:44:35.362 c.TestTimer [pool-1-thread-1] - running... 21:44:37.362 c.TestTimer [pool-1-thread-1] - running... ③scheduleWithFixedDelay123456ScheduledExecutorService pool = Executors.newScheduledThreadPool(1);log.debug(&quot;start...&quot;);pool.scheduleWithFixedDelay(()-&gt; &#123; log.debug(&quot;running...&quot;); sleep(2);&#125;, 1, 1, TimeUnit.SECONDS); 输出分析：一开始，延时 1s，scheduleWithFixedDelay 的间隔是 上一个任务结束 —&gt;延时 —&gt; 下一个任务开始 所以间隔都是 3s 1234521:40:55.078 c.TestTimer [main] - start... 21:40:56.140 c.TestTimer [pool-1-thread-1] - running... 21:40:59.143 c.TestTimer [pool-1-thread-1] - running... 21:41:02.145 c.TestTimer [pool-1-thread-1] - running... 21:41:05.147 c.TestTimer [pool-1-thread-1] - running... 这里的延时就是每个任务间固定的延时了，不会被任务本身的执行时间影响 评价 整个线程池表现为：线程数固定，任务数多于线程数时，会放入无界队列排队。任务执行完毕，这些线程也不会被释放。用来执行延迟或反复执行的任务 3）异常处理①主动捉异常123456789ExecutorService pool = Executors.newFixedThreadPool(1);pool.submit(() -&gt; &#123; try &#123; log.debug(&quot;task1&quot;); int i = 1 / 0; //这里会产生异常 &#125; catch (Exception e) &#123; log.error(&quot;error:&quot;, e); &#125;&#125;); 输出 12345678921:59:04.558 c.TestTimer [pool-1-thread-1] - task1 21:59:04.562 c.TestTimer [pool-1-thread-1] - error: java.lang.ArithmeticException: / by zero at cn.itcast.n8.TestTimer.lambda$main$0(TestTimer.java:28) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) ②使用 Future pool.submit()可以接收Runnable task也可以 接收Future task 没有写返回值，默认就是接收Runnable 如果有返回值，接收Future task 如果发生异常，会返回该异常，也就是把异常信息封装到了返回的Future对象中 12345678ExecutorService pool = Executors.newFixedThreadPool(1);Future&lt;Boolean&gt; f = pool.submit(() -&gt; &#123; log.debug(&quot;task1&quot;); int i = 1 / 0; return true; //有错会返回异常&#125;);log.debug(&quot;result:&#123;&#125;&quot;, f.get());//得到的是异常信息 输出 12345678910111221:54:58.208 c.TestTimer [pool-1-thread-1] - task1 Exception in thread &quot;main&quot; java.util.concurrent.ExecutionException: java.lang.ArithmeticException: / by zero at java.util.concurrent.FutureTask.report(FutureTask.java:122) at java.util.concurrent.FutureTask.get(FutureTask.java:192) at cn.itcast.n8.TestTimer.main(TestTimer.java:31) Caused by: java.lang.ArithmeticException: / by zero at cn.itcast.n8.TestTimer.lambda$main$0(TestTimer.java:28) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) 4） 应用之定时任务定期执行 如何让每周四 18:00:00 定时执行任务？ 123456789101112131415161718192021// 获得当前时间LocalDateTime now = LocalDateTime.now();// 获取本周四 18:00:00.000LocalDateTime thursday = now.with(DayOfWeek.THURSDAY).withHour(18).withMinute(0).withSecond(0).withNano(0);// 如果当前时间已经超过 本周四 18:00:00.000， 那么找下周四 18:00:00.000if(now.compareTo(thursday) &gt;= 0) &#123; thursday = thursday.plusWeeks(1);&#125;// 计算时间差，即延时执行时间long initialDelay = Duration.between(now, thursday).toMillis();// 计算间隔时间，即 1 周的毫秒值long oneWeek = 7 * 24 * 3600 * 1000;ScheduledExecutorService executor = Executors.newScheduledThreadPool(2);System.out.println(&quot;开始时间：&quot; + new Date());executor.scheduleAtFixedRate(() -&gt; &#123; System.out.println(&quot;执行时间：&quot; + new Date());&#125;, initialDelay, oneWeek, TimeUnit.MILLISECONDS); 5.Tomcat 线程池Tomcat 在哪里用到了线程池呢 LimitLatch 用来限流，可以控制最大连接个数，类似 J.U.C 中的 Semaphore 后面再讲 Acceptor 只负责【接收新的 socket 连接】 默认只有一个 Poller 只负责监听 socket channel 是否有【可读的 I&#x2F;O 事件】 默认只有一个，采用I&#x2F;O复用 一旦可读，封装一个任务对象（socketProcessor），提交给 Executor 线程池处理 Executor 线程池中的工作线程最终负责【处理请求】 1）ThreadPoolExecutorTomcat 线程池扩展了 ThreadPoolExecutor，行为稍有不同 如果总线程数达到 maximumPoolSize，说明所有线程都在执行任务，阻塞队列也满了 这时不会立刻抛 RejectedExecutionException 异常（默认拒绝策略） 而是再次尝试将任务放入队列，看队列是否还有位置，如果还失败，才抛出 RejectedExecutionException 异常 父类抛异常，子类catch该异常并再次重试 ​ 源码 tomcat-7.0.42 1234567891011121314151617181920212223public void execute(Runnable command, long timeout, TimeUnit unit) &#123; submittedCount.incrementAndGet(); try &#123; super.execute(command);//父类抛异常 &#125; catch (RejectedExecutionException rx) &#123;//子类catch该异常并再次重试 if (super.getQueue() instanceof TaskQueue) &#123; final TaskQueue queue = (TaskQueue)super.getQueue(); try &#123; if (!queue.force(command, timeout, unit)) &#123; submittedCount.decrementAndGet(); throw new RejectedExecutionException(&quot;Queue capacity is full.&quot;); &#125; &#125; catch (InterruptedException x) &#123; submittedCount.decrementAndGet(); Thread.interrupted(); throw new RejectedExecutionException(x); &#125; &#125; else &#123; submittedCount.decrementAndGet(); throw rx; &#125; &#125;&#125; 2）TaskQueue.java12345678public boolean force(Runnable o, long timeout, TimeUnit unit) throws InterruptedException &#123; if ( parent.isShutdown() ) throw new RejectedExecutionException( &quot;Executor not running, can&#x27;t force a command into the queue&quot; ); return super.offer(o,timeout,unit); //forces the item onto the queue, to be used if the task is rejected&#125; tomcat对阻塞队列进行了自己的实现 3）配置Connector 配置： Executor进行了详细的配置，优先级比minSpareThreads等更高 Executor 线程配置： tomcat的线程都是守护线程，主线程关闭后会跟着关闭 队列长度的默认值是Integer的最大值，这里可以修改，防止大量任务堆积在队列中 4）具体流程 tomcat会对任务进行一个计数 任务计数&lt;核心线程数，直接加入队列 核心线程数&lt;任务计数 任务计数&lt;最大线程数，创建救急线程 最大线程数&lt;任务计数，将任务加入队列 如果线程数满，队列也满了 父类的execute会抛出RejectedExecutionException异常（默认拒绝策略） 子类会catch该异常，再次尝试将任务放入队列中，如果放入失败，就再次会抛出RejectedExecutionException异常 6.Fork&#x2F;Join (分治思想)1) 概念 Fork&#x2F;Join 是 JDK 1.7 加入的新的线程池实现，它体现的是一种分治思想，适用于能够进行任务拆分的 cpu 密集型运算 所谓的任务拆分，是将一个大任务拆分为算法上相同的小任务，直至不能拆分可以直接求解。跟递归相关的一些计算，如归并排序、斐波那契数列、都可以用分治思想进行求解 Fork&#x2F;Join 在分治的基础上加入了多线程，可以把每个任务的分解和合并交给不同的线程来完成，进一步提升了运算效率 Fork&#x2F;Join 默认会创建与 cpu 核心数大小相同的线程池 ，因为是要进行CPU密集型运算 2) 使用提交给 Fork&#x2F;Join 线程池的任务需要继承 RecursiveTask（有返回值）或 RecursiveAction（没有返回值），例如下面定义了一个对 1~n 之间的整数求和的任务 1234567891011121314151617181920212223242526272829303132@Slf4j(topic = &quot;c.AddTask&quot;)class AddTask1 extends RecursiveTask&lt;Integer&gt; &#123;//泛型就是返回值 int n; public AddTask1(int n) &#123; this.n = n; &#125; @Override public String toString() &#123; return &quot;&#123;&quot; + n + &#x27;&#125;&#x27;; &#125; @Override protected Integer compute() &#123; // 如果 n 已经为 1，可以求得结果了 if (n == 1) &#123; log.debug(&quot;join() &#123;&#125;&quot;, n); return n; &#125; // 将任务进行拆分(fork) AddTask1 t1 = new AddTask1(n - 1); t1.fork(); log.debug(&quot;fork() &#123;&#125; + &#123;&#125;&quot;, n, t1); // 合并(join)结果 int result = n + t1.join(); log.debug(&quot;join() &#123;&#125; + &#123;&#125; = &#123;&#125;&quot;, n, t1, result); return result; &#125;&#125; 然后提交给 ForkJoinPool 来执行 1234public static void main(String[] args) &#123; ForkJoinPool pool = new ForkJoinPool(4);//参数就手动设置线程数，默认是和CPU核心一致 System.out.println(pool.invoke(new AddTask1(5)));&#125; 结果 12345678910[ForkJoinPool-1-worker-0] - fork() 2 + &#123;1&#125; [ForkJoinPool-1-worker-1] - fork() 5 + &#123;4&#125; [ForkJoinPool-1-worker-0] - join() 1 [ForkJoinPool-1-worker-0] - join() 2 + &#123;1&#125; = 3 [ForkJoinPool-1-worker-2] - fork() 4 + &#123;3&#125; [ForkJoinPool-1-worker-3] - fork() 3 + &#123;2&#125; [ForkJoinPool-1-worker-3] - join() 3 + &#123;2&#125; = 6 [ForkJoinPool-1-worker-2] - join() 4 + &#123;3&#125; = 10 [ForkJoinPool-1-worker-1] - join() 5 + &#123;4&#125; = 15 15 用图来表示 改进： 12345678910111213141516171819202122232425262728293031323334353637383940class AddTask3 extends RecursiveTask&lt;Integer&gt; &#123; int begin; int end; public AddTask3(int begin, int end) &#123; this.begin = begin; this.end = end; &#125; @Override public String toString() &#123; return &quot;&#123;&quot; + begin + &quot;,&quot; + end + &#x27;&#125;&#x27;; &#125; @Override protected Integer compute() &#123; // 5, 5 if (begin == end) &#123; log.debug(&quot;join() &#123;&#125;&quot;, begin); return begin; &#125; // 4, 5 if (end - begin == 1) &#123; log.debug(&quot;join() &#123;&#125; + &#123;&#125; = &#123;&#125;&quot;, begin, end, end + begin); return end + begin; &#125; // 1 5 int mid = (end + begin) / 2; // 3 AddTask3 t1 = new AddTask3(begin, mid); // 1,3 t1.fork(); AddTask3 t2 = new AddTask3(mid + 1, end); // 4,5 t2.fork(); log.debug(&quot;fork() &#123;&#125; + &#123;&#125; = ?&quot;, t1, t2); int result = t1.join() + t2.join(); log.debug(&quot;join() &#123;&#125; + &#123;&#125; = &#123;&#125;&quot;, t1, t2, result); return result; &#125;&#125; 然后提交给 ForkJoinPool 来执行 1234public static void main(String[] args) &#123; ForkJoinPool pool = new ForkJoinPool(4); System.out.println(pool.invoke(new AddTask3(1, 10)));&#125; 结果 12345678[ForkJoinPool-1-worker-0] - join() 1 + 2 = 3 [ForkJoinPool-1-worker-3] - join() 4 + 5 = 9 [ForkJoinPool-1-worker-0] - join() 3 [ForkJoinPool-1-worker-1] - fork() &#123;1,3&#125; + &#123;4,5&#125; = ? [ForkJoinPool-1-worker-2] - fork() &#123;1,2&#125; + &#123;3,3&#125; = ? [ForkJoinPool-1-worker-2] - join() &#123;1,2&#125; + &#123;3,3&#125; = 6 [ForkJoinPool-1-worker-1] - join() &#123;1,3&#125; + &#123;4,5&#125; = 15 15 用图来表示 7.AQS 原理1）原理全称是 AbstractQueuedSynchronizer，是阻塞式锁和相关的同步器工具的框架 ①特点state属性 独占&#x2F;共享模式 用 state 属性来表示资源的状态（分独占模式和共享模式），子类需要定义如何维护这个状态，控制如何获取锁和释放锁 getState - 获取 state 状态 setState - 设置 state 状态 compareAndSetState - cas 机制设置 state 状态 独占模式是只有一个线程能够访问资源，而共享模式可以允许多个线程访问资源 等待队列 提供了基于 FIFO 的等待队列，类似于 Monitor 的 EntryList 条件变量 条件变量来实现等待、唤醒机制，支持多个条件变量，类似于 Monitor 的 WaitSet ②相关方法需要子类实现的方法 子类主要实现这样一些方法（默认抛出 UnsupportedOperationException） tryAcquire tryRelease tryAcquireShared tryReleaseShared isHeldExclusively 获取锁的姿势 1234// 如果获取锁失败if (!tryAcquire(arg)) &#123; // 入队, 可以选择阻塞当前线程 阻塞的实现是：park unpark&#125; 释放锁的姿势 1234// 如果释放锁成功if (tryRelease(arg)) &#123; // 让阻塞线程恢复运行&#125; 2）实现不可重入①自定义同步器123456789101112131415161718192021222324252627class MySync extends AbstractQueuedSynchronizer &#123; //只尝试一次 @Override protected boolean tryAcquire(int arg) &#123; if(compareAndSetState(0, 1)) &#123; //cas修改锁状态 setExclusiveOwnerThread(Thread.currentThread()); //设置owner return true; &#125; return false; //cas失败则返回false &#125; @Override protected boolean tryRelease(int arg) &#123; setExclusiveOwnerThread(null);//只有一个线程能获得锁 setState(0); return true; &#125; public Condition newCondition() &#123; return new ConditionObject(); &#125; @Override protected boolean isHeldExclusively() &#123; //是否持有独占锁 return getState() == 1; &#125;&#125; 自己实现了AQS的一些方法，其它方法就直接使用父类的 ②自定义锁 作为锁的类需要实现Lock接口，规范了锁的方法 有了自定义同步器，很容易复用 AQS ，实现一个功能完备的自定义锁 123456789101112131415161718192021222324252627282930313233343536373839404142// 自定义锁（不可重入锁）class MyLock implements Lock &#123;private MySync mySync = new MySync(); // 加锁 @Override public void lock() &#123; mySync.acquire(1);//用的父类的方法 &#125; // 可中断的锁 @Override public void lockInterruptibly() throws InterruptedException &#123; mySync.acquireInterruptibly(1); //用的父类的方法 &#125; // 只会尝试一次加锁 @Override public boolean tryLock() &#123; return mySync.tryAcquire(1);//自己实现的 &#125; // 带超时时间的 @Override public boolean tryLock(long time, TimeUnit unit) throws InterruptedException &#123; return mySync.tryAcquireNanos(1, unit.toNanos(time));//用的父类的方法 &#125; // 解锁 @Override public void unlock() &#123; mySync.release(1);//自己实现的 &#125; // 创建条件变量 @Override public Condition newCondition() &#123; return mySync.newCondition();//自己实现的 &#125;&#125; ③测试123456789101112131415161718192021@Slf4j(topic = &quot;c.Code_11_UnRepeatLockTest&quot;)public class Code_11_UnRepeatLockTest &#123; public static void main(String[] args) &#123; MyLock myLock = new MyLock(); new Thread(() -&gt; &#123; myLock.lock(); log.info(&quot;lock ... &quot;); // 测试是否不可重入 myLock.lock(); try &#123; log.info(&quot;starting ... &quot;); Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; log.info(&quot;unlock ... &quot;); myLock.unlock(); &#125; &#125;, &quot;t1&quot;).start(); &#125;&#125; 8.ReentrantLock 原理可以看到 ReentrantLock 提供了两个同步器，实现公平锁和非公平锁，默认是非公平锁！ 1）加锁流程先从构造器开始看，默认为非公平锁实现 123public ReentrantLock() &#123; sync = new NonfairSync();&#125; NonfairSync 继承自 AQS ①没有竞争时123456789final void lock() &#123; // 没有竞争时, 直接加锁 if (compareAndSetState(0, 1)) // 设置持有锁的线程 setExclusiveOwnerThread(Thread.currentThread()); else // 有竞争, 会调用这个方法 acquire(1); &#125; ②第一个竞争出现时123456public final void acquire(int arg) &#123; // 再次尝试加锁, 然后为 true 就不走下面逻辑，为 false，则创建一个 Node 节点对象加入到等待队列中去 if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125; Thread-1 执行了： lock方法中CAS 尝试将 state 由 0 改为 1，结果失败 lock方法中进一步调用acquire方法，进入 tryAcquire 逻辑，这里我们认为这时 state 已经是1，结果仍然失败 接下来进入 acquire方法的addWaiter 逻辑，构造 Node 队列 图中黄色三角表示该 Node 的 waitStatus 状态，其中 0 为默认正常状态 Node 的创建是懒惰的 其中第一个 Node 称为 Dummy（哑元）或哨兵，用来占位，并不关联线程 当前线程进入 acquireQueued 逻辑 ： acquireQueued 会在一个死循环中不断尝试获得锁，失败后进入 park 阻塞 如果自己是紧邻着 head（排第二位），那么再次 tryAcquire 尝试获取锁，当然这时 state 仍为 1，失败 进入 shouldParkAfterFailedAcquire 逻辑，将前驱 node，即 head 的 waitStatus 改为 -1，这次返回 false shouldParkAfterFailedAcquire 执行完毕回到 acquireQueued ，再次 tryAcquire 尝试获取锁，当然这时 state 仍为 1，失败 当再次进入 shouldParkAfterFailedAcquire 时，这时因为其前驱 node 的 waitStatus 已经是 -1，这次返回 true 进入 parkAndCheckInterrupt， Thread-1 park（灰色表示已经阻塞） 这里的逻辑就是，在让Node 1进入阻塞队列并被park阻塞的过程中会进行多次尝试， 如果这几次仍然不能获得锁，那么该线程就会在阻塞队列中真正的被阻塞 阻塞队列中的waitStatus &#x3D;-1，代表其有责任叫醒后继节点 2）解锁流程①解锁hread-0 释放锁，进入 tryRelease 流程，如果成功 设置 exclusiveOwnerThread 为 null state &#x3D; 0 ②队列内线程抢到锁当前队列不为 null，并且 head 的 waitStatus &#x3D; -1，进入 unparkSuccessor 流程 找到队列中离 head 最近的一个 Node（没取消的），unpark 恢复其运行，本例中即为 Thread-1 回到 Thread-1 的 acquireQueued 流程 如果加锁成功（没有竞争），会设置 exclusiveOwnerThread 为 Thread-1，state &#x3D; 1 head 指向刚刚 Thread-1 所在的 Node，该Node的信息清空为null，成为新的head 原本的 head 因为从链表断开，而可被垃圾回收 ③队列外线程抢到锁如果这时候有其它线程来竞争（非公平的体现），例如这时有 Thread-4 来了 如果不巧又被 Thread-4 占了先 Thread-4 被设置为 exclusiveOwnerThread，state &#x3D; 1 Thread-1 再次进入 acquireQueued 流程，获取锁失败，重新进入 park 阻塞 3）加锁和解锁源码①加锁源码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156// Sync 继承自 AQSstatic final class NonfairSync extends Sync &#123; private static final long serialVersionUID = 7316153563782823691L; // 加锁实现 final void lock() &#123; // 首先用 cas 尝试（仅尝试一次）将 state 从 0 改为 1, 如果成功表示获得了独占锁 if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else // 如果尝试失败，进入 ㈠ acquire(1); &#125; // ㈠ AQS 继承过来的方法, 方便阅读, 放在此处 public final void acquire(int arg) &#123; // ㈡ tryAcquire if ( !tryAcquire(arg) &amp;&amp; // 当 tryAcquire 返回为 false 时, 先调用 addWaiter ㈣, 接着 acquireQueued ㈤ acquireQueued(addWaiter(Node.EXCLUSIVE), arg) ) &#123; selfInterrupt(); &#125; &#125; // ㈡ 进入 ㈢ protected final boolean tryAcquire(int acquires) &#123; return nonfairTryAcquire(acquires); &#125; // ㈢ Sync 继承过来的方法, 方便阅读, 放在此处 final boolean nonfairTryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); // 如果还没有获得锁 if (c == 0) &#123; // 尝试用 cas 获得, 这里体现了非公平性: 不去检查 AQS 队列 if (compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; // 如果已经获得了锁, 线程还是当前线程, 表示发生了锁重入 else if (current == getExclusiveOwnerThread()) &#123; // state++ int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; &#125; // 获取失败, 回到调用处 return false; &#125; // ㈣ AQS 继承过来的方法, 方便阅读, 放在此处 private Node addWaiter(Node mode) &#123; // 将当前线程关联到一个 Node 对象上, 模式为独占模式 Node node = new Node(Thread.currentThread(), mode); // 如果 tail 不为 null, cas 尝试将 Node 对象加入 AQS 队列尾部 Node pred = tail; if (pred != null) &#123; node.prev = pred; if (compareAndSetTail(pred, node)) &#123; // 双向链表 pred.next = node; return node; &#125; &#125; // 尝试将 Node 加入 AQS, 进入 ㈥ enq(node); return node; &#125; // ㈥ AQS 继承过来的方法, 方便阅读, 放在此处 private Node enq(final Node node) &#123; for (;;) &#123; Node t = tail; if (t == null) &#123; // 还没有, 设置 head 为哨兵节点（不对应线程，状态为 0） if (compareAndSetHead(new Node())) &#123; tail = head; &#125; &#125; else &#123; // cas 尝试将 Node 对象加入 AQS 队列尾部 node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125; &#125; // ㈤ AQS 继承过来的方法, 方便阅读, 放在此处 final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); // 上一个节点是 head, 表示轮到自己（当前线程对应的 node）了, 尝试获取 if (p == head &amp;&amp; tryAcquire(arg)) &#123; // 获取成功, 设置自己（当前线程对应的 node）为 head setHead(node); // 上一个节点 help GC p.next = null; failed = false; // 返回中断标记 false return interrupted; &#125; if ( // 判断是否应当 park, 进入 ㈦ shouldParkAfterFailedAcquire(p, node) &amp;&amp; // park 等待, 此时 Node 的状态被置为 Node.SIGNAL ㈧ parkAndCheckInterrupt() ) &#123; interrupted = true; &#125; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125; &#125; // ㈦ AQS 继承过来的方法, 方便阅读, 放在此处 private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123; // 获取上一个节点的状态 int ws = pred.waitStatus; if (ws == Node.SIGNAL) &#123; // 上一个节点都在阻塞, 那么自己也阻塞好了 return true; &#125; // &gt; 0 表示取消状态 if (ws &gt; 0) &#123; // 上一个节点取消, 那么重构删除前面所有取消的节点, 返回到外层循环重试 do &#123; node.prev = pred = pred.prev; &#125; while (pred.waitStatus &gt; 0); pred.next = node; &#125; else &#123; // 这次还没有阻塞 // 但下次如果重试不成功, 则需要阻塞，这时需要设置上一个节点状态为 Node.SIGNAL compareAndSetWaitStatus(pred, ws, Node.SIGNAL); &#125; return false; &#125; // ㈧ 阻塞当前线程 private final boolean parkAndCheckInterrupt() &#123; LockSupport.park(this); return Thread.interrupted(); &#125;&#125; 注意 是否需要 unpark 是由当前节点的前驱节点的 waitStatus &#x3D;&#x3D; Node.SIGNAL 来决定，而不是本节点的 waitStatus 决定 ②解锁源码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364// Sync 继承自 AQSstatic final class NonfairSync extends Sync &#123; // 解锁实现 public void unlock() &#123; sync.release(1); &#125; // AQS 继承过来的方法, 方便阅读, 放在此处 public final boolean release(int arg) &#123; // 尝试释放锁, 进入 ㈠ if (tryRelease(arg)) &#123; // 队列头节点 unpark Node h = head; if ( // 队列不为 null h != null &amp;&amp; // waitStatus == Node.SIGNAL 才需要 unpark h.waitStatus != 0 ) &#123; // unpark AQS 中等待的线程, 进入 ㈡ unparkSuccessor(h); &#125; return true; &#125; return false; &#125; // ㈠ Sync 继承过来的方法, 方便阅读, 放在此处 protected final boolean tryRelease(int releases) &#123; // state-- int c = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; // 支持锁重入, 只有 state 减为 0, 才释放成功 if (c == 0) &#123; free = true; setExclusiveOwnerThread(null); &#125; setState(c); return free; &#125; // ㈡ AQS 继承过来的方法, 方便阅读, 放在此处 private void unparkSuccessor(Node node) &#123; // 如果状态为 Node.SIGNAL 尝试重置状态为 0 // 不成功也可以 int ws = node.waitStatus; if (ws &lt; 0) &#123; compareAndSetWaitStatus(node, ws, 0); &#125; // 找到需要 unpark 的节点, 但本节点从 AQS 队列中脱离, 是由唤醒节点完成的 Node s = node.next; // 不考虑已取消的节点, 从 AQS 队列从后至前找到队列最前面需要 unpark 的节点 if (s == null || s.waitStatus &gt; 0) &#123; s = null; for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; &#125; if (s != null) LockSupport.unpark(s.thread); &#125;&#125; 4）可重入原理1234567891011121314151617181920212223242526272829303132333435363738394041static final class NonfairSync extends Sync &#123; // ... // Sync 继承过来的方法, 方便阅读, 放在此处 final boolean nonfairTryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; if (compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; // 如果已经获得了锁, 线程还是当前线程, 表示发生了锁重入 else if (current == getExclusiveOwnerThread()) &#123; // state++ int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; &#125; return false; &#125; // Sync 继承过来的方法, 方便阅读, 放在此处 protected final boolean tryRelease(int releases) &#123; // state-- int c = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; // 支持锁重入, 只有 state 减为 0, 才释放成功 if (c == 0) &#123; free = true; setExclusiveOwnerThread(null); &#125; setState(c); return free; &#125;&#125; 重入过程就是判断目前的owner是不是自己线程，如果是自己的线程那么就让state的值+1，表示重入的次数 只有当state&#x3D;0的时候才表示该线程所有重入都被释放了，才能真正释放锁成功 设置 exclusiveOwnerThread 为 null state &#x3D; 0 5）可打断原理①(默认)不可打断模式1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950// Sync 继承自 AQSstatic final class NonfairSync extends Sync &#123; // ... private final boolean parkAndCheckInterrupt() &#123; // 如果打断标记已经是 true, 则 park 会失效 LockSupport.park(this); // interrupted 会清除打断标记 return Thread.interrupted(); &#125; final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) &#123; // 还是需要获得锁后, 才能返回打断状态 setHead(node); p.next = null; failed = false; return interrupted; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp;parkAndCheckInterrupt()) &#123; // 如果是因为 interrupt 被唤醒, 返回打断状态为 true //并不会抛出 打断异常，还是会继续死循环 interrupted = true; &#125; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125; &#125; public final void acquire(int arg) &#123; if ( !tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg) ) &#123; // 如果打断状态为 true selfInterrupt(); &#125; &#125; static void selfInterrupt() &#123; // 重新产生一次中断 Thread.currentThread().interrupt(); &#125;&#125; 在此模式下，即使它被打断，仍会驻留在 AQS 队列中，一直要等到获得锁后方能得知自己被打断了 即被打断后会把打断标志设置为true，但是仍然不会有行动，只有当获得锁以后才能返回打断状态 ②可打断模式1234567891011121314151617181920212223242526272829303132333435static final class NonfairSync extends Sync &#123; public final void acquireInterruptibly(int arg) throws InterruptedException &#123; if (Thread.interrupted()) throw new InterruptedException(); // 如果没有获得到锁, 进入 ㈠ if (!tryAcquire(arg)) doAcquireInterruptibly(arg); &#125; // ㈠ 可打断的获取锁流程 private void doAcquireInterruptibly(int arg) throws InterruptedException &#123; final Node node = addWaiter(Node.EXCLUSIVE); boolean failed = true; try &#123; for (;;) &#123; final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC failed = false; return; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) &#123; // 在 park 过程中如果被 interrupt 会进入此 // 这时候抛出异常, 而不会再次进入 for (;;) throw new InterruptedException(); &#125; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125; &#125;&#125; 在AQS队列中在park的时候，如果被interrupt 会抛出InterruptedException();异常 6）公平锁实现原理12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152static final class FairSync extends Sync &#123; private static final long serialVersionUID = -3000897897090466540L; final void lock() &#123; acquire(1); &#125; // AQS 继承过来的方法, 方便阅读, 放在此处 public final void acquire(int arg) &#123; if ( !tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg) ) &#123; selfInterrupt(); &#125; &#125; // 与非公平锁主要区别在于 tryAcquire 方法的实现 protected final boolean tryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; // 先检查 AQS 队列中是否有前驱节点, 没有才去竞争 if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; &#125; return false; &#125; // ㈠ AQS 继承过来的方法, 方便阅读, 放在此处 public final boolean hasQueuedPredecessors() &#123; Node t = tail; Node h = head; Node s; // h != t 时表示队列中有 Node return h != t &amp;&amp; ( // (s = h.next) == null 表示队列中还有没有老二 (s = h.next) == null || // 或者队列中老二线程不是此线程 s.thread != Thread.currentThread() ); &#125;&#125; 就是在竞争锁之前，需要先检查阻塞队列中是否有节点 没节点，再去竞争锁 有节点，则将自己加入阻塞队列的队尾，并将其前驱结点的waitStatus设置为-1 7 ）条件变量实现原理每个条件变量其实就对应着一个等待队列，其实现类是 ConditionObject ①await 流程开始 Thread-0 持有锁，调用 await，进入 ConditionObject 的 addConditionWaiter 流程 创建新的 Node 状态为 -2（Node.CONDITION），关联 Thread-0，加入等待队列尾部 接下来进入 AQS 的 fullyRelease 流程，释放同步器上的锁： 因为可能该线程之前重入了多次，所以需要fullyRelease 保证该线程将锁完全释放 unpark AQS 队列中的下一个节点，竞争锁，假设没有其他竞争线程，那么 Thread-1 竞争成功： park 阻塞 Thread-0： ②signal 流程假设 Thread-1 要来唤醒 Thread-0 进入 ConditionObject 的 doSignal 流程，取得等待队列中第一个 Node，即 Thread-0 所在 Node 执行 transferForSignal 流程，将该 Node 加入 AQS 队列尾部： 注意，要将 Thread-0 的 waitStatus 从-2 改为 0，Thread-3 的waitStatus 改为 -1（阻塞队列最后一个一定是0） Thread-1 释放锁，进入 unlock 流程，略 9.ReentrantReadWriteLock当读操作远远高于写操作时，这时候使用 读写锁 让 读-读 可以并发，提高性能。 类似于数据库中的 select ... from ... lock in share mode 1）示例提供一个 数据容器类 内部分别使用读锁保护数据的 read() 方法，写锁保护数据的 write() 方法 123456789101112131415161718192021222324252627282930313233class DataContainer &#123; private Object data; private ReentrantReadWriteLock rw = new ReentrantReadWriteLock(); private ReentrantReadWriteLock.ReadLock r = rw.readLock(); private ReentrantReadWriteLock.WriteLock w = rw.writeLock(); public Object read() &#123; log.debug(&quot;获取读锁...&quot;); r.lock(); try &#123; log.debug(&quot;读取&quot;); sleep(1); return data; &#125; finally &#123; log.debug(&quot;释放读锁...&quot;); r.unlock(); &#125; &#125; public void write() &#123; log.debug(&quot;获取写锁...&quot;); w.lock(); try &#123; log.debug(&quot;写入&quot;); sleep(1); &#125; finally &#123; log.debug(&quot;释放写锁...&quot;); w.unlock(); &#125; &#125; &#125; 2）读-读 可并发测试 读锁-读锁 可以并发 123456789DataContainer dataContainer = new DataContainer();new Thread(() -&gt; &#123; dataContainer.read();&#125;, &quot;t1&quot;).start();new Thread(() -&gt; &#123; dataContainer.read();&#125;, &quot;t2&quot;).start(); 输出结果，从这里可以看到 Thread-0 锁定期间，Thread-1 的读操作不受影响 12345614:05:14.341 c.DataContainer [t2] - 获取读锁... 14:05:14.341 c.DataContainer [t1] - 获取读锁... 14:05:14.345 c.DataContainer [t1] - 读取14:05:14.345 c.DataContainer [t2] - 读取14:05:15.365 c.DataContainer [t2] - 释放读锁... 14:05:15.386 c.DataContainer [t1] - 释放读锁... 3）读-写 &#x2F; 写-写 互斥测试 读锁-写锁 相互阻塞 12345678910DataContainer dataContainer = new DataContainer();new Thread(() -&gt; &#123; dataContainer.read();&#125;, &quot;t1&quot;).start();Thread.sleep(100);new Thread(() -&gt; &#123; dataContainer.write();&#125;, &quot;t2&quot;).start(); 输出结果 12345614:04:21.838 c.DataContainer [t1] - 获取读锁... 14:04:21.838 c.DataContainer [t2] - 获取写锁... 14:04:21.841 c.DataContainer [t2] - 写入14:04:22.843 c.DataContainer [t2] - 释放写锁... 14:04:22.843 c.DataContainer [t1] - 读取14:04:23.843 c.DataContainer [t1] - 释放读锁... 写锁-写锁 也是相互阻塞的，这里就不测试了 3）锁升降级 读锁不支持条件变量,写锁支持 重入时不支持升级：即持有读锁的情况下去获取写锁，会导致获取写锁永久等待 123456789101112r.lock();try &#123; // ... w.lock(); try &#123; // ... &#125; finally&#123; w.unlock(); &#125;&#125; finally&#123; r.unlock();&#125; 重入时支持降级：即持有写锁的情况下去获取读锁 12345678910111213141516171819202122232425262728293031class CachedData &#123; Object data; // 是否有效，如果失效，需要重新计算 data volatile boolean cacheValid; final ReentrantReadWriteLock rwl = new ReentrantReadWriteLock(); void processCachedData() &#123; rwl.readLock().lock(); if (!cacheValid) &#123; // 获取写锁前必须释放读锁 rwl.readLock().unlock(); rwl.writeLock().lock(); try &#123; // 判断是否有其它线程已经获取了写锁、更新了缓存, 避免重复更新 if (!cacheValid) &#123; data = ... cacheValid = true; &#125; // 降级为读锁, 释放写锁, 这样能够让其它线程读取缓存 rwl.readLock().lock(); &#125; finally &#123; rwl.writeLock().unlock(); &#125; &#125; // 自己用完数据, 释放读锁 try &#123; use(data); &#125; finally &#123; rwl.readLock().unlock(); &#125; &#125;&#125; 4）应用之缓存①缓存更新策略更新时，是先清缓存还是先更新数据库 先清缓存，再更新数据库 先更新数据库，再删缓存 补充一种情况，假设查询线程 A 查询数据时恰好缓存数据由于时间到期失效，或是第一次查询 这种情况的出现几率非常小 ②读写锁实现一致性缓存使用读写锁实现一个简单的按需加载缓存 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778class GenericCachedDao&lt;T&gt; &#123; // HashMap 作为缓存非线程安全, 需要保护 HashMap&lt;SqlPair, T&gt; map = new HashMap&lt;&gt;(); ReentrantReadWriteLock lock = new ReentrantReadWriteLock(); GenericDao genericDao = new GenericDao(); public int update(String sql, Object... params) &#123; SqlPair key = new SqlPair(sql, params); // 加写锁, 防止其它线程对缓存读取和更改 lock.writeLock().lock(); try &#123; int rows = genericDao.update(sql, params); map.clear(); return rows; &#125; finally &#123; lock.writeLock().unlock(); &#125; &#125; public T queryOne(Class&lt;T&gt; beanClass, String sql, Object... params) &#123; SqlPair key = new SqlPair(sql, params); // 加读锁, 防止其它线程对缓存更改，多个线程读不互斥，读-读并发 lock.readLock().lock(); try &#123; T value = map.get(key); if (value != null) &#123; return value; &#125; &#125; finally &#123; lock.readLock().unlock(); &#125; // 加写锁, 防止其它线程对缓存读取和更改 lock.writeLock().lock(); try &#123; // get 方法上面部分是可能多个线程进来的, 可能已经向缓存填充了数据 // 为防止重复查询数据库, 再次验证 T value = map.get(key); if (value == null) &#123; // 如果没有, 查询数据库 value = genericDao.queryOne(beanClass, sql, params); map.put(key, value); &#125; return value; &#125; finally &#123; lock.writeLock().unlock(); &#125; &#125; // 作为 key 保证其是不可变的 class SqlPair &#123; private String sql; private Object[] params; public SqlPair(String sql, Object[] params) &#123; this.sql = sql; this.params = params; &#125; @Override public boolean equals(Object o) &#123; if (this == o) &#123; return true; &#125; if (o == null || getClass() != o.getClass()) &#123; return false; &#125; SqlPair sqlPair = (SqlPair) o; return sql.equals(sqlPair.sql) &amp;&amp; Arrays.equals(params, sqlPair.params); &#125; @Override public int hashCode() &#123; int result = Objects.hash(sql); result = 31 * result + Arrays.hashCode(params); return result; &#125; &#125; &#125; 读锁在多个线程做查询的时候，这时候数据不会被修改 注意 以上实现体现的是读写锁的应用，保证缓存和数据库的一致性，但有下面的问题没有考虑 适合读多写少，如果写操作比较频繁，以上实现性能低 没有考虑缓存容量 没有考虑缓存过期 只适合单机 并发性还是低，目前只会用一把锁 更新方法太过简单粗暴，清空了所有 key（考虑按类型分区或重新设计 key） 乐观锁实现：用 CAS 去更新 10.读写锁原理1）t1 w.lock，t2 r.lock① t1 成功上锁，流程与 ReentrantLock 加锁相比没有特殊之处， 低16位state+1，owner写自己 不同是写锁状态占了 state 的低 16 位，而读锁使用的是 state 的高 16 位 也就是说读锁通过低16位来计数，读锁通过高16位计数 ② t2 执行 r.lock，这时进入读锁的 sync.acquireShared(1) 流程，首先会进入tryAcquireShared 流程。如果有写锁占据，那么 tryAcquireShared 返回 -1 表示失败 tryAcquireShared 返回值表示 -1 表示失败 0 表示成功，但后继节点不会继续唤醒 正数表示成功，而且数值是还有几个后继节点需要唤醒，读写锁返回 1 ③这时会进入 sync.doAcquireShared(1) 流程，首先也是调用 addWaiter 添加节点，不同之处在于节点被设置为Node.SHARED 模式而非 Node.EXCLUSIVE 模式，注意此时 t2 仍处于活跃状态 ④ t2 会看看自己的节点是不是老二，如果是，还会再次调用 tryAcquireShared(1) 来尝试获取锁 ⑤ 如果没有成功，在 doAcquireShared 内 for (;;) 循环一次，把前驱节点的 waitStatus 改为 -1，再 for (;;) 循环一次尝试 tryAcquireShared(1) 如果还不成功，那么在 parkAndCheckInterrupt() 处 park 2）t3 r.lock，t4 w.lock这种状态下，假设又有 t3 加读锁和 t4 加写锁，这期间 t1 仍然持有锁，就变成了下面的样子 3）w.unlock这时会走到写锁的 sync.release(1) 流程，调用 sync.tryRelease(1) 成功，变成下面的样子 state &#x3D; 0 owner&#x3D;&#x3D;null 接下来执行唤醒流程 sync.unparkSuccessor，即让老二恢复运行，这时 t2 在 doAcquireShared 内parkAndCheckInterrupt() 处恢复运行 这回再来一次 for (;;) 执行 tryAcquireShared 成功则让读锁计数加一 事情还没完，在 setHeadAndPropagate 方法内还会检查下一个节点是否是 shared，如果是则调用 这回再来一次 for (;;) 执行 tryAcquireShared 成功则让读锁计数加一 这时 t3 已经恢复运行，接下来 t3 调用 setHeadAndPropagate(node, 1)，它原本所在节点被置为头节点 下一个节点不是 shared 了，因此不会继续唤醒 t4 所在节点 4）t2 r.unlock，t3 r.unlockt2 进入 sync.releaseShared(1) 中，调用 tryReleaseShared(1) 让计数减一，但由于计数还不为零 t3 进入 sync.releaseShared(1) 中，调用 tryReleaseShared(1) 让计数减一，这回计数为零了， 进入 doReleaseShared() 将头节点从 -1 改为 0 并唤醒老二，即 之后 t4 在 acquireQueued 中 parkAndCheckInterrupt 处恢复运行，再次 for (;;) 这次自己是老二，并且没有其他竞争，tryAcquire(1) 成功，修改头结点，流程结束 5）源码①写锁上锁流程12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364static final class NonfairSync extends Sync &#123; // ... 省略无关代码 // 外部类 WriteLock 方法, 方便阅读, 放在此处 public void lock() &#123; sync.acquire(1); &#125; // AQS 继承过来的方法, 方便阅读, 放在此处 public final void acquire(int arg) &#123; if ( // 尝试获得写锁失败 !tryAcquire(arg) &amp;&amp; // 将当前线程关联到一个 Node 对象上, 模式为独占模式 // 进入 AQS 队列阻塞 acquireQueued(addWaiter(Node.EXCLUSIVE), arg) ) &#123; selfInterrupt(); &#125; &#125; // Sync 继承过来的方法, 方便阅读, 放在此处 protected final boolean tryAcquire(int acquires) &#123; // 获得低 16 位, 代表写锁的 state 计数 Thread current = Thread.currentThread(); int c = getState(); int w = exclusiveCount(c); if (c != 0) &#123; if ( // c != 0 and w == 0 表示有读锁, 或者 w == 0 || // 如果 exclusiveOwnerThread 不是自己 current != getExclusiveOwnerThread() ) &#123; // 获得锁失败 return false; &#125; // 写锁计数超过低 16 位, 报异常 if (w + exclusiveCount(acquires) &gt; MAX_COUNT) throw new Error(&quot;Maximum lock count exceeded&quot;); // 写锁重入, 获得锁成功 setState(c + acquires); return true; &#125; if ( // 判断写锁是否该阻塞, 或者 writerShouldBlock() || // 尝试更改计数失败 !compareAndSetState(c, c + acquires) ) &#123; // 获得锁失败 return false; &#125; // 获得锁成功 setExclusiveOwnerThread(current); return true; &#125; // 非公平锁 writerShouldBlock 总是返回 false, 无需阻塞 final boolean writerShouldBlock() &#123; return false; &#125;&#125; ②写锁释放流程1234567891011121314151617181920212223242526272829303132333435static final class NonfairSync extends Sync &#123; // ... 省略无关代码 // WriteLock 方法, 方便阅读, 放在此处 public void unlock() &#123; sync.release(1); &#125; // AQS 继承过来的方法, 方便阅读, 放在此处 public final boolean release(int arg) &#123; // 尝试释放写锁成功 if (tryRelease(arg)) &#123; // unpark AQS 中等待的线程 Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false; &#125; // Sync 继承过来的方法, 方便阅读, 放在此处 protected final boolean tryRelease(int releases) &#123; if (!isHeldExclusively()) throw new IllegalMonitorStateException(); int nextc = getState() - releases; // 因为可重入的原因, 写锁计数为 0, 才算释放成功 boolean free = exclusiveCount(nextc) == 0; if (free) &#123; setExclusiveOwnerThread(null); &#125; setState(nextc); return free; &#125;&#125; ③读锁上锁流程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153static final class NonfairSync extends Sync &#123; // ReadLock 方法, 方便阅读, 放在此处 public void lock() &#123; sync.acquireShared(1); &#125; // AQS 继承过来的方法, 方便阅读, 放在此处 public final void acquireShared(int arg) &#123; // tryAcquireShared 返回负数, 表示获取读锁失败 if (tryAcquireShared(arg) &lt; 0) &#123; doAcquireShared(arg); &#125; &#125; // Sync 继承过来的方法, 方便阅读, 放在此处 protected final int tryAcquireShared(int unused) &#123; Thread current = Thread.currentThread(); int c = getState(); // 如果是其它线程持有写锁, 获取读锁失败 if ( exclusiveCount(c) != 0 &amp;&amp; getExclusiveOwnerThread() != current ) &#123; return -1; &#125; int r = sharedCount(c); if ( // 读锁不该阻塞(如果老二是写锁，读锁该阻塞), 并且 !readerShouldBlock() &amp;&amp; // 小于读锁计数, 并且 r &lt; MAX_COUNT &amp;&amp; // 尝试增加计数成功 compareAndSetState(c, c + SHARED_UNIT) ) &#123; // ... 省略不重要的代码 return 1; &#125; return fullTryAcquireShared(current); &#125; // 非公平锁 readerShouldBlock 看 AQS 队列中第一个节点是否是写锁 // true 则该阻塞, false 则不阻塞 final boolean readerShouldBlock() &#123; return apparentlyFirstQueuedIsExclusive(); &#125; // AQS 继承过来的方法, 方便阅读, 放在此处 // 与 tryAcquireShared 功能类似, 但会不断尝试 for (;;) 获取读锁, 执行过程中无阻塞 final int fullTryAcquireShared(Thread current) &#123; HoldCounter rh = null; for (;;) &#123; int c = getState(); if (exclusiveCount(c) != 0) &#123; if (getExclusiveOwnerThread() != current) return -1; &#125; else if (readerShouldBlock()) &#123; // ... 省略不重要的代码 &#125; if (sharedCount(c) == MAX_COUNT) throw new Error(&quot;Maximum lock count exceeded&quot;); if (compareAndSetState(c, c + SHARED_UNIT)) &#123; // ... 省略不重要的代码 return 1; &#125; &#125; &#125; // AQS 继承过来的方法, 方便阅读, 放在此处 private void doAcquireShared(int arg) &#123; // 将当前线程关联到一个 Node 对象上, 模式为共享模式 final Node node = addWaiter(Node.SHARED); boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); if (p == head) &#123; // 再一次尝试获取读锁 int r = tryAcquireShared(arg); // 成功 if (r &gt;= 0) &#123; // ㈠ // r 表示可用资源数, 在这里总是 1 允许传播 //（唤醒 AQS 中下一个 Share 节点） setHeadAndPropagate(node, r); p.next = null; // help GC if (interrupted) selfInterrupt(); failed = false; return; &#125; &#125; if ( // 是否在获取读锁失败时阻塞（前一个阶段 waitStatus == Node.SIGNAL） shouldParkAfterFailedAcquire(p, node) &amp;&amp; // park 当前线程 parkAndCheckInterrupt() ) &#123; interrupted = true; &#125; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125; &#125; // ㈠ AQS 继承过来的方法, 方便阅读, 放在此处 private void setHeadAndPropagate(Node node, int propagate) &#123; Node h = head; // Record old head for check below // 设置自己为 head setHead(node); // propagate 表示有共享资源（例如共享读锁或信号量） // 原 head waitStatus == Node.SIGNAL 或 Node.PROPAGATE // 现在 head waitStatus == Node.SIGNAL 或 Node.PROPAGATE if (propagate &gt; 0 || h == null || h.waitStatus &lt; 0 || (h = head) == null || h.waitStatus &lt; 0) &#123; Node s = node.next; // 如果是最后一个节点或者是等待共享读锁的节点 if (s == null || s.isShared()) &#123; // 进入 ㈡ doReleaseShared(); &#125; &#125; &#125; // ㈡ AQS 继承过来的方法, 方便阅读, 放在此处 private void doReleaseShared() &#123; // 如果 head.waitStatus == Node.SIGNAL ==&gt; 0 成功, 下一个节点 unpark // 如果 head.waitStatus == 0 ==&gt; Node.PROPAGATE, 为了解决 bug, 见后面分析 for (;;) &#123; Node h = head; // 队列还有节点 if (h != null &amp;&amp; h != tail) &#123; int ws = h.waitStatus; if (ws == Node.SIGNAL) &#123; if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases // 下一个节点 unpark 如果成功获取读锁 // 并且下下个节点还是 shared, 继续 doReleaseShared unparkSuccessor(h); &#125; else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS &#125; if (h == head) // loop if head changed break; &#125; &#125;&#125; ④读锁释放流程12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455static final class NonfairSync extends Sync &#123; // ReadLock 方法, 方便阅读, 放在此处 public void unlock() &#123; sync.releaseShared(1); &#125; // AQS 继承过来的方法, 方便阅读, 放在此处 public final boolean releaseShared(int arg) &#123; if (tryReleaseShared(arg)) &#123; doReleaseShared(); return true; &#125; return false; &#125; // Sync 继承过来的方法, 方便阅读, 放在此处 protected final boolean tryReleaseShared(int unused) &#123; // ... 省略不重要的代码 for (;;) &#123; int c = getState(); int nextc = c - SHARED_UNIT; if (compareAndSetState(c, nextc)) &#123; // 读锁的计数不会影响其它获取读锁线程, 但会影响其它获取写锁线程 // 计数为 0 才是真正释放 return nextc == 0; &#125; &#125; &#125; // AQS 继承过来的方法, 方便阅读, 放在此处 private void doReleaseShared() &#123; // 如果 head.waitStatus == Node.SIGNAL ==&gt; 0 成功, 下一个节点 unpark // 如果 head.waitStatus == 0 ==&gt; Node.PROPAGATE for (;;) &#123; Node h = head; if (h != null &amp;&amp; h != tail) &#123; int ws = h.waitStatus; // 如果有其它线程也在释放读锁，那么需要将 waitStatus 先改为 0 // 防止 unparkSuccessor 被多次执行 if (ws == Node.SIGNAL) &#123; if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases unparkSuccessor(h); &#125; // 如果已经是 0 了，改为 -3，用来解决传播性，见后文信号量 bug 分析 else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS &#125; if (h == head) // loop if head changed break; &#125; &#125; &#125; 11.StampedLock1）乐观读因为读线程在并发读的时候还是会先去获得readLock，有性能损耗，所以有了StampedLock作乐观读 该类自 JDK 8 加入，是为了进一步优化读性能，它的特点是在使用读锁、写锁时都必须配合【戳】使用加解读锁 读的时候首先获得stamp，如果stamp没有改变 获得戳： 12long stamp = lock.readLock();lock.unlockRead(stamp); 加解写锁： 12long stamp = lock.writeLock();lock.unlockWrite(stamp); 2）乐观读-锁升级乐观读，StampedLock 支持 tryOptimisticRead() 方法（乐观读），读取完毕后需要做一次 戳校验 如果校验通过，表示这期间确实没有写操作，数据可以安全使用，如果校验没通过，需要重新获取读锁，保证数据安全。 12345long stamp = lock.tryOptimisticRead();// 验戳if(!lock.validate(stamp))&#123; // 锁升级&#125; 3）使用提供一个 数据容器类 内部分别使用读锁保护数据的 read() 方法，写锁保护数据的 write() 方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445class DataContainerStamped &#123; private int data; private final StampedLock lock = new StampedLock(); public DataContainerStamped(int data) &#123; this.data = data; &#125; public int read(int readTime) &#123; long stamp = lock.tryOptimisticRead(); log.debug(&quot;optimistic read locking...&#123;&#125;&quot;, stamp); sleep(readTime); if (lock.validate(stamp)) &#123; //验证stamp，就不需要加锁 log.debug(&quot;read finish...&#123;&#125;, data:&#123;&#125;&quot;, stamp, data); return data; &#125; // 锁升级 - 读锁 log.debug(&quot;updating to read lock... &#123;&#125;&quot;, stamp); try &#123; stamp = lock.readLock(); log.debug(&quot;read lock &#123;&#125;&quot;, stamp); sleep(readTime); log.debug(&quot;read finish...&#123;&#125;, data:&#123;&#125;&quot;, stamp, data); return data; &#125; finally &#123; log.debug(&quot;read unlock &#123;&#125;&quot;, stamp); lock.unlockRead(stamp); &#125; &#125; public void write(int newData) &#123; long stamp = lock.writeLock(); log.debug(&quot;write lock &#123;&#125;&quot;, stamp); try &#123; sleep(2); this.data = newData; &#125; finally &#123; log.debug(&quot;write unlock &#123;&#125;&quot;, stamp); lock.unlockWrite(stamp); &#125; &#125; &#125; 测试 读-读 可以优化 1234567891011public static void main(String[] args) &#123; DataContainerStamped dataContainer = new DataContainerStamped(1); new Thread(() -&gt; &#123; dataContainer.read(1); &#125;, &quot;t1&quot;).start(); sleep(0.5); new Thread(() -&gt; &#123; dataContainer.read(0); &#125;, &quot;t2&quot;).start();&#125; 输出结果，可以看到实际没有加读锁 123415:58:50.217 c.DataContainerStamped [t1] - optimistic read locking...256 //两个线程获得的stamp是一样的15:58:50.717 c.DataContainerStamped [t2] - optimistic read locking...256 //中间没有修改，stamp验证通过15:58:50.717 c.DataContainerStamped [t2] - read finish...256, data:1 15:58:51.220 c.DataContainerStamped [t1] - read finish...256, data:1 测试 读-写 时优化读补加读锁 123456789101112public static void main(String[] args) &#123; DataContainerStamped dataContainer = new DataContainerStamped(1); new Thread(() -&gt; &#123; dataContainer.read(1); &#125;, &quot;t1&quot;).start(); sleep(0.5); new Thread(() -&gt; &#123; dataContainer.write(100); &#125;, &quot;t2&quot;).start();&#125; 输出结果 123456715:57:00.219 c.DataContainerStamped [t1] - optimistic read locking...256 15:57:00.717 c.DataContainerStamped [t2] - write lock 384 //加写锁的时候，stamp就变了15:57:01.225 c.DataContainerStamped [t1] - updating to read lock... 256 //验stamp发现已经改变15:57:02.719 c.DataContainerStamped [t2] - write unlock 384 //解读锁15:57:02.719 c.DataContainerStamped [t1] - read lock 513 //获得读锁的时候，stamp也变了15:57:03.719 c.DataContainerStamped [t1] - read finish...513, data:1000 15:57:03.719 c.DataContainerStamped [t1] - read unlock 513 注意 StampedLock 不支持条件变量 StampedLock 不支持锁重入 12.Semaphore1）基本使用 信号量，用来限制能同时访问共享资源的线程上限 semaphore限制的是同时访问的线程上限，并不是资源上限 适用于线程数和资源数相等的情况，如数据库连接池：连接数&#x3D;线程数 1234567891011121314151617181920212223public static void main(String[] args) &#123; // 1. 创建 semaphore 对象 Semaphore semaphore = new Semaphore(3); // 2. 10个线程同时运行 for (int i = 0; i &lt; 10; i++) &#123; new Thread(() -&gt; &#123; // 3. 获取许可 try &#123; semaphore.acquire(); //没有获得许可会被阻塞 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; try &#123; log.debug(&quot;running...&quot;); sleep(1); log.debug(&quot;end...&quot;); &#125; finally &#123; // 4. 释放许可 semaphore.release(); &#125; &#125;).start(); &#125; &#125; 输出： 123456789101112131415161718192007:35:15.485 c.TestSemaphore [Thread-2] - running... 07:35:15.485 c.TestSemaphore [Thread-1] - running... 07:35:15.485 c.TestSemaphore [Thread-0] - running... 07:35:16.490 c.TestSemaphore [Thread-2] - end... 07:35:16.490 c.TestSemaphore [Thread-0] - end... 07:35:16.490 c.TestSemaphore [Thread-1] - end... 07:35:16.490 c.TestSemaphore [Thread-3] - running... 07:35:16.490 c.TestSemaphore [Thread-5] - running... 07:35:16.490 c.TestSemaphore [Thread-4] - running... 07:35:17.490 c.TestSemaphore [Thread-5] - end... 07:35:17.490 c.TestSemaphore [Thread-4] - end... 07:35:17.490 c.TestSemaphore [Thread-3] - end... 07:35:17.490 c.TestSemaphore [Thread-6] - running... 07:35:17.490 c.TestSemaphore [Thread-7] - running... 07:35:17.490 c.TestSemaphore [Thread-9] - running... 07:35:18.491 c.TestSemaphore [Thread-6] - end... 07:35:18.491 c.TestSemaphore [Thread-7] - end... 07:35:18.491 c.TestSemaphore [Thread-9] - end... 07:35:18.491 c.TestSemaphore [Thread-8] - running... 07:35:19.492 c.TestSemaphore [Thread-8] - end... 2）应用-实现简单连接池 使用 Semaphore 限流，在访问高峰期时，让请求线程阻塞，高峰期过去再释放许可，当然它只适合限制单机线程数量，并且仅是限制线程数，而不是限制资源数（例如连接数，请对比 Tomcat LimitLatch 的实现） 用 Semaphore 实现简单连接池，对比『享元模式』下的实现（用wait notify），性能和可读性显然更好,注意下面的实现中线程数和数据库连接数是相等的 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253@Slf4j(topic = &quot;c.Pool&quot;)class Pool &#123; // 1. 连接池大小 private final int poolSize; // 2. 连接对象数组 private Connection[] connections; // 3. 连接状态数组 0 表示空闲， 1 表示繁忙 private AtomicIntegerArray states; private Semaphore semaphore; // 4. 构造方法初始化 public Pool(int poolSize) &#123; this.poolSize = poolSize; // 让许可数与资源数一致 this.semaphore = new Semaphore(poolSize); this.connections = new Connection[poolSize]; this.states = new AtomicIntegerArray(new int[poolSize]); for (int i = 0; i &lt; poolSize; i++) &#123; connections[i] = new MockConnection(&quot;连接&quot; + (i+1)); &#125; &#125; // 5. 借连接 public Connection borrow() &#123;// t1, t2, t3 // 获取许可 try &#123; semaphore.acquire(); // 没有许可的线程，在此等待 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; for (int i = 0; i &lt; poolSize; i++) &#123; // 获取空闲连接 if(states.get(i) == 0) &#123; if (states.compareAndSet(i, 0, 1)) &#123; log.debug(&quot;borrow &#123;&#125;&quot;, connections[i]); return connections[i]; &#125; &#125; &#125; // 不会执行到这里 return null; &#125; // 6. 归还连接 public void free(Connection conn) &#123; for (int i = 0; i &lt; poolSize; i++) &#123; if (connections[i] == conn) &#123; states.set(i, 0); log.debug(&quot;free &#123;&#125;&quot;, conn); semaphore.release(); break; &#125; &#125; &#125; &#125; 13.Semaphore 原理1）加锁解锁流程Semaphore 有点像一个停车场，permits 就好像停车位数量，当线程获得了 permits 就像是获得了停车位，然后 停车场显示空余车位减1 刚开始，permits（state）为 3，这时 5 个线程来获取资源 假设其中 Thread-1，Thread-2，Thread-4 cas 竞争成功，而 Thread-0 和 Thread-3 竞争失败，进入 AQS 队列park 阻塞 这时 Thread-4 释放了 permits，状态如下 接下来 Thread-0 竞争成功，permits 再次设置为 0，设置自己为 head 节点，断开原来的 head 节点，unpark 接下来的 Thread-3 节点，但由于 permits 是 0，因此 Thread-3 在尝试不成功后再次进入 park 状态 semaphore也是连续释放，释放到permit&#x3D;0耗尽为止 2）源码分析123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899static final class NonfairSync extends Sync &#123; private static final long serialVersionUID = -2694183684443567898L; NonfairSync(int permits) &#123; // permits 即 state super(permits); &#125; // Semaphore 方法, 方便阅读, 放在此处 public void acquire() throws InterruptedException &#123; sync.acquireSharedInterruptibly(1); &#125; // AQS 继承过来的方法, 方便阅读, 放在此处 public final void acquireSharedInterruptibly(int arg) throws InterruptedException &#123; if (Thread.interrupted()) throw new InterruptedException(); if (tryAcquireShared(arg) &lt; 0) doAcquireSharedInterruptibly(arg); &#125; // 尝试获得共享锁 protected int tryAcquireShared(int acquires) &#123; return nonfairTryAcquireShared(acquires); &#125; // Sync 继承过来的方法, 方便阅读, 放在此处 final int nonfairTryAcquireShared(int acquires) &#123; for (;;) &#123; int available = getState(); int remaining = available - acquires; if ( // 如果许可已经用完, 返回负数, 表示获取失败, 进入 doAcquireSharedInterruptibly remaining &lt; 0 || // 如果 cas 重试成功, 返回正数, 表示获取成功 compareAndSetState(available, remaining) ) &#123; return remaining; &#125; &#125; &#125; // AQS 继承过来的方法, 方便阅读, 放在此处 private void doAcquireSharedInterruptibly(int arg) throws InterruptedException &#123; final Node node = addWaiter(Node.SHARED); boolean failed = true; try &#123; for (;;) &#123; final Node p = node.predecessor(); if (p == head) &#123; // 再次尝试获取许可 int r = tryAcquireShared(arg); if (r &gt;= 0) &#123; // 成功后本线程出队（AQS）, 所在 Node设置为 head // 如果 head.waitStatus == Node.SIGNAL ==&gt; 0 成功, 下一个节点 unpark // 如果 head.waitStatus == 0 ==&gt; Node.PROPAGATE // r 表示可用资源数, 为 0 则不会继续传播 setHeadAndPropagate(node, r); p.next = null; // help GC failed = false; return; &#125; &#125; // 不成功, 设置上一个节点 waitStatus = Node.SIGNAL, 下轮进入 park 阻塞 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) throw new InterruptedException(); &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125; &#125; // Semaphore 方法, 方便阅读, 放在此处 public void release() &#123; sync.releaseShared(1); &#125; // AQS 继承过来的方法, 方便阅读, 放在此处 public final boolean releaseShared(int arg) &#123; if (tryReleaseShared(arg)) &#123; doReleaseShared(); return true; &#125; return false; &#125; // Sync 继承过来的方法, 方便阅读, 放在此处 protected final boolean tryReleaseShared(int releases) &#123; for (;;) &#123; int current = getState(); int next = current + releases; if (next &lt; current) // overflow throw new Error(&quot;Maximum permit count exceeded&quot;); if (compareAndSetState(current, next)) return true; &#125; &#125;&#125; 14.CountdownLatch用来进行线程同步协作，等待所有线程完成倒计时。 其中构造参数用来初始化等待计数值，await() 用来等待计数归零，countDown() 用来让计数减一 1）使用12345678910111213141516171819202122232425262728public static void main(String[] args) throws InterruptedException &#123; CountDownLatch latch = new CountDownLatch(3); new Thread(() -&gt; &#123; log.debug(&quot;begin...&quot;); sleep(1); latch.countDown(); log.debug(&quot;end...&#123;&#125;&quot;, latch.getCount()); &#125;).start(); new Thread(() -&gt; &#123; log.debug(&quot;begin...&quot;); sleep(2); latch.countDown(); log.debug(&quot;end...&#123;&#125;&quot;, latch.getCount()); &#125;).start(); new Thread(() -&gt; &#123; log.debug(&quot;begin...&quot;); sleep(1.5); latch.countDown(); log.debug(&quot;end...&#123;&#125;&quot;, latch.getCount()); &#125;).start(); log.debug(&quot;waiting...&quot;); latch.await();//主线程等待信号量归0 log.debug(&quot;wait end...&quot;);&#125; 输出 1234567818:44:00.778 c.TestCountDownLatch [main] - waiting... 18:44:00.778 c.TestCountDownLatch [Thread-2] - begin... 18:44:00.778 c.TestCountDownLatch [Thread-0] - begin... 18:44:00.778 c.TestCountDownLatch [Thread-1] - begin... 18:44:01.782 c.TestCountDownLatch [Thread-0] - end...2 18:44:02.283 c.TestCountDownLatch [Thread-2] - end...1 18:44:02.782 c.TestCountDownLatch [Thread-1] - end...0 18:44:02.782 c.TestCountDownLatch [main] - wait end... 可以配合线程池使用，改进如下 123456789101112131415161718192021222324252627282930313233343536public static void main(String[] args) throws InterruptedException &#123; CountDownLatch latch = new CountDownLatch(3); ExecutorService service = Executors.newFixedThreadPool(4); service.submit(() -&gt; &#123; log.debug(&quot;begin...&quot;); sleep(1); latch.countDown();// 计数-1 log.debug(&quot;end...&#123;&#125;&quot;, latch.getCount()); &#125;); service.submit(() -&gt; &#123; log.debug(&quot;begin...&quot;); sleep(1.5); latch.countDown(); log.debug(&quot;end...&#123;&#125;&quot;, latch.getCount()); &#125;); service.submit(() -&gt; &#123; log.debug(&quot;begin...&quot;); sleep(2); latch.countDown(); log.debug(&quot;end...&#123;&#125;&quot;, latch.getCount()); &#125;); service.submit(()-&gt;&#123; try &#123; log.debug(&quot;waiting...&quot;); latch.await();//主线程等待信号量归0 log.debug(&quot;wait end...&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; 输出 1234567818:52:25.831 c.TestCountDownLatch [pool-1-thread-3] - begin... 18:52:25.831 c.TestCountDownLatch [pool-1-thread-1] - begin... 18:52:25.831 c.TestCountDownLatch [pool-1-thread-2] - begin... 18:52:25.831 c.TestCountDownLatch [pool-1-thread-4] - waiting... 18:52:26.835 c.TestCountDownLatch [pool-1-thread-1] - end...2 18:52:27.335 c.TestCountDownLatch [pool-1-thread-2] - end...1 18:52:27.835 c.TestCountDownLatch [pool-1-thread-3] - end...0 18:52:27.835 c.TestCountDownLatch [pool-1-thread-4] - wait end... 2）应用-游戏准备阶段1234567891011121314151617181920212223242526272829AtomicInteger num = new AtomicInteger(0);ExecutorService service = Executors.newFixedThreadPool(10, (r) -&gt; &#123; return new Thread(r, &quot;t&quot; + num.getAndIncrement());&#125;);CountDownLatch latch = new CountDownLatch(10);String[] all = new String[10];Random r = new Random();for (int j = 0; j &lt; 10; j++) &#123; int x = j;//将j重新赋值，可以在lamda表达式中使用 service.submit(() -&gt; &#123; for (int i = 0; i &lt;= 100; i++) &#123; try &#123; Thread.sleep(r.nextInt(100)); &#125; catch (InterruptedException e) &#123; &#125; //注意这里不能加j，j是变量，需要重新赋值 all[x] = Thread.currentThread().getName() + &quot;(&quot; + (i + &quot;%&quot;) + &quot;)&quot;; System.out.print(&quot;\\r&quot; + Arrays.toString(all));//打印技巧，不换行，加上&quot;\\r&quot; &#125; latch.countDown(); &#125;);&#125;latch.await(); //主线程等待System.out.println(&quot;\\n游戏开始...&quot;);service.shutdown(); 这里不需要返回值，只需要知道线程执行完没有，可以使用CountDownLatch 中间输出 1[t0(52%), t1(47%), t2(51%), t3(40%), t4(49%), t5(44%), t6(49%), t7(52%), t8(46%), t9(46%)] 最后输出 12[t0(100%), t1(100%), t2(100%), t3(100%), t4(100%), t5(100%), t6(100%), t7(100%), t8(100%), t9(100%)] 游戏开始... 3）应用-等待多个远程调用结束12345678910111213141516171819202122232425262728293031323334353637383940414243@RestControllerpublic class TestCountDownlatchController &#123; @GetMapping(&quot;/order/&#123;id&#125;&quot;) public Map&lt;String, Object&gt; order(@PathVariable int id) &#123; HashMap&lt;String, Object&gt; map = new HashMap&lt;&gt;(); map.put(&quot;id&quot;, id); map.put(&quot;total&quot;, &quot;2300.00&quot;); sleep(2000); return map; &#125; @GetMapping(&quot;/product/&#123;id&#125;&quot;) public Map&lt;String, Object&gt; product(@PathVariable int id) &#123; HashMap&lt;String, Object&gt; map = new HashMap&lt;&gt;(); if (id == 1) &#123; map.put(&quot;name&quot;, &quot;小爱音箱&quot;); map.put(&quot;price&quot;, 300); &#125; else if (id == 2) &#123; map.put(&quot;name&quot;, &quot;小米手机&quot;); map.put(&quot;price&quot;, 2000); &#125; map.put(&quot;id&quot;, id); sleep(1000); return map; &#125; @GetMapping(&quot;/logistics/&#123;id&#125;&quot;) public Map&lt;String, Object&gt; logistics(@PathVariable int id) &#123; HashMap&lt;String, Object&gt; map = new HashMap&lt;&gt;(); map.put(&quot;id&quot;, id); map.put(&quot;name&quot;, &quot;中通快递&quot;); sleep(2500); return map; &#125; private void sleep(int millis) &#123; try &#123; Thread.sleep(millis); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; rest 远程调用 123456789101112131415161718192021222324252627282930RestTemplate restTemplate = new RestTemplate();log.debug(&quot;begin&quot;);ExecutorService service = Executors.newCachedThreadPool();CountDownLatch latch = new CountDownLatch(4);Future&lt;Map&lt;String,Object&gt;&gt; f1 = service.submit(() -&gt; &#123; Map&lt;String, Object&gt; r = restTemplate.getForObject(&quot;http://localhost:8080/order/&#123;1&#125;&quot;, Map.class, 1); return r;&#125;);Future&lt;Map&lt;String, Object&gt;&gt; f2 = service.submit(() -&gt; &#123; Map&lt;String, Object&gt; r = restTemplate.getForObject(&quot;http://localhost:8080/product/&#123;1&#125;&quot;, Map.class, 1); return r;&#125;);Future&lt;Map&lt;String, Object&gt;&gt; f3 = service.submit(() -&gt; &#123; Map&lt;String, Object&gt; r = restTemplate.getForObject(&quot;http://localhost:8080/product/&#123;1&#125;&quot;, Map.class, 2); return r;&#125;);Future&lt;Map&lt;String, Object&gt;&gt; f4 = service.submit(() -&gt; &#123; Map&lt;String, Object&gt; r = restTemplate.getForObject(&quot;http://localhost:8080/logistics/&#123;1&#125;&quot;, Map.class, 1); return r;&#125;);System.out.println(f1.get());System.out.println(f2.get());System.out.println(f3.get());System.out.println(f4.get());log.debug(&quot;执行完毕&quot;);service.shutdown(); 使用线程池，多个线程分别调用服务，提高效率 这里是通过future的方式等待的，这里是需要返回值，所以future更合适 也可以使用latch.countDown();和latch.await(); 执行结果： 12345619:51:39.711 c.TestCountDownLatch [main] - begin &#123;total=2300.00, id=1&#125; &#123;price=300, name=小爱音箱, id=1&#125; &#123;price=2000, name=小米手机, id=2&#125; &#123;name=中通快递, id=1&#125; 19:51:42.407 c.TestCountDownLatch [main] - 执行完毕 15.CyclicBarrier循环栅栏，用来进行线程协作，等待线程满足某个计数。构造时设置『计数个数』，每个线程执行到某个需要“同步”的时刻调用 await() 方法进行等待，当等待的线程数满足『计数个数』时，继续执行 123456789101112131415161718192021222324252627public static void main(String[] args) &#123; CyclicBarrier cb = new CyclicBarrier(2); // 个数为2时才会继续执行 for(int i=0;i&lt;2;i++)&#123;//第一次循环cb为0了以后，第二次cb又变成了2，如果是CountDonwLatch第二次就是0了 new Thread(()-&gt;&#123; System.out.println(&quot;线程1开始..&quot;+new Date()); try &#123; cb.await(); // 当个数不足时，等待 &#125; catch (InterruptedException | BrokenBarrierException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;线程1继续向下运行...&quot;+new Date()); &#125;).start(); new Thread(()-&gt;&#123; System.out.println(&quot;线程2开始..&quot;+new Date()); try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; &#125; try &#123; cb.await(); // 2 秒后，线程个数够2，继续运行 &#125; catch (InterruptedException | BrokenBarrierException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;线程2继续向下运行...&quot;+new Date()); &#125;).start(); &#125; CyclicBarrier 的数量需要和线程数量一致，否则最先消耗的是循环1的t1，和循环2的t1 CyclicBarrier 与 CountDownLatch 的主要区别在于 CyclicBarrier 是可以重用的 CyclicBarrier 可以被比喻为『人满发车』 七、线程安全集合类概述 线程安全集合类可以分为三大类： 1.遗留的线程安全集合 遗留的线程安全集合如 Hashtable ， Vector 这些都是较老的实现，所以已经不推荐使用了 底层实际就是在每个方法上加synchronized，加锁的粒度太大，并发性低 2.使用 Collections 装饰的线程安全集合 使用 Collections 装饰的线程安全集合，如： Collections.synchronizedCollection Collections.synchronizedList Collections.synchronizedMap Collections.synchronizedSet Collections.synchronizedNavigableMap Collections.synchronizedNavigableSet Collections.synchronizedSortedMap Collections.synchronizedSortedSet 12345678public static &lt;T&gt; Collection&lt;T&gt; synchronizedCollection(Collection&lt;T&gt; c) &#123; return new SynchronizedCollection&lt;&gt;(c);&#125;//以下是部分方法public int size() &#123;synchronized (mutex) &#123;return c.size();&#125;&#125;public boolean isEmpty() &#123;synchronized (mutex) &#123;return c.isEmpty();&#125;&#125;public boolean contains(Object o) &#123;synchronized (mutex) &#123;return c.contains(o);&#125;&#125; 可以看到SynchronizedCollection其实就是对原来的集合做了synchronized的封装，内部还是调用了集合本身的方法，本质上实现和Hashtable给每个方法加synchronized的做法是一致的 3.JUC下的安全集合Blocking、CopyOnWrite、Concurrent 重点介绍 java.util.concurrent.* 下的线程安全集合类，可以发现它们有规律，里面包含三类关键词： Blocking、CopyOnWrite、Concurrent Blocking 大部分实现基于锁，并提供用来阻塞的方法 CopyOnWrite 之类容器修改开销相对较重 Concurrent 类型的容器 内部很多操作使用 cas 优化，一般可以提供较高吞吐量 弱一致性 遍历时弱一致性，例如，当利用迭代器遍历时，如果容器发生修改，迭代器仍然可以继续进行遍历，这时内容是旧的 求大小弱一致性，size 操作未必是 100% 准确 读取弱一致性 遍历时如果发生了修改，对于非安全容器来讲，使用 fail-fast 机制也就是让遍历立刻失败，抛出ConcurrentModifificationException，不再继续遍历 4.ConcurrentHashMap1）基本使用eg：单词计数 生成测试数据 12345678910111213141516171819202122232425262728static final String ALPHA = &quot;abcedfghijklmnopqrstuvwxyz&quot;;public static void main(String[] args) &#123; int length = ALPHA.length(); int count = 200; List&lt;String&gt; list = new ArrayList&lt;&gt;(length * count); for (int i = 0; i &lt; length; i++) &#123; char ch = ALPHA.charAt(i); for (int j = 0; j &lt; count; j++) &#123; list.add(String.valueOf(ch)); &#125; &#125; Collections.shuffle(list); for (int i = 0; i &lt; 26; i++) &#123; try (PrintWriter out = new PrintWriter( new OutputStreamWriter( new FileOutputStream(&quot;tmp/&quot; + (i+1) + &quot;.txt&quot;)))) &#123; String collect = list.subList(i * count, (i + 1) * count).stream() .collect(Collectors.joining(&quot;\\n&quot;)); out.print(collect); &#125; catch (IOException e) &#123; &#125; &#125; &#125; 模版代码，模版代码中封装了多线程读取文件的代码 123456789101112131415161718192021222324252627282930313233343536373839404142private static &lt;V&gt; void demo(Supplier&lt;Map&lt;String,V&gt;&gt; supplier, BiConsumer&lt;Map&lt;String,V&gt;,List&lt;String&gt;&gt; consumer) &#123; Map&lt;String, V&gt; counterMap = supplier.get(); List&lt;Thread&gt; ts = new ArrayList&lt;&gt;(); for (int i = 1; i &lt;= 26; i++) &#123; int idx = i; Thread thread = new Thread(() -&gt; &#123; List&lt;String&gt; words = readFromFile(idx);//读取文件中的每一行到char consumer.accept(counterMap, words); &#125;); ts.add(thread); &#125; ts.forEach(t-&gt;t.start());//开启每个线程 ts.forEach(t-&gt; &#123; try &#123; t.join();//主线程等待每个线程执行完 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); System.out.println(counterMap);&#125;public static List&lt;String&gt; readFromFile(int i) &#123; ArrayList&lt;String&gt; words = new ArrayList&lt;&gt;(); try (BufferedReader in = new BufferedReader(new InputStreamReader( new FileInputStream(&quot;tmp/&quot;+ i +&quot;.txt&quot;)))) &#123; while(true) &#123; String word = in.readLine(); if(word == null) &#123; break; &#125; words.add(word); &#125; return words; &#125; catch (IOException e) &#123; throw new RuntimeException(e); &#125;&#125; 你要做的是实现两个参数 一是提供一个 map 集合，用来存放每个单词的计数结果，key 为单词，value 为计数 二是提供一组操作，保证计数的安全性，会传递 map 集合以及 单词 List 正确结果输出应该是每个单词出现 200 次 1&#123;a=200, b=200, c=200, d=200, e=200, f=200, g=200, h=200, i=200, j=200, k=200, l=200, m=200, n=200, o=200, p=200, q=200, r=200, s=200, t=200, u=200, v=200, w=200, x=200, y=200, z=200&#125; ①HashMap的问题1234567891011121314demo( // 创建 map 集合 // 创建 ConcurrentHashMap 对不对？ () -&gt; new HashMap&lt;String, Integer&gt;(), // 进行计数 (map, words) -&gt; &#123; for (String word : words) &#123; //这里的getter和setter无法保证原子性 Integer counter = map.get(word); int newValue = counter == null ? 1 : counter + 1; map.put(word, newValue); &#125; &#125;); 首先HashMap就不能保证自己的方法是原子性的 并且foreach的操作也不是原子性的 ②ConcurrentHasHMap的实现12345678910demo( () -&gt; new ConcurrentHashMap&lt;String, LongAdder&gt;(), (map, words) -&gt; &#123; for (String word : words) &#123; // 注意不能使用 putIfAbsent，此方法返回的是上一次的 value，首次调用返回 null LongAdder ladder =map.computeIfAbsent(word, (key) -&gt; new LongAdder()); ladder.increment(); &#125; &#125;); ConcurrentHashMap保证了自己的方法都是原子性的：map.computeIfAbsent 该方法在第一次的时候会new LongAdder()，并且返回一个LongAdder 后续在返回就是之前的value了，也是同一个LongAdder ladder.increment();的线程安全是本身保证的 2）JDK 7 HashMap 并发死链①复现流程调试工具使用 idea 在 HashMap 源码 590 行加断点 1int newCapacity = newTable.length; 断点的条件如下，目的是让 HashMap 在扩容为 32 时，并且线程为 Thread-0 或 Thread-1 时停下来 12345newTable.length==32 &amp;&amp; ( Thread.currentThread().getName().equals(&quot;Thread-0&quot;)|| Thread.currentThread().getName().equals(&quot;Thread-1&quot;) ) 断点暂停方式选择 Thread，否则在调试 Thread-0 时，Thread-1 无法恢复运行 运行代码，程序在预料的断点位置停了下来，输出 123456789长度为16时，桶下标为1的key 1 16 35 50 长度为32时，桶下标为1的key 1 35 扩容前大小[main]:12 接下来进入扩容流程调试 在 HashMap 源码 594 行加断点 123Entry&lt;K,V&gt; next = e.next; // 593if (rehash) // 594// ... 这是为了观察 e 节点和 next 节点的状态，Thread-0 单步执行到 594 行，再 594 处再添加一个断点（条件Thread.currentThread().getName().equals(“Thread-0”)） 这时可以在 Variables 面板观察到 e 和 next 变量，使用 view as -&gt; Object 查看节点状态 12e (1)-&gt;(35)-&gt;(16)-&gt;null next (35)-&gt;(16)-&gt;null 在 Threads 面板选中 Thread-1 恢复运行，可以看到控制台输出新的内容如下，Thread-1 扩容已完成 12newTable[1] (35)-&gt;(1)-&gt;null 扩容后大小:13 这时 Thread-0 还停在 594 处， Variables 面板变量的状态已经变化为 12e (1)-&gt;null next (35)-&gt;(1)-&gt;null 为什么呢，因为 Thread-1 扩容时链表也是后加入的元素放入链表头，因此链表就倒过来了，但 Thread-1 虽然结果正确，但它结束后 Thread-0 还要继续运行 接下来就可以单步调试（F8）观察死链的产生了 下一轮循环到 594，将 e 搬迁到 newTable 链表头 123newTable[1] (1)-&gt;null e (35)-&gt;(1)-&gt;null next (1)-&gt;null 下一轮循环到 594，将 e 搬迁到 newTable 链表头 123newTable[1] (35)-&gt;(1)-&gt;null e (1)-&gt;null next null 再看看源码 123456789e.next = newTable[1];// 这时 e (1,35)// 而 newTable[1] (35,1)-&gt;(1,35) 因为是同一个对象newTable[1] = e; // 再尝试将 e 作为链表头, 死链已成e = next;// 虽然 next 是 null, 会进入下一个链表的复制, 但死链已经形成了 ②源码分析HashMap 的并发死链发生在扩容时 12345678910111213141516171819// 将 table 迁移至 newTablevoid transfer(Entry[] newTable, boolean rehash) &#123; int newCapacity = newTable.length; for (Entry&lt;K,V&gt; e : table) &#123; while(null != e) &#123; Entry&lt;K,V&gt; next = e.next; // 1 处 if (rehash) &#123; e.hash = null == e.key ? 0 : hash(e.key); &#125; int i = indexFor(e.hash, newCapacity); // 2 处 // 将新元素加入 newTable[i], 原 newTable[i] 作为新元素的 next e.next = newTable[i]; newTable[i] = e; e = next; &#125; &#125;&#125; 假设 map 中初始元素是 12345678910111213141516171819202122232425262728原始链表，格式：[下标] (key,next) [1] (1,35)-&gt;(35,16)-&gt;(16,null)线程 a 执行到 1 处 ，此时局部变量 e 为 (1,35)，而局部变量 next 为 (35,16) 线程 a 挂起线程 b 开始执行第一次循环[1] (1,null)第二次循环[1] (35,1)-&gt;(1,null)第三次循环[1] (35,1)-&gt;(1,null) [17] (16,null)切换回线程 a，此时局部变量 e 和 next 被恢复，引用没变但内容变了：e 的内容被改为 (1,null)，而 next 的内容被改为 (35,1) -&gt; (1,null)第一次循环[1] (1,null)第二次循环，注意这时 e 是 (35,1) -&gt; (1,null) 所以 next 又是 (1,null) [1] (35,1)-&gt;(1,null)第三次循环，e 是 (1,null)，而 next 是 null，但 e 被放入链表头，这样 e.next 变成了 35 （2 处）[1] (1,35)-&gt;(35,1)-&gt;(1,35)已经是死链了 小结 究其原因，是因为在多线程环境下使用了非线程安全的 map 集合 JDK 8 虽然将扩容算法做了调整，不再将元素加入链表头（而是保持与扩容前一样的顺序），但仍不意味着能够在多线程环境下能够安全扩容，还会出现其它问题（如扩容丢数据） 3）JDK 8 ConcurrentHashMap①重要属性和内部类1234567891011121314151617181920212223242526// 默认为 0// 当初始化时, 为 -1// 当扩容时, 为 -(1 + 扩容线程数)// 当初始化或扩容完成后，为 下一次的扩容的阈值大小private transient volatile int sizeCtl;// 整个 ConcurrentHashMap 就是一个 Node[]static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123;&#125;// hash 表transient volatile Node&lt;K,V&gt;[] table;// 扩容时的 新 hash 表private transient volatile Node&lt;K,V&gt;[] nextTable;// 扩容时如果某个 bin 迁移完毕, 用 ForwardingNode 作为旧 table bin 的头结点static final class ForwardingNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123;&#125;// 用在 compute 以及 computeIfAbsent 时, 用来占位, 计算完成后替换为普通 Nodestatic final class ReservationNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123;&#125;// 作为 treebin 的头节点, 存储 root 和 firststatic final class TreeBin&lt;K,V&gt; extends Node&lt;K,V&gt; &#123;&#125;// 作为 treebin 的节点, 存储 parent, left, rightstatic final class TreeNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123;&#125; sizeCtl代表了下一次扩容的阈值大小，如目前是12，那么阈值就是12*3&#x2F;4&#x3D;9 TreeBin代表红黑树的头结点，TreeNode则是红黑树中的结点 链表结点数大于阈值，默认是8以后才会考虑转化为红黑树 链表结点数大于8后，首先会检查node数组长度是否&gt;&#x3D;64，否则会先进行数组扩容，因为扩容后该链表的结点会重新进行hash计算，会被分走一部分 ForwardingNode如下 ②重要方法12345678// 获取 Node[] 中第 i 个 Nodestatic final &lt;K,V&gt; Node&lt;K,V&gt; tabAt(Node&lt;K,V&gt;[] tab, int i) // cas 修改 Node[] 中第 i 个 Node 的值, c 为旧值, v 为新值static final &lt;K,V&gt; boolean casTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; c, Node&lt;K,V&gt; v) // 直接修改 Node[] 中第 i 个 Node 的值, v 为新值static final &lt;K,V&gt; void setTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; v) ③构造器分析可以看到实现了懒惰初始化，在构造方法中仅仅计算了 table 的大小，以后在第一次使用时才会真正创建 123456789101112public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) &#123; if (!(loadFactor &gt; 0.0f) || initialCapacity &lt; 0 || concurrencyLevel &lt;= 0) throw new IllegalArgumentException(); if (initialCapacity &lt; concurrencyLevel) // Use at least as many bins initialCapacity = concurrencyLevel; // as estimated threads //计算size long size = (long)(1.0 + (long)initialCapacity / loadFactor); // tableSizeFor 仍然是保证计算的大小是 2^n, 即 16,32,64 ... int cap = (size &gt;= (long)MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : tableSizeFor((int)size); this.sizeCtl = cap; &#125; 如果传入的默认大小是8，size经过计算&#x3D;11.66667 但是tableSizeFor会保证size是2^n，所以这次的结果是16 ④get 流程1234567891011121314151617181920212223public V get(Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; // spread 方法能确保返回结果是正数 int h = spread(key.hashCode()); if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; // 如果头结点已经是要查找的 key if ((eh = e.hash) == h) &#123; if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; // hash 为负数表示该 bin 在扩容中或是 treebin, 这时调用 find 方法来查找 else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; // 正常遍历链表, 用 equals 比较 while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null;&#125; 根据 hash 值计算位置。 查找到指定位置，如果头节点就是要找的，直接返回它的 value. 如果头节点 hash 值小于 0 ，都重写了对应的查找方法 -1说明正在扩容，需要去新的table中找 -2是红黑树 如果是链表，遍历查找之。 ⑤put 流程以下数组简称（table），链表简称（bin） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990public V put(K key, V value) &#123; return putVal(key, value, false);&#125;final V putVal(K key, V value, boolean onlyIfAbsent) &#123; if (key == null || value == null) throw new NullPointerException(); // 其中 spread 方法会综合高位低位, 具有更好的 hash 性 int hash = spread(key.hashCode()); int binCount = 0; for (Node&lt;K,V&gt;[] tab = table;;) &#123; // f 是链表头节点 // fh 是链表头结点的 hash // i 是链表在 table 中的下标 Node&lt;K,V&gt; f; int n, i, fh; // 要创建 table if (tab == null || (n = tab.length) == 0) // 初始化 table 使用了 cas, 无需 synchronized 创建成功, 进入下一轮循环 tab = initTable(); // 要创建链表头节点 else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; // 添加链表头使用了 cas, 无需 synchronized if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; &#125; // 帮忙扩容 else if ((fh = f.hash) == MOVED) // 帮忙之后, 进入下一轮循环 tab = helpTransfer(tab, f); else &#123; V oldVal = null; // 锁住链表头节点 synchronized (f) &#123; // 再次确认链表头节点没有被移动 if (tabAt(tab, i) == f) &#123; // 链表 if (fh &gt;= 0) &#123; binCount = 1; // 遍历链表 for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; // 找到相同的 key if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; // 更新 if (!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K,V&gt; pred = e; // 已经是最后的节点了, 新增 Node, 追加至链表尾 if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; // 红黑树 else if (f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; // putTreeVal 会看 key 是否已经在树中, 是, 则返回对应的 TreeNode if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; // 释放链表头节点的锁 &#125; if (binCount != 0) &#123; if (binCount &gt;= TREEIFY_THRESHOLD) // 如果链表长度 &gt;= 树化阈值(8), 进行链表转为红黑树 treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; &#125; &#125; // 增加 size 计数 addCount(1L, binCount); return null; &#125; 根据 key 计算出 hashcode 。 判断是否需要进行初始化。 因为chm是懒惰创建的，只有当用到的时候才会创建，底层是cas保证不会重复创建 cas失败的线程会进行yield然后while等待cas成功的线程创建完map 即为当前 key 定位出的 Node，如果为空表示当前位置可以写入数据，利用 CAS 尝试写入，失败则自旋保证成功。 如果当前位置的 hashcode == MOVED == -1，这就是个forwardingNode 说明需要进行帮忙扩容，已经有线程在扩容了，自己也要帮忙扩容 如果都不满足，则利用 synchronized 锁写入数据，锁住的也是该bin的头结点，不是整个map 如果是链表(头结点hash值&gt;0) 遍历查找，看是否有key一致的，有一致的就更新，没有一致的就加载链尾 如果是红黑树(头结点hash值&#x3D;-2) 按照红黑树的方式添加或更新 添加完了以后，如果数量大于 TREEIFY_THRESHOLD 则要执行树化方法，在treeifyBin中会首先判断当前数组长度≥64时才会将链表转换为红黑树。 最后还对整个hashMap中的元素个数进行行了更新addCount ⑥初始化方法1234567891011121314151617181920212223private final Node&lt;K,V&gt;[] initTable() &#123; Node&lt;K,V&gt;[] tab; int sc; while ((tab = table) == null || tab.length == 0) &#123; if ((sc = sizeCtl) &lt; 0) Thread.yield(); // 尝试将 sizeCtl 设置为 -1（表示初始化 table） else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; // 获得锁, 创建 table, 这时其它线程会在 while() 循环中 yield 直至 table 创建 try &#123; if ((tab = table) == null || tab.length == 0) &#123; int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = tab = nt; sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; sizeCtl = sc; &#125; break; &#125; &#125; return tab; &#125; 只有一个cas成功的线程能去初始化创建新数组 其它cas失败的线程会每次都会yield放弃cpu占用，然后while等待数组创建完毕 ⑥addCount方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950// check 是之前 binCount 的个数 private final void addCount(long x, int check) &#123; CounterCell[] as; long b, s; if ( // 已经有了 counterCells, 向 cell 累加 (as = counterCells) != null || // 还没有, 向 baseCount 累加 !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x) ) &#123; CounterCell a; long v; int m; boolean uncontended = true; if ( // 还没有 counterCells as == null || (m = as.length - 1) &lt; 0 || // 还没有 cell (a = as[ThreadLocalRandom.getProbe() &amp; m]) == null || // cell cas 增加计数失败 !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x)) ) &#123; // 创建累加单元数组和cell, 累加重试 fullAddCount(x, uncontended); return; &#125; if (check &lt;= 1) return; // 获取元素个数 s = sumCount(); &#125; if (check &gt;= 0) &#123; Node&lt;K,V&gt;[] tab, nt; int n, sc; while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123; int rs = resizeStamp(n); if (sc &lt; 0) &#123; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; // newtable 已经创建了，帮忙扩容 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); &#125; // 需要扩容，这时 newtable 未创建 else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); s = sumCount(); &#125; &#125; &#125; 计算过后会检查hashMap元素个数是否&gt;阈值，如果&gt;阈值就会进行扩容操作，如果需要扩容 有一个线程cas成功调用transfer方法进行扩容 其它线程cas失败就会帮忙扩容 ⑦size 计算流程size 计算实际发生在 put，remove 改变集合元素的操作之中 没有竞争发生，向 baseCount 累加计数 有竞争发生，新建 counterCells，向其中的一个 cell 累加计数 counterCells 初始有两个 cell 如果计数竞争比较激烈，会创建新的 cell 来累加计数 12345678910111213141516171819public int size() &#123; long n = sumCount(); return ((n &lt; 0L) ? 0 : (n &gt; (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int)n);&#125;final long sumCount() &#123; CounterCell[] as = counterCells; CounterCell a; // 将 baseCount 计数与所有 cell 计数累加 long sum = baseCount; if (as != null) &#123; for (int i = 0; i &lt; as.length; ++i) &#123; if ((a = as[i]) != null) sum += a.value; &#125; &#125; return sum;&#125; 借用了LongAdder的思想，通过多个cells来计数 最后的计数不是精确值，是一个大概值 ⑧transfer扩容方法 扩容，扩容时以 bin 为单位进行，需要对 bin 进行 synchronized 已经扩容的bin会被标记为forwardingNode 但这时妙的是其它竞争线程也不是无事可做，它们会帮助把其它 bin 进行扩容 扩容时平均只有 1&#x2F;6 的节点会把复制到新 table 中 ⑨总结Java 8 数组（Node） +（ 链表 Node | 红黑树 TreeNode ） 以下数组简称（table），链表简称（bin） 初始化，使用 cas 来保证并发安全，懒惰初始化 table 树化，当 table.length &lt; 64 时，先尝试扩容，超过 64 时，并且 bin.length &gt; 8 时，会将链表树化，树化过程会用 synchronized 锁住链表头 put，如果该 bin 尚未创建，只需要使用 cas 创建 bin 如果已经有了，synchronized锁住链表头进行后续 put 操作，元素添加至 bin 的尾部 根据红黑树和链表各自的方法来put get，无锁操作仅需要保证可见性，扩容过程中 get 操作拿到的是 ForwardingNode 它会让 get 操作在新 table 进行搜索 扩容，扩容时以 bin 为单位进行，需要对 bin 进行 synchronized，但这时妙的是其它竞争线程也不是无事可做，它们会帮助把其它 bin 进行扩容，扩容时平均只有 1&#x2F;6 的节点会被复制到新 table 中 size，元素个数保存在 baseCount 中，并发时的个数变动保存在 CounterCell[] 当中。最后统计数量时累加即可 4）JDK 7 ConcurrentHashMap①存储结构 下图存在两个笔误 : Segmeng -&gt; Segment ; HashEntity -&gt; HashEntry 它维护了一个 segment 数组，每个 segment 对应一把锁 优点：如果多个线程访问不同的 segment，实际是没有冲突的，这与 jdk8 中是类似的 缺点：Segments 数组默认大小为16，这个容量初始化指定后就不能改变了，并且不是懒惰初始化 ②构造器分析1234567891011121314151617181920212223242526272829303132public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) &#123; if (!(loadFactor &gt; 0) || initialCapacity &lt; 0 || concurrencyLevel &lt;= 0) throw new IllegalArgumentException(); if (concurrencyLevel &gt; MAX_SEGMENTS) concurrencyLevel = MAX_SEGMENTS; // ssize 必须是 2^n, 即 2, 4, 8, 16 ... 表示了 segments 数组的大小 int sshift = 0; int ssize = 1; while (ssize &lt; concurrencyLevel) &#123; ++sshift; ssize &lt;&lt;= 1; &#125; // segmentShift 默认是 32 - 4 = 28 this.segmentShift = 32 - sshift; // segmentMask 默认是 15 即 0000 0000 0000 1111 this.segmentMask = ssize - 1; if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; int c = initialCapacity / ssize; if (c * ssize &lt; initialCapacity) ++c; int cap = MIN_SEGMENT_TABLE_CAPACITY; while (cap &lt; c) cap &lt;&lt;= 1; // 创建 segments and segments[0] Segment&lt;K,V&gt; s0 = new Segment&lt;K,V&gt;(loadFactor, (int)(cap * loadFactor), (HashEntry&lt;K,V&gt;[])new HashEntry[cap]); Segment&lt;K,V&gt;[] ss = (Segment&lt;K,V&gt;[])new Segment[ssize]; UNSAFE.putOrderedObject(ss, SBASE, s0); // ordered write of segments[0] this.segments = ss; &#125; 构造完成，如下图所示 可以看到 ConcurrentHashMap 没有实现懒惰初始化，空间占用不友好 其中 this.segmentShift 和 this.segmentMask 的作用是决定将 key 的 hash 结果匹配到哪个 segment 例如，根据某一 hash 值求 segment 位置，先将高位向低位移动 this.segmentShift 位 segment和jdk 8 中锁住某个bin的链表头的思想是一致的，但segment的粒度更大 segment设置好了就不能改变了 ConcurrentHashMap 没有实现懒惰初始化，segment和segment[0]整个数组都是一开始就创建好的 ③put 流程123456789101112131415161718public V put(K key, V value) &#123; Segment&lt;K,V&gt; s; if (value == null) throw new NullPointerException(); int hash = hash(key); // 计算出 segment 下标 int j = (hash &gt;&gt;&gt; segmentShift) &amp; segmentMask; // 获得 segment 对象, 判断是否为 null, 是则创建该 segment if ((s = (Segment&lt;K,V&gt;)UNSAFE.getObject (segments, (j &lt;&lt; SSHIFT) + SBASE)) == null) &#123; // 这时不能确定是否真的为 null, 因为其它线程也发现该 segment 为 null, // 因此在 ensureSegment 里用 cas 方式保证该 segment 安全性 s = ensureSegment(j); &#125; // 进入 segment 的put 流程 return s.put(key, hash, value, false);&#125; 首先通过cas方法来获得对应的segment 然后调用segment.put方法，segment继承了可重入锁（ReentrantLock） segment 继承了可重入锁（ReentrantLock），它的 put 方法为 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455final V put(K key, int hash, V value, boolean onlyIfAbsent) &#123; // 尝试加锁 HashEntry&lt;K,V&gt; node = tryLock() ? null : // 如果不成功, 进入 scanAndLockForPut 流程 // 如果是多核 cpu 最多 tryLock 64 次, 进入 lock 流程 // 在尝试期间, 还可以顺便看该节点在链表中有没有, 如果没有顺便创建出来 scanAndLockForPut(key, hash, value); // 执行到这里 segment 已经被成功加锁, 可以安全执行 V oldValue; try &#123; HashEntry&lt;K,V&gt;[] tab = table; int index = (tab.length - 1) &amp; hash; HashEntry&lt;K,V&gt; first = entryAt(tab, index); for (HashEntry&lt;K,V&gt; e = first;;) &#123; if (e != null) &#123; // 更新 K k; if ((k = e.key) == key || (e.hash == hash &amp;&amp; key.equals(k))) &#123; oldValue = e.value; if (!onlyIfAbsent) &#123; e.value = value; ++modCount; &#125; break; &#125; e = e.next; &#125; else &#123; // 新增 // 1) 之前等待锁时, node 已经被创建, next 指向链表头 if (node != null) node.setNext(first); else // 2) 创建新 node node = new HashEntry&lt;K,V&gt;(hash, key, value, first); int c = count + 1; // 3) 扩容 if (c &gt; threshold &amp;&amp; tab.length &lt; MAXIMUM_CAPACITY) rehash(node); else // 将 node 作为链表头 setEntryAt(tab, index, node); ++modCount; count = c; oldValue = null; break; &#125; &#125; &#125; finally &#123; unlock(); &#125; return oldValue; &#125; 首先tryLock尝试获取锁，重复超过64次，如果还是没获得就会lock方式来wait 在尝试期间, 还可以顺便看该节点在链表中有没有, 如果没有顺便创建出来 获得锁了以后就会具体来put，如果有了就更新，否则就new node作为链表头 JDK7中新的node会成为链表头，原来的链表头会成为第二个 ④rehash 扩容流程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748private void rehash(HashEntry&lt;K,V&gt; node) &#123; HashEntry&lt;K,V&gt;[] oldTable = table; int oldCapacity = oldTable.length; int newCapacity = oldCapacity &lt;&lt; 1; threshold = (int)(newCapacity * loadFactor); HashEntry&lt;K,V&gt;[] newTable = (HashEntry&lt;K,V&gt;[]) new HashEntry[newCapacity]; int sizeMask = newCapacity - 1; for (int i = 0; i &lt; oldCapacity ; i++) &#123; HashEntry&lt;K,V&gt; e = oldTable[i]; if (e != null) &#123; HashEntry&lt;K,V&gt; next = e.next; int idx = e.hash &amp; sizeMask; if (next == null) // Single node on list newTable[idx] = e; else &#123; // Reuse consecutive sequence at same slot HashEntry&lt;K,V&gt; lastRun = e; int lastIdx = idx; // 过一遍链表, 尽可能把 rehash 后 idx 不变的节点重用 for (HashEntry&lt;K,V&gt; last = next; last != null; last = last.next) &#123; int k = last.hash &amp; sizeMask; if (k != lastIdx) &#123; lastIdx = k; lastRun = last; &#125; &#125; newTable[lastIdx] = lastRun; // 剩余节点需要新建 for (HashEntry&lt;K,V&gt; p = e; p != lastRun; p = p.next) &#123; V v = p.value; int h = p.hash; int k = h &amp; sizeMask; HashEntry&lt;K,V&gt; n = newTable[k]; newTable[k] = new HashEntry&lt;K,V&gt;(h, p.key, v, n); &#125; &#125; &#125; &#125; // 扩容完成, 才加入新的节点 int nodeIndex = node.hash &amp; sizeMask; // add the new node node.setNext(newTable[nodeIndex]); newTable[nodeIndex] = node; // 替换为新的 HashEntry table table = newTable; &#125; 和JDK一样发生在 put 中，因为此时已经获得了锁，因此 rehash 时不需要考虑线程安全 主要有两种模式： 如果有某个node（可能是链表）的位置不需要改变则直接将其移动到新的map中 这表明老的链表中没有该node，被移动走了 需要改变位置的node（可能是链表）才会在新的map中new 这表明老的链表中也会有该node ⑤get 流程12345678910111213141516171819public V get(Object key) &#123; Segment&lt;K,V&gt; s; // manually integrate access methods to reduce overhead HashEntry&lt;K,V&gt;[] tab; int h = hash(key); // u 为 segment 对象在数组中的偏移量 long u = (((h &gt;&gt;&gt; segmentShift) &amp; segmentMask) &lt;&lt; SSHIFT) + SBASE; // s 即为 segment if ((s = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(segments, u)) != null &amp;&amp; (tab = s.table) != null) &#123; for (HashEntry&lt;K,V&gt; e = (HashEntry&lt;K,V&gt;) UNSAFE.getObjectVolatile (tab, ((long)(((tab.length - 1) &amp; h)) &lt;&lt; TSHIFT) + TBASE); e != null; e = e.next) &#123; K k; if ((k = e.key) == key || (e.hash == h &amp;&amp; key.equals(k))) return e.value; &#125; &#125; return null; &#125; 计算得到 key 的存放的segment，然后找到对应桶下标 遍历指定桶查找相同 key 的 value 值 get 时并未加锁，用了 UNSAFE 方法保证了可见性，扩容过程中，get 先发生就从旧表取内容，get 后发生就从新表取内容 ⑥size 计算流程12345678910111213141516171819202122232425262728293031323334353637383940public int size() &#123; // Try a few times to get accurate count. On failure due to // continuous async changes in table, resort to locking. final Segment&lt;K,V&gt;[] segments = this.segments; int size; boolean overflow; // true if size overflows 32 bits long sum; // sum of modCounts long last = 0L; // previous sum int retries = -1; // first iteration isn&#x27;t retry try &#123; for (;;) &#123; if (retries++ == RETRIES_BEFORE_LOCK) &#123; // 超过重试次数, 需要创建所有 segment 并加锁 for (int j = 0; j &lt; segments.length; ++j) ensureSegment(j).lock(); // force creation &#125; sum = 0L; size = 0; overflow = false; for (int j = 0; j &lt; segments.length; ++j) &#123; Segment&lt;K,V&gt; seg = segmentAt(segments, j); if (seg != null) &#123; sum += seg.modCount; int c = seg.count; if (c &lt; 0 || (size += c) &lt; 0) overflow = true; &#125; &#125; if (sum == last) break; last = sum; &#125; &#125; finally &#123; if (retries &gt; RETRIES_BEFORE_LOCK) &#123; for (int j = 0; j &lt; segments.length; ++j) segmentAt(segments, j).unlock(); &#125; &#125; return overflow ? Integer.MAX_VALUE : size; &#125; 计算元素个数前，先不加锁计算两次，如果前后两次结果如一样，认为个数正确返回 如果不一样，进行重试，重试次数超过 3，将所有 segment 锁住，重新计算个数返回 5.LinkedBlockingQueue1）入队出队①入队12345678910111213public class LinkedBlockingQueue&lt;E&gt; extends AbstractQueue&lt;E&gt; implements BlockingQueue&lt;E&gt;, java.io.Serializable &#123; static class Node&lt;E&gt; &#123; E item; /** * 下列三种情况之一 * - 真正的后继节点 * - 自己, 发生在出队时 * - null, 表示是没有后继节点, 是最后了 */ Node&lt;E&gt; next; Node(E x) &#123; item = x; &#125; &#125;&#125; 初始化链表 last = head = new Node&lt;E&gt;(null); Dummy 节点用来占位，item 为 null 当一个节点入队 last = last.next = node; 再来一个节点入队 last = last.next = node; new的时候会有一个dummy结点 入队的方式和普通队列是一样的 ②出队12345678Node&lt;E&gt; h = head;Node&lt;E&gt; first = h.next; h.next = h; // help GChead = first; E x = first.item;first.item = null;return x;h = head h = head first = h.next h.next = h，这里主要是为了gc掉原 dummy 头结点 head = first 123E x = first.item;first.item = null;return x; 出队过程就是三步 将dummy自己指自己，help gc 掉 保存第一个有值的节点，也就是dummy的next，然后将其值设为null，作为新的dummy及结点 最后返回保存的值 2）加锁分析①基本思路高明之处在于用了两把锁和 dummy 节点 用一把锁，同一时刻，最多只允许有一个线程（生产者或消费者，二选一）执行 用两把锁，同一时刻，可以允许两个线程同时（一个生产者与一个消费者）执行 消费者与消费者线程仍然串行 生产者与生产者线程仍然串行 线程安全分析 当节点总数大于 2 时（包括 dummy 节点），putLock 保证的是 last 节点的线程安全，takeLock 保证的是 head 节点的线程安全。两把锁保证了入队和出队没有竞争 当节点总数等于 2 时（即一个 dummy 节点，一个正常节点）这时候，仍然是两把锁锁两个对象，不会竞争 当节点总数等于 1 时（就一个 dummy 节点）这时 take 线程会被 notEmpty 条件阻塞，有竞争，会阻塞 12345// 用于 put(阻塞) offer(非阻塞)private final ReentrantLock putLock = new ReentrantLock();// 用户 take(阻塞) poll(非阻塞)private final ReentrantLock takeLock = new ReentrantLock(); ②put 操作12345678910111213141516171819202122232425262728public void put(E e) throws InterruptedException &#123; if (e == null) throw new NullPointerException(); int c = -1; Node&lt;E&gt; node = new Node&lt;E&gt;(e); final ReentrantLock putLock = this.putLock; // count 用来维护元素计数 final AtomicInteger count = this.count; putLock.lockInterruptibly(); try &#123; // 满了等待 while (count.get() == capacity) &#123; // 倒过来读就好: 等待 notFull notFull.await(); &#125; // 有空位, 入队且计数加一 enqueue(node); c = count.getAndIncrement(); // 除了自己 put 以外, 队列还有空位, 由自己叫醒其他 put 线程 if (c + 1 &lt; capacity) notFull.signal(); &#125; finally &#123; putLock.unlock(); &#125; // 如果队列中有一个元素, 叫醒 take 线程 if (c == 0) // 这里调用的是 notEmpty.signal() 而不是 notEmpty.signalAll() 是为了减少竞争 signalNotEmpty();&#125; ③take 操作123456789101112131415161718192021222324public E take() throws InterruptedException &#123; E x; int c = -1; final AtomicInteger count = this.count; final ReentrantLock takeLock = this.takeLock; takeLock.lockInterruptibly(); try &#123; while (count.get() == 0) &#123; notEmpty.await(); &#125; x = dequeue(); c = count.getAndDecrement(); if (c &gt; 1) notEmpty.signal();//take以后还有货，那就再叫一个线程来拿 &#125; finally &#123; takeLock.unlock(); &#125; // 如果队列中只有一个空位时, 叫醒 put 线程 // 如果有多个线程进行出队, 第一个线程满足 c == capacity, 但后续线程 c &lt; capacity if (c == capacity) // 这里调用的是 notFull.signal() 而不是 notFull.signalAll() 是为了减少竞争 signalNotFull() return x; &#125; 由 put 唤醒 put 是为了避免信号不足 3）性能比较主要列举 LinkedBlockingQueue 与 ArrayBlockingQueue 的性能比较 ArrayBlockingQueue是LinkedBlockingQueue 基于数组的兄弟实现 Linked 支持有界，Array 强制有界 Array是数组实现，肯定是要有界的 Linked 实现是链表，Array 实现是数组 Linked 是懒惰的，而 Array 需要提前初始化 Node 数组 Linked 每次入队会生成新 Node，而 Array 的 Node 是提前创建好的 Linked 两把锁，Array 一把锁 6.ConcurrentLinkedQueueConcurrentLinkedQueue 的设计与 LinkedBlockingQueue 非常像，也是 两把【锁】，同一时刻，可以允许两个线程同时（一个生产者与一个消费者）执行 dummy 节点的引入让两把【锁】将来锁住的是不同对象，避免竞争 只是这【锁】使用了 cas 来实现 事实上，ConcurrentLinkedQueue 应用还是非常广泛的 例如之前讲的 Tomcat 的 Connector 结构时，Acceptor 作为生产者向 Poller 消费者传递事件信息时，正是采用了ConcurrentLinkedQueue 将 SocketChannel 给 Poller 使用 7.CopyOnWriteArrayList①基本原理CopyOnWriteArraySet 是它的马甲 底层实现采用了 写入时拷贝 的思想，增删改操作会将底层数组拷贝一份，更改操作在新数组上执行，这时不影响其它线程的并发读，读写分离。 以新增为例： 1234567891011121314public boolean add(E e) &#123; synchronized (lock) &#123; // 获取旧的数组 Object[] es = getArray(); int len = es.length; // 拷贝新的数组（这里是比较耗时的操作，但不影响其它读线程） es = Arrays.copyOf(es, len + 1); //会将copy出的Array作为最后的引用，原来 // 添加新元素 es[len] = e; // 替换旧的数组 setArray(es); return true; &#125;&#125; 这里的源码版本是 Java 11，在 Java 1.8 中使用的是可重入锁ReentrantLock而不是 synchronized 其它读操作并未加锁，例如： 1234567public void forEach(Consumer&lt;? super E&gt; action) &#123; Objects.requireNonNull(action); for (Object x : getArray()) &#123; @SuppressWarnings(&quot;unchecked&quot;) E e = (E) x; action.accept(e); &#125;&#125; 适合『读多写少』的应用场景 写太多，每次写都会copy一次，其实就是用空间换取了线程安全 ②get 弱一致性 不容易测试，但问题确实存在 ③迭代器弱一致性 12345678910111213CopyOnWriteArrayList&lt;Integer&gt; list = new CopyOnWriteArrayList&lt;&gt;();list.add(1);list.add(2);list.add(3);Iterator&lt;Integer&gt; iter = list.iterator();new Thread(() -&gt; &#123; list.remove(0); System.out.println(list);&#125;).start();sleep1s();while (iter.hasNext()) &#123; System.out.println(iter.next());&#125; 不要觉得弱一致性就不好 数据库的 MVCC 都是弱一致性的表现 并发高和一致性是矛盾的，需要权衡","categories":[{"name":"并发","slug":"并发","permalink":"http://example.com/categories/%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"JUC并发","slug":"JUC并发","permalink":"http://example.com/tags/JUC%E5%B9%B6%E5%8F%91/"}]},{"title":"mysql深入","slug":"mysql深入1","date":"2022-04-13T07:29:48.000Z","updated":"2022-08-17T06:01:30.932Z","comments":true,"path":"2022/04/13/mysql深入1/","link":"","permalink":"http://example.com/2022/04/13/mysql%E6%B7%B1%E5%85%A51/","excerpt":"事务：四大特性、隔离级别 常见引擎：InnoDB、Myisam、Memory，各自特性及比较 索引：结构、分类、语法、使用规则 sql优化：insert、order by、group by、limit、count、update 视图&#x2F;存储过程&#x2F;触发器：语法、使用 锁：全局锁、表级锁（读写锁、元数据锁、意向锁）、行级锁（读写锁、临键锁、间隙锁） InnoDB引擎：物理结构、逻辑结构、事务原理（redo log、undo log、MVCC） 高可用：日志（错误、bin log、查询、慢查询）、主从复制（binlog）、分库分表、读写分离","text":"事务：四大特性、隔离级别 常见引擎：InnoDB、Myisam、Memory，各自特性及比较 索引：结构、分类、语法、使用规则 sql优化：insert、order by、group by、limit、count、update 视图&#x2F;存储过程&#x2F;触发器：语法、使用 锁：全局锁、表级锁（读写锁、元数据锁、意向锁）、行级锁（读写锁、临键锁、间隙锁） InnoDB引擎：物理结构、逻辑结构、事务原理（redo log、undo log、MVCC） 高可用：日志（错误、bin log、查询、慢查询）、主从复制（binlog）、分库分表、读写分离 一、事务事务是一组操作的集合，事务会把所有操作作为一个整体一起向系统提交或撤销操作请求，即这些操作要么同时成功，要么同时失败。 基本操作： 1234567891011121314151617181920212223-- 1. 查询张三账户余额select * from account where name = &#x27;张三&#x27;;-- 2. 将张三账户余额-1000update account set money = money - 1000 where name = &#x27;张三&#x27;;-- 此语句出错后张三钱减少但是李四钱没有增加模拟sql语句错误-- 3. 将李四账户余额+1000update account set money = money + 1000 where name = &#x27;李四&#x27;;-- 查看事务提交方式SELECT @@AUTOCOMMIT;-- 设置事务提交方式，1为自动提交，0为手动提交，该设置只对当前会话有效SET @@AUTOCOMMIT = 0;-- 提交事务COMMIT;-- 回滚事务ROLLBACK;-- 设置手动提交后上面代码改为：select * from account where name = &#x27;张三&#x27;;update account set money = money - 1000 where name = &#x27;张三&#x27;;update account set money = money + 1000 where name = &#x27;李四&#x27;;commit; 操作方式二： 开启事务：START TRANSACTION 或 BEGIN TRANSACTION;提交事务：COMMIT;回滚事务：ROLLBACK; 操作实例： 12345start transaction;select * from account where name = &#x27;张三&#x27;;update account set money = money - 1000 where name = &#x27;张三&#x27;;update account set money = money + 1000 where name = &#x27;李四&#x27;;commit; 1.四大特性ACID 原子性(Atomicity)：事务是不可分割的最小操作但愿，要么全部成功，要么全部失败 一致性(Consistency)：事务完成时，必须使所有数据都保持一致状态 隔离性(Isolation)：数据库系统提供的隔离机制，保证事务在不受外部并发操作影响的独立环境下运行 持久性(Durability)：事务一旦提交或回滚，它对数据库中的数据的改变就是永久的 2.并发事务 问题 描述 脏读 一个事务读到另一个事务还没提交的数据 不可重复读 一个事务先后读取同一条记录，但两次读取的数据不同 幻读 一个事务按照条件查询数据时，没有对应的数据行，但是再插入数据时，又发现这行数据已经存在 ①脏读（Dirty read）： ​ 数据库中A&#x3D;200，事务1中修改了A&#x3D;100，但是这时候事务1还没有commit，而此时事务2要读取A的值，事务2读取A&#x3D;100，这是事务A修改了但是还没有commit的数据，这种数据就称为’脏数据’，此时就发生了脏读。 ②不可重复读（Unrepeatable read） ​ 数据库中id&#x3D;1的数据有一个值为A&#x3D;200，事务A第一次selectid=1的数据查询出A&#x3D;200，此时事务2修改updateid=1的数据使A&#x3D;100并且commit了，此时数据库中的id=1的数据A&#x3D;100，然后事务1第二次读取selectid=1的数据查询出A&#x3D;100和第一次查询出的A&#x3D;200不一致，此时就发生了不可重复读现象。 ③幻读（Phantom read） ​ 幻读与不可重复读类似，事务1第一步select查询id&#x3D;1的数据，此时数据库为空没有查到，刚好此时事务2向数据库中插入了id&#x3D;1的数据，此时事务1第二步要插入id&#x3D;1的数据的时候就会报错，当事务1再次查询数据库的时候发现id为1的数据还是没有（因为已经解决了不可重复读的问题，两次查询的结果一致），这时候就发生了’幻读’。因为在插入数据的时候会因为事务2插入的数据失败，事务1会发现已经有了插入的数据，但是查询的时候又看见，就像幻觉一样Phantom。 ④丢失更新（Lost to modify） 事务1想要修改A&#x3D;20并且读取到了A&#x3D;20数据（此时事务2页读取到了A&#x3D;20），使A&#x3D;A-1，修改的结果为19；事务2读取到A&#x3D;20数据后也要进行修改，使A&#x3D;A-3，得到的结果为17；最终结果为17，看起来就是事务1的结果丢失了。 注： 不可重复读和幻读类似，不可重复读是发生在事务A因为事务B的修改导致两次read的数据不一致，幻读是因为事务A因为事务B的插入数据导致自身插入数据失败，但是两次查询（特别是第二次）发现没有数据（解决了不可重复度的基础上），好像出现了幻觉。 这三个问题的详细演示：https://www.bilibili.com/video/BV1Kr4y1i7ru?p=55cd 丢失更新相关博客：https://blog.csdn.net/sun8112133/article/details/89853755 并发事务隔离级别： 隔离级别 脏读 不可重复读 幻读 Read uncommitted √ √ √ Read committed × √ √ Repeatable Read(默认) × × √ Serializable × × × √表示在当前隔离级别下该问题会出现 Serializable 性能最低；Read uncommitted 性能最高，数据安全性最差 查看事务隔离级别：SELECT @@TRANSACTION_ISOLATION;设置事务隔离级别：SET [ SESSION | GLOBAL ] TRANSACTION ISOLATION LEVEL &#123;READ UNCOMMITTED | READ COMMITTED | REPEATABLE READ | SERIALIZABLE &#125;;SESSION 是会话级别，表示只针对当前会话有效，GLOBAL 表示对所有会话有效 二、存储引擎1. MySQL体系结构 Mysql结构（4层）： ①连接层 ​ 处理来自各个客户端的连接（验证密码等），授权认证及相关安全信息 ②服务层 ​ 最核心的一层，mysql在这一层中完成各类服务，执行sql、sql分析和优化，过程函数都在这一层实行 ③引擎层 ​ mysql实现的是可插拔存储引擎，想用哪个插哪个，默认为InnoDB。同时索引(index)也在这一层，所以各个存储引擎实现索引的方式可能不一样。存储引擎是数据库存储数据、建立索引、更新&#x2F;查询等技术的实现方式，存储引擎是基于表的 ④存储层 ​ 主要是将存储的数据存入系统硬盘上，完成与存储引擎的交互 存储引擎就是存储数据、建立索引、更新&#x2F;查询数据等技术的实现方式。存储引擎是基于表而不是基于库的，所以存储引擎也可以被称为表引擎。默认存储引擎是InnoDB。 相关操作： 12345678-- 查询建表语句show create table account;-- 建表时指定存储引擎CREATE TABLE 表名( ...) ENGINE=INNODB;-- 查看当前数据库支持的存储引擎show engines; 2. 常见引擎1）InnoDBInnoDB 是一种兼顾高可靠性和高性能的通用存储引擎，在 MySQL 5.5 之后，InnoDB 是默认的 MySQL 引擎。 特点： DML 操作遵循 ACID 模型，支持事务 行级锁，提高并发访问性能 支持外键约束，保证数据的完整性和正确性 文件： xxx.ibd: xxx代表表名，InnoDB 引擎的每张表都会对应这样一个表空间文件，存储该表的表结构（frm、sdi）、数据和索引。 参数：innodb_file_per_table，决定多张表共享一个表空间还是每张表对应一个表空间 知识点： 查看 Mysql 变量：show variables like &#39;innodb_file_per_table&#39;; 从idb文件提取表结构数据：（在cmd运行）ibd2sdi xxx.ibd InnoDB 逻辑存储结构： 2）MyISAMMyISAM 是 MySQL 早期的默认存储引擎。 特点： 不支持事务，不支持外键 支持表锁，不支持行锁 访问速度快 文件： xxx.sdi: 存储表结构信息 xxx.MYD: 存储数据 xxx.MYI: 存储索引 3）MemoryMemory 引擎的表数据是存储在内存中的，受硬件问题、断电问题的影响，只能将这些表作为临时表或缓存使用。 特点： 存放在内存中，速度快 hash索引（默认） 文件： xxx.sdi: 存储表结构信息 3. 存储引擎特点比较 特点 InnoDB MyISAM Memory 存储限制 64TB 有 有 事务安全 支持 - - 锁机制 行锁 表锁 表锁 B+tree索引 支持 支持 支持 Hash索引 - - 支持 全文索引 支持（5.6版本之后） 支持 - 空间使用 高 低 N&#x2F;A 内存使用 高 低 中等 批量插入速度 低 高 高 支持外键 支持 - - 4. 存储引擎的选择在选择存储引擎时，应该根据应用系统的特点选择合适的存储引擎。对于复杂的应用系统，还可以根据实际情况选择多种存储引擎进行组合。 InnoDB: 如果应用对事物的完整性有比较高的要求，在并发条件下要求数据的一致性，数据操作除了插入和查询之外，还包含很多的更新、删除操作，则 InnoDB 是比较合适的选择 MyISAM: 如果应用是以读操作和插入操作为主，只有很少的更新和删除操作，并且对事务的完整性、并发性要求不高，那这个存储引擎是非常合适的。 Memory: 将所有数据保存在内存中，访问速度快，通常用于临时表及缓存。Memory 的缺陷是对表的大小有限制，太大的表无法缓存在内存中，而且无法保障数据的安全性 电商中的足迹和评论适合使用 MyISAM 引擎，缓存适合使用 Memory 引擎。 三、 索引1. 索引结构 索引结构 描述 B+Tree 最常见的索引类型，大部分引擎都支持B+树索引 Hash 底层数据结构是用哈希表实现，只有精确匹配索引列的查询才有效，不支持范围查询 R-Tree(空间索引) 空间索引是 MyISAM 引擎的一个特殊索引类型，主要用于地理空间数据类型，通常使用较少 Full-Text(全文索引) 是一种通过建立倒排索引，快速匹配文档的方式，类似于 Lucene, Solr, ES 索引 InnoDB MyISAM Memory B+Tree索引 支持 支持 支持 Hash索引 不支持 不支持 支持 R-Tree索引 不支持 支持 不支持 Full-text 5.6版本后支持 支持 不支持 1）二叉搜索树 二叉树的缺点可以用红黑树来解决：红黑树也存在大数据量情况下，层级较深，检索速度慢的问题。 2）B TreeB Tree相较于二叉搜索树维护了自平衡（左右高度差距不会过大）并且增加了一个节点的字节点数量，eg： 如下图degree&#x3D;5（子节点个数），一共可存4个key（值），x&lt;20，20&lt;&#x3D;x&lt;30，…，…，89&#x3D;&lt;x 为了解决上述问题，可以使用 B-Tree 结构： ​ B-Tree (多路平衡查找树) 以一棵最大度数（max-degree，指一个节点的子节点个数）为5（5阶）的 b-tree 为例（每个节点最多存储4个key，5个指针） B-Tree 的数据插入过程动画参照：https://www.bilibili.com/video/BV1Kr4y1i7ru?p=68演示地址：https://www.cs.usfca.edu/~galles/visualization/BTree.html 3）B+TreeB+ Tree 相较于B Tree就是将数据全部存储在了叶子节点，如下图左下角x&lt;16，16&lt;&#x3D;x&lt;29，29&lt;&#x3D;x 这样key&#x3D;16的数据就存储在了第二个叶子节点 结构图： 演示地址：https://www.cs.usfca.edu/~galles/visualization/BPlusTree.html 与 B Tree 的区别： 所有的数据都会出现在叶子节点 叶子节点形成一个单向链表 4）InooDB的改进B+ TreeMySQL 索引数据结构对经典的 B+Tree 进行了优化。在原 B+Tree 的基础上，增加一个指向相邻叶子节点的链表指针，就形成了带有顺序指针的 B+Tree，提高区间访问的性能。 ​ mysql中的B+树对经典的B+树做了优化，1.在叶子相邻叶子节点间添加了一个向前的指针使其成为了双向链表（首尾也是双向指针），这是便于范围搜索和排序 5）Hash索引哈希索引就是采用一定的hash算法，将键值换算成新的hash值，映射到对应的槽位上，然后存储在hash表中。如果两个（或多个）键值，映射到一个相同的槽位上，他们就产生了hash冲突（也称为hash碰撞），可以通过链表来解决。 特点： Hash索引只能用于对等比较（&#x3D;、in），不支持范围查询（betwwn、&gt;、&lt;、…） 无法利用索引完成排序操作 查询效率高，通常只需要一次检索就可以了，效率通常要高于 B+Tree 索引 存储引擎支持： Memory InnoDB: 具有自适应hash功能，hash索引是存储引擎根据 B+Tree 索引在指定条件下自动构建的 6）面试题 为什么 InnoDB 存储引擎选择使用 B+Tree 索引结构？ 相对于二叉树，层级更少，搜索效率高 对于 B-Tree，无论是叶子节点还是非叶子节点，都会保存数据，这样导致一页中存储的键值减少，指针也跟着减少，要同样保存大量数据，只能增加树的高度，导致性能降低 相对于 Hash 索引，B+Tree 支持范围匹配及排序操作 mine： 二叉搜索树在顺序插入的时候会形成一个链表，查询的过程就变成线性的一条一条查询；如果数据量较大，层级变深，搜索效率变慢 B Tree 无论节点还是非叶子节点都会保存数据，而每一个节点都是存储在一页中的，如果非叶子节点要保存value，那么这一页存储的keys就会减少，指针也变少了，要存储更多的数据，只能增加树的高度，导致性能降低了 同时因为改进后的B+树在叶子节点间添加了双向指针，便于范围匹配和排序，而Hash索引不能 2. 索引分类1）基本结构 分类 含义 特点 关键字 主键索引 针对于表中主键创建的索引 默认自动创建，只能有一个 PRIMARY 唯一索引 避免同一个表中某数据列中的值重复 可以有多个 UNIQUE 常规索引 快速定位特定数据 可以有多个 全文索引 全文索引查找的是文本中的关键词，而不是比较索引中的值 可以有多个 FULLTEXT 在 InnoDB 存储引擎中，根据索引的存储形式，又可以分为以下两种： 分类 含义 特点 聚集索引(Clustered Index) 将数据存储与索引放一块，索引结构的叶子节点保存了行数据 必须有，而且只有一个 二级索引(Secondary Index) 将数据与索引分开存储，索引结构的叶子节点关联的是对应的主键 可以存在多个 演示图： 聚集索引选取规则： 如果存在主键，主键索引就是聚集索引 如果不存在主键，将使用第一个唯一(UNIQUE)列作为聚集索引 如果表没有主键或没有合适的唯一索引，则 InnoDB 会自动生成一个 rowid 作为隐藏的聚集索引 2）思考题1. 以下 SQL 语句，哪个执行效率高？为什么？ 123select * from user where id = 10;select * from user where name = &#x27;Arm&#x27;;-- 备注：id为主键，name字段创建的有索引 答：第一条语句，因为第二条需要回表查询，相当于两个步骤。 2. InnoDB 主键索引的 B+Tree 高度为多少？ 答：假设一行数据大小为1k，一页中可以存储16行这样的数据。InnoDB 的指针占用6个字节的空间，主键假设为bigint，占用字节数为8.可得公式：n * 8 + (n + 1) * 6 = 16 * 1024，其中 8 表示 bigint 占用的字节数，n 表示当前节点存储的key的数量，(n + 1) 表示指针数量（比key多一个）。算出n约为1170。 如果树的高度为2，那么他能存储的数据量大概为：1171 * 16 = 18736；如果树的高度为3，那么他能存储的数据量大概为：1171 * 1171 * 16 = 21939856。 另外，如果有成千上万的数据，那么就要考虑分表 3. 索引语法创建索引：CREATE [ UNIQUE | FULLTEXT ] INDEX index_name ON table_name (index_col_name, ...);如果不加 CREATE 后面不加索引类型参数，则创建的是常规索引 查看索引：SHOW INDEX FROM table_name; 删除索引：DROP INDEX index_name ON table_name; 案例： 1234567891011-- name字段为姓名字段，该字段的值可能会重复，为该字段创建索引create index idx_user_name on tb_user(name);-- phone手机号字段的值非空，且唯一，为该字段创建唯一索引create unique index idx_user_phone on tb_user (phone);-- 为profession, age, status创建联合索引create index idx_user_pro_age_stat on tb_user(profession, age, status);-- 为email建立合适的索引来提升查询效率create index idx_user_email on tb_user(email);-- 删除索引drop index idx_user_email on tb_user; 4. sql性能分析①首先查看执行频次来分析当前数据库，是增删改语句执行的多，还是select语句执行的多 ②通过慢查询日志可以定位哪台主机通过哪个user执行了哪条sql语句，执行效率较低 ③慢查询日志只会记录超过设定时间的sql，如果要查看没有超过时限的sql则需要通过profiles来查询 ④explain会查看sql具体的执行信息 1）查看执行频次查看当前数据库的 INSERT, UPDATE, DELETE, SELECT 访问频次：全局：SHOW GLOBAL STATUS LIKE &#39;Com_______&#39;; 会话：SHOW SESSION STATUS LIKE &#39;Com_______&#39;; 例：show global status like &#39;Com_______&#39; 2）慢查询日志慢查询日志记录了所有执行时间超过指定参数（long_query_time，单位：秒，默认10秒）的所有SQL语句的日志。MySQL的慢查询日志默认没有开启，需要在MySQL的配置文件（&#x2F;etc&#x2F;my.cnf）中配置如下信息： 1234#开启慢查询日志开关slow_query_log=1#设置慢查询日志的时间为2秒，SQL语句执行时间超过2秒，就会视为慢查询，记录慢查询日志：long_query_time=2 更改后重启MySQL服务，日志文件位置：&#x2F;var&#x2F;lib&#x2F;mysql&#x2F;localhost-slow.log systemctl restart mysqld 查看慢查询日志开关状态： show variables like &#39;slow_query_log&#39;; 3）profileshow profile 能在做SQL优化时帮我们了解时间都耗费在哪里。通过 have_profiling 参数，能看到当前 MySQL 是否支持 profile 操作： SELECT @@have_profiling; profiling 默认关闭，可查询其开启情况（0是关闭） select @@profiling; 可以通过set语句在session&#x2F;global级别开启 profiling： SET profiling = 1; 查看所有语句的耗时： show profiles; 查看指定query_id的SQL语句各个阶段的耗时： show profile for query query_id; 查看指定query_id的SQL语句CPU的使用情况 show profile cpu for query query_id; 4）explainEXPLAIN 或者 DESC 命令获取 MySQL 如何执行 SELECT 语句的信息，包括在 SELECT 语句执行过程中表如何连接和连接的顺序。语法： # 直接在select语句之前加上关键字 explain &#x2F; desc EXPLAIN SELECT 字段列表 FROM 表名 HWERE 条件; EXPLAIN 各字段含义： id：select 查询的序列号，表示查询中执行 select 子句或者操作表的顺序（id相同，执行顺序从上到下；id不同，值越大越先执行） select_type： 表示 SELECT 的类型，常见取值有 SIMPLE（简单表，即不适用表连接或者子查询）、PRIMARY（主查询，即外层的查询）、UNION（UNION中的第二个或者后面的查询语句）、SUBQUERY（SELECT&#x2F;WHERE之后包含了子查询）等 type： 表示连接类型，性能由好到差的连接类型为 NULL、system、const、eq_ref、ref、range、index、all 用主键索引或者唯一索引查询，const 用普通索引，ref select ‘A’，NULL possible_key： 可能应用在这张表上的索引，一个或多个 Key： 实际使用的索引，如果为 NULL，则没有使用索引 Key_len： 表示索引中使用的字节数，该值为索引字段最大可能长度，并非实际使用长度，在不损失精确性的前提下，长度越短越好 rows： MySQL认为必须要执行的行数，在InnoDB引擎的表中，是一个估计值，可能并不总是准确的 filtered： 表示返回结果的行数占需读取行数的百分比，filtered的值越大越好 5. 使用规则1）最左前缀法则（联合索引）如果索引关联了多列（联合索引），要遵守最左前缀法则，最左前缀法则指的是查询从索引的最左列开始，并且不跳过索引中的列。如果跳跃某一列，索引将部分失效（后面的字段索引失效）。 联合索引中，出现范围查询（&lt;, &gt;），范围查询右侧的列索引失效。可以用&gt;&#x3D;或者&lt;&#x3D;来规避索引失效问题。 ​ 在这样一条联合索引中，索引的顺序分别是：profession（47），age（2），status（5）， profession列一定存在，但在where中的顺序不影响 这样两条sql都会走该联合索引，只是索引长度可能不一致，第一条会按顺序查询 pro，age，status三条索引（54） 下面一条没有status，只会顺序查询pro，age两条索引（49） 中间跳过某一列后，后面的索引就会失效 这样两条sql都会走联合查询，因为有pro在（最左边的列在）（where后and的顺序不重要） 但是只会查询pro索引，因为跳过了中间的age索引（没有），那么age后status索引也不会查询（47） 可以这样说：age单说是没有索引的，因为如果只根据age去查，没有pro的话，age的索引也不会用到 注：如果select字段中包含了该联合查询的字段而不需要回表查询也会使用联合查询 ​ 因为select字段在pro_age_sta的联合索引中都含有，而id则是联合索引的叶子节点值，所以通过索引能将这些值全部查询出来，这样就会使用索引，虽然不满足最左前缀原则，之前不使用是因为select * ，不能直接通过该联合索引获取所有值，需要回表查询 联合索引失效tips： 跳过联合索引中的某一列，该列右边的所有索引都会失效 若在索引上使用了&gt;，&lt; 等范围查询会让该索引右边的列索引失效（自身不会失效） 上面这条查询会查询pro，age，但是age使用了范围查询&gt;，所以age不会失效，status索引会失效（49） 解决办法：在业务允许的情况下，尽量使用&gt;&#x3D;和&lt;&#x3D; 这样查询就会查询pro，age，status三条索引 ​ 2）普通索引失效情况 在索引列上进行运算操作（如函数运算），索引将失效。 如：查询手机号后两位是15的用户 explain select * from tb_user where substring(phone, 10, 2) = &#39;15&#39;; 这样不会走phone索引，因为在phone索引上进行了函数运算 字符串类型字段使用时，不加引号，索引将失效。 如：explain select * from tb_user where phone = 17799990015;，此处phone的值没有加引号 虽然可以查询出来，但是并不会走phone的索引 模糊查询中，如果仅仅是尾部模糊匹配，索引不会是失效；如果是头部模糊匹配，索引失效。 如：explain select * from tb_user where profession like &#39;%工程&#39;; 前面有%会失效 ​ explain select * from tb_user where profession like &#39;工程%&#39;;后面有%不会失效 注：’_’ 和 ‘%’ 的情况是一样的 用 or 分割开的条件，如果 or 其中一个条件的列没有索引，那么涉及的索引都不会被用到。 如该条数据，可能会走主键索引，但是最后没走索引，因为age没有索引，解决办法就是为age添加上索引 如果 MySQL 评估使用索引比全表更慢，则不使用索引。 如：explain select * from tb_user where profession is null; pro字段只有少部分是null，所以会走索引 ​ explain select * from tb_user where profession is not null; pro字段大部分都不是null，这样还不如扫描all，所以不会走索引 3）SQL 提示​ 是优化数据库的一个重要手段，简单来说，就是在SQL语句中加入一些人为的提示来达到优化操作的目的： 如用户告诉mysql在有多个索引的情况下，选择哪个索引而不是让mysql的优化选择器自己选择 例如： 在可能多个索引的情况下： 使用哪个索引： explain select * from tb_user use index(idx_user_pro) where profession=&quot;软件工程&quot;; 不使用哪个索引： explain select * from tb_user ignore index(idx_user_pro) where profession=&quot;软件工程&quot;; 必须使用哪个索引： explain select * from tb_user force index(idx_user_pro) where profession=&quot;软件工程&quot;; use 是建议，实际使用哪个索引 MySQL 还会自己权衡运行速度去更改，force就是无论如何都强制使用该索引。 4）覆盖索引&amp;回表查询 覆盖索引：根据索引可以查询出所有数据 查询的数据中id值在该联合索引（二级索引）的叶子节点，所以只需要查一次联合索引就可以查询出所有值而不需要回表查询 回表查询：二级索引不能查询出所有值，需要通过查询出的主键id来查询聚集索引获取值 虽然使用了索引查询，但是因为查询的字段中的gender和email是无法通过该联合索引查询出来的，所以在通过二级索引获取了id，profession，age后，还需要通过id值查询聚集索引来获取剩下的gender和email，这就进行了回表查询 5）前缀索引当字段类型为字符串（varchar, text等）时，如果在这些字段上建立索引会导致建立的索引较大，查询时，也会浪费大量的磁盘IO，影响 查询效率，此时可以只降字符串的一部分前缀，建立索引，这样可以大大节约索引空间，从而提高索引效率。 比如要在email上建立索引，我们可以截取email的前几个char作为索引，这样就可以减小索引长度，可以根据一个比值选择性k来判断取值的合理性：不同的subString&#x2F;all，eg：当我们截取email字段前5个的时候，10个email中8个的前5个char都不一样，只有两个的前5个char是一样的，这时候k&#x3D;8&#x2F;10&#x3D;0.8，我就可以在k和长度之间做取舍 求选择性公式： 12select count(distinct email) / count(*) from tb_user;select count(distinct substring(email, 1, 5)) / count(*) from tb_user; 语法：create index idx_xxxx on table_name(columnn(n)); show index 里面的sub_part可以看到接取的长度 6）单列索引&amp;联合索引单列索引：即一个索引只包含单个列联合索引：即一个索引包含了多个列在业务场景中，如果存在多个查询条件，考虑针对于查询字段建立索引时，建议建立联合索引，而非单列索引。 单列索引情况：explain select id, phone, name from tb_user where phone = &#39;17799990010&#39; and name = &#39;韩信&#39;;这句只会用到phone索引字段，该sql会进行回表查询，因为phone的单列索引中并不包含name列 所以我们可以根据查询条件构建联合索引（注意顺序——最左前缀法则） create index idx_user_phone_name on tb_user(phone,name); 注意事项 多条件联合查询时，MySQL优化器会评估哪个字段的索引效率更高，会选择该索引完成本次查询 7）设计原则 针对于数据量较大，且查询比较频繁的表建立索引 针对于常作为查询条件（where）、排序（order by）、分组（group by）操作的字段建立索引 尽量选择区分度高的列作为索引，尽量建立唯一索引，区分度越高，使用索引的效率越高 如果是字符串类型的字段，字段长度较长，可以针对于字段的特点，建立前缀索引 尽量使用联合索引，减少单列索引，查询时，联合索引很多时候可以覆盖索引，节省存储空间，避免回表，提高查询效率 要控制索引的数量，索引并不是多多益善，索引越多，维护索引结构的代价就越大，会影响增删改的效率 如果索引列不能存储NULL值，请在创建表时使用NOT NULL约束它。当优化器知道每列是否包含NULL值时，它可以更好地确定哪个索引最有效地用于查询 四、 sql优化 1.插入数据（insert，load）普通插入： 采用批量插入（一次插入的数据不建议超过1000条） 手动提交事务（mysql默认自动提交事务，执行3条insert就会有三次事务，可以直接start transaction (3条insert) commit） 主键顺序插入 大批量插入（load本地文件）：如果一次性需要插入大批量数据，使用insert语句插入性能较低，此时可以使用MySQL数据库提供的load指令插入。 ①客户端连接服务端时，加上参数 –local-infile（这一行在bash&#x2F;cmd界面输入）mysql --local-infile -u root -p ②设置全局参数local_infile为1，开启从本地加载文件导入数据的开关 set global local_infile = 1;select @@local_infile; ③执行load指令将准备好的数据，加载到表结构中 load data local infile &#39;/root/sql1.log&#39; into table &#39;tb_user&#39; fields terminated by &#39;,&#39; lines terminated by &#39;\\n&#39;; 2.页分裂和页合并数据组织方式：在InnoDB存储引擎中，表数据都是根据主键顺序组织存放的，这种存储方式的表称为索引组织表（Index organized table, IOT） 1）页分裂 若不按顺序排列，在后面就会涉及到一个重新排序的页分裂动作 2）页合并 当删除一行记录时，实际上记录并没有被物理删除，只是记录被标记（flaged）为删除并且它的空间变得允许被其他记录声明使用。当页中删除的记录到达 MERGE_THRESHOLD（默认为页的50%），InnoDB会开始寻找最靠近的页（前后）看看是否可以将这两个页合并以优化空间使用。 如图中14，15，16三条数据已经被标记不被使用，如果此时page2的删除达到阈值，就会尝试将右边的17，18，19合并进一个pageMERGE_THRESHOLD：合并页的阈值，可以自己设置，在创建表或创建索引时指定 文字说明不够清晰明了，具体可以看视频里的PPT演示过程：https://www.bilibili.com/video/BV1Kr4y1i7ru?p=90 个主键设计原则： 满足业务需求的情况下，尽量降低主键的长度 二级索引叶子节点挂的是主键，如果主键较长，二级索引较大会占用IO，效率变低 插入数据时，尽量选择顺序插入，选择使用 AUTO_INCREMENT 自增主键 尽量不要使用 UUID 做主键或者是其他的自然主键，如身份证号 因为这些是无序插入，会产生页分裂现象，并且长度可能较长 业务操作时，避免对主键的修改 3.order by优化 Using filesort：通过表的索引或全表扫描，读取满足条件的数据行，然后在排序缓冲区 sort buffer 中完成排序操作，所有不是通过索引直接返回排序结果的排序都叫 FileSort 排序 Using index：通过有序索引顺序扫描直接返回有序数据，这种情况即为 using index，不需要额外排序，操作效率高 如果order by字段全部使用升序排序或者降序排序，则都会走索引，但是如果一个字段升序排序，另一个字段降序排序，则不会走索引，explain的extra信息显示的是Using index, Using filesort，如果要优化掉Using filesort，则需要另外再创建一个索引，如：create index idx_user_age_phone_ad on tb_user(age asc, phone desc);，此时使用select id, age, phone from tb_user order by age asc, phone desc;会全部走索引 总结： 根据排序字段建立合适的索引，多字段排序时，也遵循最左前缀法则 order by phone，age; 在index&#x3D;age，phone的情况下是不会使用索引的，因为首先按phone排序，但是索引第一个是age 尽量使用覆盖索引 多字段排序，一个升序一个降序，此时需要注意联合索引在创建时的规则（ASC&#x2F;DESC） 如果不可避免出现filesort，大数据量排序时，可以适当增大排序缓冲区大小 sort_buffer_size（默认256k） 4.group by优化 在分组操作时，可以通过索引来提高效率 分组操作时，索引的使用也是满足最左前缀法则的 如索引为idx_user_pro_age_stat， select profession,age,coun(*) from tb_user group by profession select age where profession group by age （where 后有了profession，也算是最左） 以上两条都符合最左前缀法则 注意： explain select age,count(age) from tb_user group by age; 虽然也用到了索引，但是也用到了临时表，其实效率并不高 5.limit优化常见的问题如limit 2000000, 10，此时需要 MySQL 排序前2000000条记录，但仅仅返回2000000 - 2000010的记录，其他记录丢弃，查询排序的代价非常大。优化方案：一般分页查询时，通过创建覆盖索引能够比较好地提高性能，可以通过覆盖索引加子查询形式进行优化 例如： 12345678-- 此语句耗时很长select * from tb_sku limit 9000000, 10;-- 通过覆盖索引加快速度，直接通过主键索引进行排序及查询select id from tb_sku order by id limit 9000000, 10;-- 下面的语句是错误的，因为 MySQL 不支持 in 里面使用 limit-- select * from tb_sku where id in (select id from tb_sku order by id limit 9000000, 10);-- 通过连表查询即可实现第一句的效果，并且能达到第二句的速度select * from tb_sku as s, (select id from tb_sku order by id limit 9000000, 10) as a where s.id = a.id; 6.count优化MyISAM 引擎把一个表的总行数存在了磁盘上，因此执行 count(*) 的时候会直接返回这个数，效率很高（前提是不适用where）；InnoDB 在执行 count(*) 时，需要把数据一行一行地从引擎里面读出来，然后累计计数。优化方案：自己计数，如创建key-value表存储在内存或硬盘，或者是用redis ​ 从上面可以看出，count(1)和count(*)是没有取值的过程，所以小 7.update优化（避免行锁升级为表锁）InnoDB 的行锁是针对索引加的锁，不是针对记录加的锁，并且该索引不能失效，否则会从行锁升级为表锁。 如以下两条语句：update student set no = &#39;123&#39; where id = 1;，这句由于id有主键索引，所以只会锁这一行；update student set no = &#39;123&#39; where name = &#39;test&#39;;，这句由于name没有索引，所以会把整张表都锁住进行数据更新，解决方法是给name字段添加索引 五、视图&#x2F;存储过程&#x2F;触发器 1. 视图①语法 1）with cascaded check option v1 通过 user基表建立，v2通过v1建立，v3通过v2建立 当v2表cascaded后，它在insert数据的时候也会检查其所依赖的v1条件；v3没有cascaded，虽然它依赖了cascaded的v2，但是insert的数据检查也只会检查v1，v2的条件，并不会检查v3 2）with local check option local就是会递归的去寻找，满足v2后会看v1的option，如果没有则不会管v1的条件 3）相关事项及实例 注意： 视图的增删改操作必须是视图中的一条数据对应基表的一条数据，若使用了聚合函数、group by、distinct、having、union(all)等，视图是无法更新的 视图的作用： 简单：方便用户对数据理解，可以将经常查询的数据定义成一张视图 安全：视图能让用户只浏览和修改到他们被允许的数据（mysql的表权限中不包含columns的权限） 数据独立：屏蔽真实表结构变化带来的影响 name -&gt; stuName ​ 三表联查： ​ 2. 存储过程1）变量1）系统变量（@@） ​ session：会话变量，在另外一个会话中则不存在，未指定默认是session ​ global：全局变量，在所有会话中都存在，但是在restart mysql后还是不会改变（持久改变需要在配置文件中配置） 2）用户自定义变量（@） 3）局部变量 ​ 只在存储过程中生效 2）存储过程中的语法①if ​ ②case ​ ③while ④repeat ⑤loop ⑥cursor 游标游标类似于一个集合、迭代器，每次从游标中取一行数据 ​ eg： ⑦handler 条件处理程序类似于异常处理，根据捕获的异常码来处理 eg： 这个对游标的改进就是在，游标抛出异常02000（not found）后关闭cursor 3）存储函数 3. 触发器触发器可以在insert&#x2F;update&#x2F;delete 语句执行之后，执行触发器中定义的sql，可以完成日志记录、数据校验等工作 触发器目前只支持 行级触发：如一条语句影响了5行数据，那么触发器会被执行5次 ​ 语句级触发：如一条语句影响了多行数据，但是触发器只会触发一次 1）语法 2）案例 eg1：insert案例 eg2：update案例 eg3：delete案例 六、锁 1. 全局锁1）基本概念全局锁就是对整个数据库实例加锁，加锁后整个实例就处于只读状态，后续的DML的写语句，DDL语句，已经更新操作的事务提交语句都将被阻塞。其典型的使用场景是做全库的逻辑备份，对所有的表进行锁定，从而获取一致性视图，保证数据的完整性。 全局锁是对整个数据库加锁，加锁后可以查询DQL，但是DDL和DML语句都不能执行，主要用于备份： ①备份不加锁 假设在数据库中存在这样三张表: tb_stock 库存表，tb_order 订单表，tb_orderlog 订单日志表。 在进行数据备份时，先备份了tb_stock库存表。 然后接下来，在业务系统中，执行了下单操作，扣减库存，生成订单（更新tb_stock表，插入tb_order表）。 然后再执行备份 tb_order表的逻辑。 业务中执行插入订单日志操作。 最后，又备份了tb_orderlog表。 此时备份出来的数据，是存在问题的。因为备份出来的数据，tb_stock表与tb_order表的数据不一致（有新的订单信息,但是库存数没减)，此时就可以借助于MySQL的全局锁来解决。 ②备份加锁 加了全局锁后的情况： 对数据库进行进行逻辑备份之前，先对整个数据库加上全局锁，一旦加了全局锁之后，其他的DDL、DML全部都处于阻塞状态，但是可以执行DQL语句，也就是处于只读状态，而数据备份就是查询操作。那么数据在进行逻辑备份的过程中，数据库中的数据就是不会发生变化的，这样就保证了数据的一致性和完整性。 2）使用123456789# 加全局锁flush tables with read lock ;# 数据备份 这并不是mysql的命令，而是mysql提供的一个工具# xxx代表被封的dataBase名，yyy代表备份的地址，备份文件一般以.sql结尾mysqldump -uroot –p1234 xxx &gt; yyy.sql# 释放锁unlock tables ; 3）特点数据库中加全局锁，是一个比较重的操作，存在以下问题： 如果在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆。 如果在从库上备份，那么在备份期间从库不能执行主库同步过来的二进制日志（binlog），会导致主从延迟。 在InnoDB引擎中，我们可以在备份时加上参数 --single-transaction 参数来完成不加锁的一致性数据备份。 123# 数据备份 这并不是mysql的命令，而是mysql提供的一个工具# xxx代表被封的dataBase名，yyy代表备份的地址，备份文件一般以.sql结尾mysqldump --single-transaction -uroot –p123456 xxx &gt; yyy.sqlxxx &gt; itcast.sql 2. 表级锁1）表锁 读锁（表共享读锁）：读锁本会话可读不可写，其他会话也是可读不可写（阻塞其它客户端的写） 写锁（表独占写锁）：写锁本会话可读可写，其他会话不可读不可写（会被阻塞，释放锁以后会继续执行） 2）元数据锁 MDL加锁过程是系统自动控制，无需显式使用，在访问一张表的时候会自动加上 MDL锁主要作用是维护表元数据的数据一致性（表结构），在表上有活动事务的时候，不可以对元数据进行写入操作 在My5QL5.5中引入了MDL，当对一张表进行增删改查的时候，加MDL读锁(共享) 当对表结构进行变更操作的时候，加MDL写锁(排他) 事务A在做select查询并且事务没结束，那么这时候通过alter改变表结构（元数据）是不被允许的 3）意向锁意向锁的作用，一个例子： ​ 当线程A中的事务中执行update语句的时候（根据id索引）会锁住这一行的数据，如果此时有另外一个线程要为该表加read&#x2F;write lock，那么这个线程有需要一条一条的检查看是否有行锁，这样效率太低了 ​ 为解决这一问题，在事务中执行sql语句的时候，mysql除了添加行锁，还会为整张表添加一个意向锁，当其他线程要为该表加锁的时候，就可以根据所加的意向锁判断是否能加锁，而不需要一条一条检查数据 select …. lock in share mode 会添加意向共享锁，可以与表共享读锁（read）兼容，与表独占写锁（write）互斥（会发生阻塞） 增删改、查询for update 会添加意向排他锁，与read和write都互斥 意向锁直接都是兼容的，意向共享锁和read锁兼容和write锁互斥，意向排他锁与read锁和write都互锁 3. 行级锁 1）行锁行锁有两种：s（共享锁）、x（排他锁） 总结： 增删改操作都是加排他锁 普通select不加锁 select语句后加lock in share mode 共享锁，加for update 加排他锁 注意： mysql的行级锁是针对索引的锁，锁住的是聚集索引叶子结点的一个数据，若在增删改的时候没有用到索引，那么行锁就会升级成为表锁 查询元数据锁和行锁的语句 select object_schema,object_name,index_name,lock_type,lock_mode,lock_data fromperformance_schema.data_locks; 2）间隙锁、临键锁 默认情况下，使用的是next-key临键锁来进行搜索和索引扫描，当某些情况下临键锁可以优化成间隙锁： ①如当事务A用主键搜寻不存在的数据的时候：123begin;update user set name = &#x27;ykk&#x27; where id = 4; 此时A事务未提交，事务B开启： 12345begin;insert into user (id, username, no) value (4,&#x27;ydd&#x27;,200100105);commit; #无法提交，卡主，需要等事务A解开间隙锁才行 事务A我们想update一条id&#x3D;4的数据，第一次查询id&#x3D;4发现没有数据，这时候临键锁优化为间隙锁，将3与5的间隙锁起来（B+树结构中，这两个row是相邻的），那么事务B就不能在3和5间插入id&#x3D;4的事务，那么事务A在插入id&#x3D;4的数据的时候就不会出现幻读的现象 ②因为是非唯一索引，所以会出现重复值 由于是非唯一，为了防止其它事务在15之间的间隙插入数据造成幻读，那么需要将间隙和值都锁上，一直到最后一个不匹配的row ③范围查询锁到不满足要求为止如select * from user where id&gt;&#x3D;19，那么锁就会从19的row开始一直锁到正无穷大，其中的键和间隙都会被锁住 七、InnoDB引擎1. 架构图1）逻辑结构 ①表空间 表空间是InnoDB存储引擎逻辑结构的最高层， 如果用户启用了参数 innodb_file_per_table(在8.0版本中默认开启) ，则每张表都会有一个表空间（xxx.ibd），一个mysql实例可以对应多个表空间，用于存储记录、索引等数据。 ②段 段，分为数据段（Leaf node segment）、索引段（Non-leaf node segment）、回滚段（Rollback segment），InnoDB是索引组织表，数据段就是B+树的叶子节点， 索引段即为B+树的非叶子节点。段用来管理多个Extent（区）。 ③区 区，表空间的单元结构，每个区的大小为1M。 默认情况下， InnoDB存储引擎页大小为16K， 即一个区中一共有 64 个连续的页。 ④页 页，是InnoDB 存储引擎磁盘管理的最小单元，每个页的大小默认为 16KB。为了保证页的连续性，InnoDB 存储引擎每次从磁盘申请 4-5 个区。 ⑤行 行，InnoDB 存储引擎数据是按行进行存放的。 在行中，默认有两个隐藏字段： Trx_id：每次对某条记录进行改动时，都会把对应的事务id赋值给trx_id隐藏列。 Roll_pointer：每次对某条引记录进行改动时，都会把旧的版本写入到undo日志中，然后这个隐藏列就相当于一个指针，可以通过它来找到该记录修改前的信息 2）物理结构 2. 事务原理 ​ MVCC（无锁）+锁保证了事务的隔离性，redo log 保证了事务的持久性，undo log 保证了事务的原子性，redo log 和 undo log 一起保证了事务的一致性。 1）redo log​ redo log保证了事务的持久性 问题情境： ​ InnoDB内存结构中，主要的内存区就是缓冲池，缓冲池中缓冲了很多的数据页。当我们在一个事务中执行增删改操作的时候，InnoDB会直接操作缓冲池中的数据，将缓冲池中的数据修改，如果缓冲池中没有对应的页，那么就会通过后台线程调入对应需要操作的数据。在缓冲区中被修改过的数据页称为脏页，脏页会在一定的时机，通过后台线程刷新到磁盘中，从而保证缓冲区中数据和磁盘中的数据一致。因此缓冲区的脏页数据并不是实时刷新的，若过一段时间后在从缓冲区向磁盘刷新数据的过程中出错了，但提示给用户事务提交成功，但是数据却没持久化下来。 解决办法redo log： ​ 有了redo log 后，在对缓冲区的数据进行增删改操作后，会首先将数据页的变化记录在redo log buffer中，在事务提交时会将redo log buffer中的数据刷新到redo log磁盘文件中。如果脏页数据出错，此时就可以借助于redo log来进行数据恢复，以此来保护数据持久性。若 脏页数据已经成功刷新到磁盘 或 涉及到的数据已经落盘，那么redo log就没用了，可以删除了，所以存在的两个redo log文件是循环写的（一个没用了删掉用另一个） 总结： redo log 有两部分 内存：redo log buffer 在事务提交后马上刷新给磁盘中的redo log 磁盘：两个redo log 循环写 为什么不能直接将脏页数据实时刷新，而是刷新redo log buffer中的数据？ 因为操作的数据页地址是随机的，如果实时刷新脏数据，那么就是随机IO（随机读写磁盘），性能低 redo log的刷新是顺序IO（日志按顺序写的），数据效率高 这种机制叫WAL(Write-Ahead-Logging) redo log其实就是为了脏页刷新发生错误的时候进行数据恢复 2）undo log​ undo log 保证了事务的原子性 ​ undo log 主要是为了 回滚事务 和 MVCC，不同于redo log 是物理日志，undo log 是逻辑日志： ​ 当在事务中执行了一条insert语句后，undo log就会记录一条相反逻辑的delete语句，执行一条update语句后，undo log就会记录一条相反逻辑的update语句 undo log 的销毁： 在执行完事务后（commit 或 rollback后），并不会马上删除，因为MVCC还可能用到undo log undo log 的存储：undo log 通过段segment的方式存储，（segments：数据段—B+树叶子节点，索引段—非叶子节点，回滚段—undo log） 段，分为数据段（Leaf node segment）、索引段（Non-leaf node segment）、回滚段（Rollback segment），InnoDB是索引组织表，数据段就是B+树的叶子节点， 索引段即为B+树的非叶子节点。段用来管理多个Extent（区） 两个作用： 回滚：回滚是在事务结束的时候（commit或rollback），如果选择回滚，那么mysql会执行undo log中的逻辑记录 MVCC：undo log 中记录了版本链，版本链配合readview来完成MVCC版本控制 3）MVCC MVCC的实现原理就是通过InnoDB表的隐藏字段、UndoLog 版本链、Readview来实现的。而MVCC＋锁，则实现了事务的隔离性。而一致性则是由redolog 与undolog保证。 MVCC是在快照读的时候通过事务id，undo log 版本链，ReadView来查找返回的历史版本数据。 MVCC就是通过表中的隐藏字段事务id，undo log 版本链，ReadView（事务确定）三者来实现的： 查询的时候会根据当前的事务活动情况和事务id来确定ReadView，然后再从undo log版本链头部开始通过查看事务id是否满足ReadView的匹配条件，满足条件则返回该版本 ①当前读和快照读a）当前读读取的是记录的最新版本，读取时还要保证其他并发事务不能修改当前记录，会对读取的记录进行加锁。 对于我们日常的操作如: select … lock in share mode (共享锁) select …for update、update、insert、delete(排他锁)都是一种当前读。 当前读读取的是记录的最新版本，读取的需要保证其他并发事务，所以会对读取的数据加锁 eg： ​ 当A事务开启后，①的查询结果id&#x3D;2是PHP，此时开启B事务并执行修改id&#x3D;2为JSP数据操作，再次通过事务A的①查，查不出该次变化，因为当前隔离级别是可重复读，保证了两次读取事务的一致性；如果读取到当前事务的最新数据，可以使用②select …. lock in share mode 来进行当前读，这样读取到的就是最新的数据id&#x3D;2是JSP。 b）快照读简单的select(不加锁)就是快照读，快照读，读取的是记录数据的可见版本，有可能是历史数据，不加锁，是非阻塞读。 Read Committed：每次select，都生成一个快照读。 Repeatable Read：开启事务后第一个select语句才是快照读的地方。 serializable：快照读会退化为当前读。 ​ 快照读是根据一定规则MVCC读取记录数据的可见版本，有可能是历史数据，这个过程是不加锁的 * 读已提交：每次的select都是一个快照读 * 可重复读：开启事务后的第一个select是快照读，后续的select只是直接使用第一个快照读的数据（查询的实际上是第一个快照数据） * 串行化：快照读退化成当前读 ②隐藏字段 ​ 每一张表都会有两个隐藏字段：最近事务id、回滚指针，如果该表结构未指定主键，那么会生成隐藏字段row_id。 ③undo logundo log 是回滚日志，在insert、update、delete的时候会产生回滚数据来产生： 当insert的时候，数据只需要回滚的时候使用，所以commit后（证明没有rollback）可以直接删除 update、delete，数据不仅在回滚的时候需要使用，在快照读的时候也需要，所以不能删除 https://www.bilibili.com/video/BV1Kr4y1i7ru?p=143&amp;spm_id_from=pageDriver undo log 版本链： 每当要修改一条记录的时候，就会在undo log存储该版本（包括事务id，回滚指针指向上个版本），最新记录的回滚指针指向该版本 版本之间通过回滚指针连接，头部是最新记录，尾部是最旧记录 ④readview(读视图）readview是每个事务独有的，主要内容有两部分：有四个字段、定义了 快照读sql访问版本链的规则 a）字段 注意max_trx_id 并不是目前最大id，而是最大事务id+1（下一个分配的id） b）定义的访问undo log 版本链的规则 trx_id 代表的当前版本链中隐藏字段中的 事务id 注意：不同的隔离级别，会在不同的时机生成readview来进行快照读 读已提交 会在每次select时都生成一个readview来作为此次快照读的依据（读已提交每次select都是一个快照读） 可重复读 会在第一次select时生成一个readview来作为本次快照读的依据（可重复读只有第一次select是快照读，后序select都是复用该次数据） ⑤RC和RR的MVCCa）RC 隔离级别 PC的每次select都是一个快照读，所以事务5的两次select会有两个ReadView ​ MVCC就是在快照读的时候，根据该次select产生的readView，将undo log 的版本链中从头开始 通过每条数据中隐藏字段中的事务id与readView指定的规则做匹配，如果匹配了则返回该条版本记录。 b）RR 隔离级别 ​ 在RR隔离级别下，只有在第一个select快照读的时候会生成ReadView，后序的select都是复用该ReadView，所以可以保证多次select的数据是一致的。 八、高可用1.日志1）错误日志错误日志是 MySQL 中最重要的日志之一，它记录了当 mysqld 启动和停止时，以及服务器在运行过程中发生任何严重错误时的相关信息。当数据库出现任何故障导致无法正常使用时，建议首先查看此日志。 该日志是默认开启的，默认存放目录 &#x2F;var&#x2F;log&#x2F;，默认的日志文件名为 mysqld.log 。查看日志位置： 1show variables like &#x27;%log_error%&#x27;; 2）二进制日志①基本概念二进制日志（BINLOG）记录了所有的 DDL（数据定义语言）语句和 DML（数据操纵语言）语句，但不包括DQL数据查询（SELECT、SHOW）语句。 作用：①灾难时的数据恢复 ②MySQL的主从复制 在MySQL8版本中，默认二进制日志是开启着的，涉及到的参数如下： 1show variables like &#x27;%log_bin%&#x27;; 参数说明： og_bin_basename：当前数据库服务器的binlog日志的基础名称(前缀)，具体的binlog文件名需要再该basename的基础上加上编号(编号从 000001 开始)。 log_bin_index：binlog的索引文件，里面记录了当前服务器关联的binlog文件有哪些。 因为主要是为了恢复数据，所以记录的都是DDL和DML，DQL都是查询，和恢复数据没关系。 ②binlog格式MySQL服务器中提供了多种格式来记录二进制日志，具体格式及特点如下： 日志格式 含义 STATEMENT 基于SQL语句的日志记录，记录的是SQL语句，对数据进行修改的SQL都会记录在日志文件中。 ROW 基于行的日志记录，记录的是每一行的数据变更。（默认） MIXED 混合了STATEMENT和ROW两种格式，默认采用STATEMENT，在某些特殊情况下会自动切换为ROW进行记录。 statement就是记录一条条sql语句，也就是DDL和DML row记录的是每一行的变更，如一条update语句影响了5行数据，那么row模式就会记录这5行的数据变更 查看当前mysql的binlog格式： 1show variables like &#x27;%binlog_format%&#x27;; 如果我们需要配置二进制日志的格式，只需要在 &#x2F;etc&#x2F;my.cnf 中配置 binlog_format 参数即可 ③binlog的查看由于日志是以二进制方式存储的，不能直接读取，需要通过二进制日志查询工具 mysqlbinlog 来查看，具体语法： 1234567mysqlbinlog [ 参数选项 ] logfilename参数选项：-d 指定数据库名称，只列出指定的数据库相关操作。-o 忽略掉日志中的前n行命令。-v 将行事件(数据变更)重构为SQL语句-vv 将行事件(数据变更)重构为SQL语句，并输出注释信息，注意是v v 由于默认是row记录的，不方便查看，可以先转换成SQL语句的格式 ④binlog的删除对于比较繁忙的业务系统，每天生成的binlog数据巨大，如果长时间不清除，将会占用大量磁盘空间。可以通过以下几种方式清理日志： 指令 含义 reset master 删除全部 binlog 日志，删除之后，日志编号，将从 binlog.000001重新开始 purge master logs to ‘binlog.*’ 删除 * 编号之前的所有日志 purge master logs before ‘yyyy-mm-dd hh24:mi:ss’ 删除日志为 “yyyy-mm-dd hh24:mi:ss” 之前产生的所有日志 也可以在mysql的配置文件中配置二进制日志的过期时间，设置了之后，二进制日志过期会自动删除。 1show variables like &#x27;%binlog_expire_logs_seconds%&#x27;; #默认是30天 3）查询日志询日志中记录了客户端的所有操作语句，而二进制日志不包含查询数据的SQL语句。因为记录了所有的操作语句，所以查询日志会非常大，所以查询日志默认情况下是未开启的。 查询日志的开启状态和存放地址： 1show variables like &#x27;%general%&#x27;; #默认是30天 如果需要开启查询日志，可以修改MySQL的配置文件 &#x2F;etc&#x2F;my.cnf 文件，添加如下内容： 1234# 该选项用来开启查询日志 可选值：0(代表关闭) 或者 1(代表开启) general_log= 1# 设置日志的文件名 ， 如果没有指定， 默认的文件名为 host_name.loggeneral_log_file=mysql_query.log 开启了查询日志之后，在MySQL的数据存放目录，也就是 &#x2F;var&#x2F;lib&#x2F;mysql&#x2F; 目录下就会出现mysql_query.log 文件。之后所有的客户端的增删改查操作都会记录在该日志文件之中，长时间运行后，该日志文件将会非常大。 4）慢查询日志慢查询日志记录了所有执行时间超过参数long_query_time设置值并且扫描记录数不小于min_examined_row_limit的所有的SQL语句的日志，默认未开启。long_query_time默认为10 秒，最小为 0 ， 精度可以到微秒。 如果需要开启慢查询日志，需要在MySQL的配置文件 &#x2F;etc&#x2F;my.cnf 中配置如下参数： 1234# 开启慢查询日志slow_query_log= 1# 执行时间参数long_query_time= 2 默认情况下，不会记录管理语句，也不会记录不使用索引进行查找的查询。可以使用log_slow_admin_statements和更改此行为 log_queries_not_using_indexes，入下： 1234# 记录执行较慢的管理语句log_slow_admin_statements = 1# 记录执行较慢的未使用索引的语句log_queries_not_using_indexes = 1 2.主从复制1）基本概念 主从复制是指将主数据库的 DDL 和 DML 操作通过二进制日志binlog传到从库服务器中，然后在从库上对这些日志重新执行（也叫重做），从而使得从库和主库的数据保持同步。 MySQL支持一台主库同时向多台从库进行复制， 从库同时也可以作为其他从服务器的主库，实现链状复制 主库A有从库B，从库B通过主库A的binlog同步，从库B也可以作为从库C的主库，从库C同步的是从库B的数据 MySQL 复制的优点主要包含以下三个方面： 主库出现问题，可以快速切换到从库提供服务。 实现读写分离，降低主库的访问压力。 可以在从库中执行备份，以避免备份期间影响主库服务。 从库本来就是只接受查询，所以加上全局锁后进行磁盘备份也不会影响查询的请求 2）原理 MySQL主从复制的核心就是 二进制日志，具体的过程如下： Master 主库在事务提交时，会把数据变更记录在二进制日志文件 Binlog 中。 IO线程读：从库的取主库的二进制日志文件 Binlog ，写入到从库的中继日志 Relay Log 。 SQL线程：slave重做中继日志中的事件，复制完成。 3）搭建①基本组成 需要暴露对应的端口，或者直接关闭防火墙 ②主库 修改配置文件 &#x2F;etc&#x2F;my.cnf 12345678# mysql 服务ID，保证整个集群环境中唯一，取值范围：1 – 232-1，默认为 1server-id= 1# 是否只读,1 代表只读, 0 代表读写read-only= 0# 忽略的数据, 指不需要同步的数据库# binlog-ignore-db=mysql# 指定同步的数据库# binlog-do-db=db 重启MySQL服务器 1systemctl restart mysqld 登录mysql，创建远程连接的账号，并授予主从复制权限 12345#创建itcast用户，并设置密码，该用户可在任意主机连接该MySQL服务CREATE USER &#x27;itcast&#x27;@&#x27;%&#x27; IDENTIFIED WITH mysql_native_password BY &#x27;Root@123456&#x27;;#为 &#x27;itcast&#x27;@&#x27;%&#x27; 用户分配主从复制权限GRANT REPLICATION SLAVE ON *.* TO &#x27;itcast&#x27;@&#x27;%&#x27;; 通过指令，查看二进制日志坐标 1show master status; 字段含义说明： file: 从哪个日志文件开始推送日志文件 position： 从哪个位置开始推送日志，即开始同步的位置 binlog_ignore_db: 指定不需要同步的数据库 ②从库 修改配置文件 &#x2F;etc&#x2F;my.cnf 1234# mysql 服务ID，保证整个集群环境中唯一，取值范围：1 – 2^32-1，和主库不一样即可server-id= 2# 是否只读,1 代表只读, 0 代表读写read-only= 1 尽管从库设置的是只读，但是super用户还是有权限来进行写操作，可以通过以下命令关闭 super-read-only=1，设置超级管理员也只能进行读操作 重新启动MySQL服务 1systemctl restart mysqld 登录mysql，设置主库配置 123CHANGE REPLICATION SOURCE TO SOURCE_HOST=&#x27;192.168.200.200&#x27;, SOURCE_USER=&#x27;itcast&#x27;,SOURCE_PASSWORD=&#x27;Root@123456&#x27;, SOURCE_LOG_FILE=&#x27;binlog.000004&#x27;,SOURCE_LOG_POS= 663 ; 参数 SOURCE_HOST：主库地址 SOURCE_USER：从主库进行主从复制的账户 SOURCE_PASSWORD：账户密码 SOURCE_LOG_FILE：从主库哪一个binlog开始复制，在主库中执行了show master status;查看到的 SOURCE_LOG_POS：从binlog哪个位置开始复制 上述是8.0.23中的语法（兼容了8.0.23 之前的表述）。如果mysql是 8.0.23 之前的版本，执行如下SQL： 123CHANGE MASTER TO MASTER_HOST=&#x27;192.168.200.200&#x27;, MASTER_USER=&#x27;itcast&#x27;,MASTER_PASSWORD=&#x27;Root@123456&#x27;, MASTER_LOG_FILE=&#x27;binlog.000004&#x27;,MASTER_LOG_POS= 663 ; 参数版本对比 参数名 含义 8.0.23之前 SOURCE_HOST 主库IP地址 MASTER_HOST SOURCE_USER 连接主库的用户名 MASTER_USER SOURCE_PASSWORD 连接主库的密码 MASTER_PASSWORD SOURCE_LOG_FILE binlog 起始logbin日志文件名 MASTER_LOG_FILE SOURCE_LOG_POS binlog 日志文件位置 MASTER_LOG_POS 开启同步操作 12start replica ; #8.0.22之后start slave ; #8.0.22之前 查看主从同步状态 12show replica status ; #8.0.22之后show slave status ; #8.0.22之前 3.分库分表1）基本概念 随着互联网及移动互联网的发展，应用系统的数据量也是成指数式增长，若采用单数据库进行数据存储，存在以下性能瓶颈： IO瓶颈 热点数据太多，数据库缓存Buffer pool不足，产生大量磁盘IO，效率较低。 请求数据太多，带宽不够，网络IO瓶颈。 CPU瓶颈 排序、分组、连接查询、聚合统计等SQL会耗费大量的CPU资源，请求数太多，CPU出现瓶颈。 为了解决上述问题，我们需要对数据库进行分库分表处理： 分库分表的中心思想都是将数据分散存储，使得单一数据库&#x2F;表的数据量变小来缓解单一数据库的性能问题，从而达到提升数据库性能的目的。 2）分片策略①基本概念 分库、分表是粒度上的控制 垂直和水平 在业务系统中，为了缓解磁盘IO及CPU的性能瓶颈，到底是垂直拆分，还是水平拆分；具体是分库，还是分表，都需要根据具体的业务需求具体分析 ②垂直分库 以库为单位，有用户user、订单order、库存sku三部分数据，可以将三部分数据分散到3个主机中 以表为依据，根据业务将不同表拆分到不同库中： 每个库的表结构都不一样 每个库的数据也不一样 所有库的并集是全量数据 ③垂直分表 以表为单位，将sku表按特性分类，比如部分sku信息和sku描述 以字段为依据，根据字段属性将不同字段拆分到不同表中： 每个表的结构都不一样 每个表的数据也不一样，一般通过一列（主键&#x2F;外键）关联 所有表的并集是全量数据 ④水平分库 将数据库的表分散到多台主机，每台主机的表存部分数据（类似slots） 以字段为依据，按照一定策略，将一个库的数据拆分到多个库中： 每个库的表结构都一样 每个库的数据都不一样 所有库的并集是全量数据 ⑤水平分表 其实就是以表的角度看水平分库 水平分表：以字段为依据，按照一定策略，将一个表的数据拆分到多个表中。特点： 每个表的表结构都一样 每个表的数据都不一样 所有表的并集是全量数据 ⑥实现技术 shardingJDBC：基于AOP原理，在应用程序中对本地执行的SQL进行拦截，解析、改写、路由处理。需要自行编码配置实现，只支持java语言，性能较高 MyCat：数据库分库分表中间件，不用调整代码即可实现分库分表，支持多种语言，性能不及前者 3）Mycat的概述①基本概念 Mycat是开源的、活跃的、基于Java语言编写的MySQL数据库中间件。可以像使用mysql一样来使用mycat，对于开发人员来说根本感觉不到mycat的存在 开发人员只需要连接MyCat即可，而具体底层用到几台数据库，每一台数据库服务器里面存储了什么数据，都无需关心。 具体的分库分表的策略，只需要在MyCat中配置即可 这就意味着对我们来说就正常进行数据库操作，具体的分发策略交给mycat处理 优势： 性能可靠稳定 强大的技术团队 体系完善 社区活跃 ②目录结构 bin : 存放可执行文件，用于启动停止mycat conf：存放mycat的配置文件 lib：存放mycat的项目依赖包（jar） logs：存放mycat的日志文件 ③基本结构 在MyCat的整体结构中，分为两个部分：上面的逻辑结构、下面的物理结构： 逻辑结构 逻辑结构主要负责逻辑库、逻辑表、分片规则、分片节点等逻辑结构的处理 物理结构： 具体的数据存储还是在物理结构，也就是数据库服务器中存储的 4）Mycat example①需求由于 tb_order 表中数据量很大，磁盘IO及容量都到达了瓶颈，现在需要对 tb_order 表进行数据分片，分为三个数据节点，每一个节点主机位于不同的服务器上, 具体的结构，参考下图： ②环境准备准备 3 台服务器： 192.168.200.210：MyCat中间件服务器，同时也是第一个分片服务器。 192.168.200.213：第二个分片服务器。 192.168.200.214：第三个分片服务器。 并且在上述 3 台数据库中创建数据库 db01 ③配置schema.xml 在schema.xml中配置逻辑库、逻辑表、数据节点、节点主机等相关信息。具体的配置如下： 12345678910111213141516171819202122232425262728293031&lt;?xml version=&quot;1.0&quot;?&gt;&lt;!DOCTYPE mycat:schema SYSTEM &quot;schema.dtd&quot;&gt;&lt;!--逻辑库，逻辑表--&gt;&lt;mycat:schema xmlns:mycat=&quot;http://io.mycat/&quot;&gt; &lt;schema name=&quot;DB01&quot; checkSQLschema=&quot;true&quot; sqlMaxLimit=&quot;100&quot;&gt; &lt;table name=&quot;TB_ORDER&quot; dataNode=&quot;dn1,dn2,dn3&quot; rule=&quot;auto-sharding-long&quot;/&gt;&lt;/schema&gt;&lt;!--逻辑表及节点关系--&gt;&lt;dataNode name=&quot;dn1&quot; dataHost=&quot;dhost1&quot; database=&quot;db01&quot; /&gt;&lt;dataNode name=&quot;dn2&quot; dataHost=&quot;dhost2&quot; database=&quot;db01&quot; /&gt;&lt;dataNode name=&quot;dn3&quot; dataHost=&quot;dhost3&quot; database=&quot;db01&quot; /&gt;&lt;!--节点主机配置--&gt;&lt;dataHost name=&quot;dhost1&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot;slaveThreshold=&quot;100&quot;&gt; &lt;heartbeat&gt;select user()&lt;/heartbeat&gt; &lt;writeHost host=&quot;master&quot;url=&quot;jdbc:mysql://192.168.200.210:3306useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot;user=&quot;root&quot; password=&quot;1234&quot; /&gt;&lt;/dataHost&gt;&lt;dataHost name=&quot;dhost2&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot; writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot; slaveThreshold=&quot;100&quot;&gt; &lt;heartbeat&gt;select user()&lt;/heartbeat&gt; &lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.200.213:3306useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot; user=&quot;root&quot; password=&quot;1234&quot; /&gt;&lt;/dataHost&gt;&lt;dataHost name=&quot;dhost3&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot; slaveThreshold=&quot;100&quot;&gt; &lt;heartbeat&gt;select user()&lt;/heartbeat&gt; &lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.200.214:3306useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot; user=&quot;root&quot; password=&quot;1234&quot; /&gt;&lt;/dataHost&gt;&lt;/mycat:schema&gt; server.xml 需要在server.xml中配置用户名、密码，以及用户的访问权限信息，具体的配置如下： 123456789101112131415161718&lt;user name=&quot;root&quot; defaultAccount=&quot;true&quot;&gt; &lt;property name=&quot;password&quot;&gt; 123456 &lt;/property&gt; &lt;property name=&quot;schemas&quot;&gt;DB01&lt;/property&gt;&lt;!--能够访问的数据库--&gt; &lt;!-- 表级 DML 权限设置 --&gt; &lt;!-- &lt;privileges check=&quot;true&quot;&gt; &lt;schema name=&quot;DB01&quot; dml=&quot;0110&quot; &gt; &lt;table name=&quot;TB_ORDER&quot; dml=&quot;1110&quot;&gt;&lt;/table&gt; &lt;/schema&gt; &lt;/privileges&gt; --&gt;&lt;/user&gt;&lt;user name=&quot;user&quot;&gt; &lt;property name=&quot;password&quot;&gt; 123456 &lt;/property&gt; &lt;property name=&quot;schemas&quot;&gt;DB01&lt;/property&gt; &lt;property name=&quot;readOnly&quot;&gt;true&lt;/property&gt;&lt;/user&gt; ④使用启动配置完毕后，先启动涉及到的 3 台分片服务器，然后启动MyCat服务器。切换到Mycat的安装目录，执行如下指令，启动Mycat：Mycat启动之后，占用端口号 8066 1234#启动bin/mycat start#停止bin/mycat stop 启动完毕之后，可以查看logs目录下的启动日志，查看Mycat是否启动完成。 连接MyCat通过如下指令，就可以连接并登陆MyCat。 1mysql -h 192.168.200.210 -P 8066 -uroot -p 我们看到我们是通过MySQL的指令来连接的MyCat，因为MyCat在底层实际上是模拟了MySQL的协议。 数据测试然后就可以在MyCat中来创建表，并往表结构中插入数据，查看数据在MySQL中的分布情况： 123456789101112131415161718CREATE TABLE TB_ORDER (id BIGINT( 20 ) NOT NULL,title VARCHAR( 100 ) NOT NULL ,PRIMARY KEY (id)) ENGINE=INNODB DEFAULT CHARSET=utf8 ;INSERT INTO TB_ORDER(id,title) VALUES( 1 ,&#x27;goods1&#x27;);INSERT INTO TB_ORDER(id,title) VALUES( 2 ,&#x27;goods2&#x27;);INSERT INTO TB_ORDER(id,title) VALUES( 3 ,&#x27;goods3&#x27;);INSERT INTO TB_ORDER(id,title) VALUES( 1 ,&#x27;goods1&#x27;);INSERT INTO TB_ORDER(id,title) VALUES( 2 ,&#x27;goods2&#x27;);INSERT INTO TB_ORDER(id,title) VALUES( 3 ,&#x27;goods3&#x27;);INSERT INTO TB_ORDER(id,title) VALUES( 5000000 ,&#x27;goods5000000&#x27;);INSERT INTO TB_ORDER(id,title) VALUES( 10000000 ,&#x27;goods10000000&#x27;);INSERT INTO TB_ORDER(id,title) VALUES( 10000001 ,&#x27;goods10000001&#x27;);INSERT INTO TB_ORDER(id,title) VALUES( 15000000 ,&#x27;goods15000000&#x27;);INSERT INTO TB_ORDER(id,title) VALUES( 15000001 ,&#x27;goods15000001&#x27;); 经过测试，我们发现，在往 TB_ORDER 表中插入数据时： 如果id的值在1-500w之间，数据将会存储在第一个分片数据库中。 如果id的值在500w-1000w之间，数据将会存储在第二个分片数据库中。 如果id的值在1000w-1500w之间，数据将会存储在第三个分片数据库中。 如果id的值超出1500w，在插入数据时，将会报错。 这是跟之前xml配置中的分片策略有关 5）Mycat 配置①schema.xmlschema.xml作为MyCat中最重要的配置文件之一 , 涵盖了MyCat的逻辑库 、 逻辑表 、 分片规则、分片节点及数据源的配置。 主要包含以下三组标签： schema标签 datanode标签 datahost标签 a）schema 标签schema 定义逻辑库 schema 标签用于定义 MyCat实例中的逻辑库 , 一个MyCat实例中, 可以有多个逻辑库 , 可以通过 schema 标签来划分不同的逻辑库。MyCat中的逻辑库的概念，等同于MySQL中的database概念, 需要操作某个逻辑库下的表时, 也需要切换逻辑库(use xxx)。 核心属性： name：指定自定义的逻辑库库名 checkSQLschema：在SQL语句操作时指定了数据库名称，执行时是否自动去除；true：自动去除，false：不自动去除 sqlMaxLimit：如果未指定limit进行查询，列表查询模式查询多少条记录 schema 中的table定义逻辑表 table 标签定义了MyCat中逻辑库schema下的逻辑表 , 所有需要拆分的表都需要在table标签中定义 。 核心属性： name：定义逻辑表表名，在该逻辑库下唯一 dataNode：定义逻辑表所属的dataNode，该属性需要与dataNode标签中name对应；多个 dataNode逗号分隔 rule：分片规则的名字，分片规则名字是在rule.xml中定义的 primaryKey：逻辑表对应真实表的主键 type：逻辑表的类型，目前逻辑表只有全局表和普通表，如果未配置，就是普通表；全局表，配置为 global b）datanode标签 核心属性： name：定义数据节点名称 dataHost：数据库实例主机名称，引用自 dataHost 标签中name属性 database：定义分片所属数据库 c）datahost标签 该标签在MyCat逻辑库中作为底层标签存在, 直接定义了具体的数据库实例、读写分离、心跳语句。核心属性： name：唯一标识，供上层标签使用 maxCon&#x2F;minCon：最大连接数&#x2F;最小连接数 balance：负载均衡策略，取值 0,1,2,3 writeType：写操作分发方式（ 0 ：写操作转发到第一个writeHost，第一个挂了，切换到第二个； 1 ：写操作随机分发到配置的writeHost） dbDriver：数据库驱动，支持 native、jdbc ②rule.xmlrule.xml中定义所有拆分表的规则, 在使用过程中可以灵活的使用分片算法, 或者对同一个分片算法使用不同的参数, 它让分片过程可配置化。主要包含两类标签：tableRule、Function： ③server.xmlserver.xml配置文件包含了MyCat的系统配置信息，主要有两个重要的标签：system、user： a）system标签 主要配置MyCat中的系统配置信息，对应的系统配置项及其含义，如下： 属性 取值 含义 charset utf8 设置Mycat的字符集, 字符集需要与MySQL的字符集保持一致 nonePasswordLogin 0,1 0为需要密码登陆、1为不需要密码登陆 ,默认为0，设置为1则需要指定默认账户 useHandshakeV10 0,1 使用该选项主要的目的是为了能够兼容高版本的jdbc驱动, 是否采用HandshakeV10Packet来与client进行通信, 1:是, 0:否 useSqlStat 0,1 开启SQL实时统计, 1 为开启 , 0 为关闭 ; 开启之后, MyCat会自动统计SQL语句的执行情况 ; mysql -h 127.0.0.1 -P 9066 -u root -p 查看MyCat执行的SQL, 执行效率比较低的SQL , SQL的整体执行情况、读写比例等 ; show @@sql ; show @@sql.slow ; show @@sql.sum ; useGlobleTableCheck 0,1 是否开启全局表的一致性检测。1为开启 ，0为关闭 。 sqlExecuteTimeout 1000 SQL语句执行的超时时间 , 单位为 s ; sequnceHandlerType 0,1,2 用来指定Mycat全局序列类型，0 为本地文件，1 为数据库方式，2 为时间戳列方式，默认使用本地文件方式，文件方式主要用于测试 sequnceHandlerPattern 正则表达式 必须带有MYCATSEQ或者 mycatseq进入序列匹配流程 注意MYCATSEQ_有空格的情况 subqueryRelationshipCheck true,false 子查询中存在关联查询的情况下,检查关联字段中是否有分片字段 .默认 false useCompression 0,1 开启mysql压缩协议 , 0 : 关闭, 1 : 开启 fakeMySQLVersion 5.5,5.6 设置模拟的MySQL版本号 defaultSqlParser 由于MyCat的最初版本使用了FoundationDB的SQL解析器, 在MyCat1.3后增加了Druid解析器, 所以要设置defaultSqlParser属性来指定默认的解析器; 解析器有两个 : druidparser 和 fdbparser, 在MyCat1.4之后,默认是druidparser, fdbparser已经废除了 processors 1,2…. 指定系统可用的线程数量, 默认值为CPU核心 x 每个核心运行线程数量; processors 会影响processorBufferPool, processorBufferLocalPercent, processorExecutor属性, 所有, 在性能调优时, 可以适当地修改processors值 processorBufferChunk 指定每次分配Socket Direct Buffer默认值为4096字节, 也会影响BufferPool长度, 如果一次性获取字节过多而导致buffer不够用, 则会出现警告, 可以调大该值 processorExecutor 指定NIOProcessor上共享 businessExecutor固定线程池的大小; MyCat把异步任务交给 businessExecutor线程池中, 在新版本的MyCat中这个连接池使用频次不高, 可以适当地把该值调小 packetHeaderSize 指定MySQL协议中的报文头长度, 默认4个字节 maxPacketSize 指定MySQL协议可以携带的数据最大大小, 默认值为16M idleTimeout 30 指定连接的空闲时间的超时长度;如果超时,将关闭资源并回收, 默认30分钟 txIsolation 1,2,3,4 初始化前端连接的事务隔离级别,默认为 REPEATED_READ , 对应数字为3 READ_UNCOMMITED&#x3D;1; READ_COMMITTED&#x3D;2; REPEATED_READ&#x3D;3; SERIALIZABLE&#x3D;4; sqlExecuteTimeout 300 执行SQL的超时时间, 如果SQL语句执行超时,将关闭连接; 默认300秒; serverPort 8066 定义MyCat的使用端口, 默认8066 managerPort 9066 定义MyCat的管理端口, 默认9066 b）user标签配置MyCat中的用户、访问密码，以及用户针对于逻辑库、逻辑表的权限信息，具体的权限描述方式及配置说明如下： 在测试权限操作时，我们只需要将 privileges 标签的注释放开。 在 privileges 下的schema标签中配置的dml属性配置的是逻辑库的权限。 在privileges的schema下的table标签的dml属性中配置逻辑表的权限。 6）Mycat 分片①垂直拆分a）场景在业务系统中, 涉及以下表结构 ,但是由于用户与订单每天都会产生大量的数据, 单台服务器的数据存储及处理能力是有限的, 可以对数据库表进行拆分, 原有的数据库表如下。 现在考虑将其进行垂直分库操作，将商品相关的表拆分到一个数据库服务器，订单表拆分的一个数据库服务器，用户及省市区表拆分到一个服务器。最终结构如下： b）配置schema.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;schema name=&quot;SHOPPING&quot; checkSQLschema=&quot;true&quot; sqlMaxLimit=&quot;100&quot;&gt; &lt;table name=&quot;tb_goods_base&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt; &lt;table name=&quot;tb_goods_brand&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt; &lt;table name=&quot;tb_goods_cat&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt; &lt;table name=&quot;tb_goods_desc&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;goods_id&quot; /&gt; &lt;table name=&quot;tb_goods_item&quot; dataNode=&quot;dn1&quot; primaryKey=&quot;id&quot; /&gt; &lt;table name=&quot;tb_order_item&quot; dataNode=&quot;dn2&quot; primaryKey=&quot;id&quot; /&gt; &lt;table name=&quot;tb_order_master&quot; dataNode=&quot;dn2&quot; primaryKey=&quot;order_id&quot; /&gt; &lt;table name=&quot;tb_order_pay_log&quot; dataNode=&quot;dn2&quot; primaryKey=&quot;out_trade_no&quot; /&gt; &lt;table name=&quot;tb_user&quot; dataNode=&quot;dn3&quot; primaryKey=&quot;id&quot; /&gt; &lt;table name=&quot;tb_user_address&quot; dataNode=&quot;dn3&quot; primaryKey=&quot;id&quot; /&gt; &lt;table name=&quot;tb_areas_provinces&quot; dataNode=&quot;dn3&quot; primaryKey=&quot;id&quot;/&gt; &lt;table name=&quot;tb_areas_city&quot; dataNode=&quot;dn3&quot; primaryKey=&quot;id&quot;/&gt; &lt;table name=&quot;tb_areas_region&quot; dataNode=&quot;dn3&quot; primaryKey=&quot;id&quot;/&gt;&lt;/schema&gt;&lt;dataNode name=&quot;dn1&quot; dataHost=&quot;dhost1&quot; database=&quot;shopping&quot; /&gt;&lt;dataNode name=&quot;dn2&quot; dataHost=&quot;dhost2&quot; database=&quot;shopping&quot; /&gt;&lt;dataNode name=&quot;dn3&quot; dataHost=&quot;dhost3&quot; database=&quot;shopping&quot; /&gt;&lt;dataHost name=&quot;dhost1&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot;slaveThreshold=&quot;100&quot;&gt;&lt;heartbeat&gt;select user()&lt;/heartbeat&gt;&lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.200.210:3306?useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot;user=&quot;root&quot; password=&quot;1234&quot; /&gt;&lt;/dataHost&gt;&lt;dataHost name=&quot;dhost2&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot;slaveThreshold=&quot;100&quot;&gt;&lt;heartbeat&gt;select user()&lt;/heartbeat&gt;&lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.200.213:3306?useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot;user=&quot;root&quot; password=&quot;1234&quot; /&gt;&lt;/dataHost&gt;&lt;dataHost name=&quot;dhost3&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot;writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot;slaveThreshold=&quot;100&quot;&gt;&lt;heartbeat&gt;select user()&lt;/heartbeat&gt;&lt;writeHost host=&quot;master&quot; url=&quot;jdbc:mysql://192.168.200.214:3306?useSSL=false&amp;amp;serverTimezone=Asia/Shanghai&amp;amp;characterEncoding=utf8&quot;user=&quot;root&quot; password=&quot;1234&quot; /&gt;&lt;/dataHost&gt; server.xml 123456789101112131415161718&lt;user name=&quot;root&quot; defaultAccount=&quot;true&quot;&gt; &lt;property name=&quot;password&quot;&gt; 123456 &lt;/property&gt; &lt;property name=&quot;schemas&quot;&gt;SHOPPING&lt;/property&gt; &lt;!-- 表级 DML 权限设置 --&gt; &lt;!-- &lt;privileges check=&quot;true&quot;&gt; &lt;schema name=&quot;DB01&quot; dml=&quot;0110&quot; &gt; &lt;table name=&quot;TB_ORDER&quot; dml=&quot;1110&quot;&gt;&lt;/table&gt; &lt;/schema&gt; &lt;/privileges&gt; --&gt;&lt;/user&gt;&lt;user name=&quot;user&quot;&gt; &lt;property name=&quot;password&quot;&gt; 123456 &lt;/property&gt; &lt;property name=&quot;schemas&quot;&gt;SHOPPING&lt;/property&gt; &lt;property name=&quot;readOnly&quot;&gt;true&lt;/property&gt;&lt;/user&gt; c）测试在MyCat的命令行中，当我们执行以下多表联查的SQL语句时，可以正常查询出数据： 1234select ua.user_id, ua.contact, p.province, c.city, r.area , ua.address fromtb_user_address ua ,tb_areas_city c , tb_areas_provinces p ,tb_areas_region rwhere ua.province_id = p.provinceid and ua.city_id = c.cityid and ua.town_id =r.areaid ; 那么以下sql呢？ 1234SELECT order_id , payment ,receiver, province , city , area FROM tb_order_master o, tb_areas_provinces p , tb_areas_city c , tb_areas_region r WHEREo.receiver_province = p.provinceid AND o.receiver_city = c.cityid ANDo.receiver_region = r.areaid ; 现在存在一个问题，订单相关的表结构是在 192.168.200.213 数据库服务器中，而省市区的数据库表是在 192.168.200.214 数据库服务器中。那么在MyCat中执行是否可以成功呢？ 经过测试，我们看到，SQL语句执行报错。原因就是因为MyCat在执行该SQL语句时，需要往具体的数据库服务器中路由，而当前没有一个数据库服务器完全包含了订单以及省市区的表结构，造成SQL语句失败，报错。 d）全局表对于省、市、区&#x2F;县表tb_areas_provinces , tb_areas_city , tb_areas_region，是属于数据字典表，在多个业务模块中都可能会遇到，可以将其设置为全局表，利于业务操作。修改schema.xml中的逻辑表的配置，修改 tb_areas_provinces、tb_areas_city、tb_areas_region 三个逻辑表，增加 type 属性，配置为global，就代表该表是全局表，就会在所涉及到的dataNode中创建给表。对于当前配置来说，也就意味着所有的节点中都有该表了。 当在MyCat中更新全局表的时候，所有分片节点中的数据都发生了变化，每个节点的全局表数据时刻保持一致。 修改后，结构如下： 再次执行之前的sql，可以成功执行。 ②水平拆分a）场景 ​ 在业务系统中, 有一张表(日志表), 业务系统每天都会产生大量的日志数据 , 单台服务器的数据存储及处理能力是有限的, 可以对数据库表进行拆分。在三台主机上共同存储log数据 b）配置schema.xml 123456&lt;schema name=&quot;ITCAST&quot; checkSQLschema=&quot;true&quot; sqlMaxLimit=&quot;100&quot;&gt;&lt;table name=&quot;tb_log&quot; dataNode=&quot;dn4,dn5,dn6&quot; primaryKey=&quot;id&quot; rule=&quot;mod-long&quot; /&gt;&lt;/schema&gt;&lt;dataNode name=&quot;dn4&quot; dataHost=&quot;dhost1&quot; database=&quot;itcast&quot; /&gt;&lt;dataNode name=&quot;dn5&quot; dataHost=&quot;dhost2&quot; database=&quot;itcast&quot; /&gt;&lt;dataNode name=&quot;dn6&quot; dataHost=&quot;dhost3&quot; database=&quot;itcast&quot; /&gt; rule=&quot;mod-long&quot;，采用的是取模策略 可以在rule.xml中配置模的数字 tb_log表最终落在 3 个节点中，分别是 dn4、dn5、dn6 ，而具体的数据分别存储在 dhost1、dhost2、dhost3的itcast数据库中。 server.xml配置root用户既可以访问 SHOPPING 逻辑库，又可以访问ITCAST逻辑库。 123456789101112&lt;user name=&quot;root&quot; defaultAccount=&quot;true&quot;&gt; &lt;property name=&quot;password&quot;&gt; 123456 &lt;/property&gt; &lt;property name=&quot;schemas&quot;&gt;SHOPPING,ITCAST&lt;/property&gt; &lt;!-- 表级 DML 权限设置 --&gt; &lt;!-- &lt;privileges check=&quot;true&quot;&gt; &lt;schema name=&quot;DB01&quot; dml=&quot;0110&quot; &gt; &lt;table name=&quot;TB_ORDER&quot; dml=&quot;1110&quot;&gt;&lt;/table&gt; &lt;/schema&gt; &lt;/privileges&gt; --&gt;&lt;/user&gt; 7）Mycat 分片策略①范围分片a）场景 b）配置 ②取模分片a）场景 根据指定的字段值与节点数量进行求模运算，根据运算结果， 来决定该数据属于哪一个分片 主要针对的是整数型id，如果id是uuid则需要hash算法 b）配置 ③一致性hash分片a）场景 一致性哈希就是将相同的哈希因子计算值总是被划分到相同的分区表中，不会因为分区节点的增加而改变原来数据的分区位置，有效的解决了分布式数据的拓容问题。 如一开始只有3个节点，数据计算后落入节点2，如果再新增一个新的节点4，数据不会因节点增加而被计算到其它节点，这和取模的hash算法不同，不受新增节点的影响 适用于uuid生成的id（不能取模） b）配置 ④枚举分片a）场景 通过在配置文件中配置可能的枚举值, 指定数据分布到不同数据节点上, 本规则适用于按照省份、性别、状态拆分数据等业务 如一个省份的用户数据放在某一数据库 b）配置 ⑤应用指定算法a）场景 运行阶段由应用自主决定路由到那个分片 , 直接根据字符子串（必须是数字）计算分片号 就是求某个字符串字段的subStr，然后根据subStr和映射配置分配到对应的节点 如varchar类型的id：0100001,0200002…. b）配置 ⑥固定hash算法a）场景 如果是求模，连续的值，分别分配到各个不同的分片；但是此算法会将连续的值可能分配到相同的分片，降低事务处理的难度 最后10位和全1做&amp;运算 可以均匀分配，也可以非均匀分配。 分片字段必须为数字类型 b）配置 ⑦字符串hash解析a）场景 截取字符串中的指定位置的子字符串, 进行hash算法， 算出分片 其实就是对字符串型，非整数型数据进行hash处理 b）配置 512代表一个分区长度是512 2代表有2个分区，正好是1024 0:2，start:end，从0开始截，截到2 最后还是和10个1做&amp;运算 ⑧按天分片a）场景 按照日期及对应的时间周期来分片 配置的时间是2022-01-012022-01-30，如果到期后，比如2022-01-312022-02-09的数据就会放入第一个分片 b）配置 注意分片数量要和规则一致，这里只有3个分片，所以10天为一个周期进行分片 ⑨按自然月分片a）场景 使用场景为按照月份来分片, 每个自然月为一个分片 到期后又继续重新分片 b）配置 注意分片数量要和规则一致，这里只有3个分片所以配置的是1-3月 8）Mycat 管理和监控①基本原理 在MyCat中，当执行一条SQL语句时，MyCat需要进行SQL解析、分片分析、路由分析、读写分离分析等操作，最终经过一系列的分析决定将当前的SQL语句到底路由到那几个(或哪一个)节点数据库，数据库将数据执行完毕后，如果有返回的结果，则将结果返回给MyCat，最终还需要在MyCat中进行结果合并、聚合处理、排序处理、分页处理等操作，最终再将结果返回给客户端。 在MyCat的使用过程中，MyCat官方也提供了一个管理监控平台MyCat-Web（MyCat-eye）。Mycat-web 是 Mycat 可视化运维的管和监控平台，弥补了 Mycat 在监控上的空白。帮 Mycat分担统计任务和配置管理任务。Mycat-web 引入了 ZooKeeper 作为配置中心，以管理多个节点。Mycat-web 主要管理和监控 Mycat 的流量、连接、活动线程和内存等，具备 IP 白名单、邮件告警等模块，还可以统计SQL 并分析慢 SQL 和高频 SQL 等。为优化 SQL 提供依据。 ②Mycat 管理工具Mycat默认开通 2 个端口，可以在server.xml中进行修改。 8066 数据访问端口，即进行 DML 和 DDL 操作。 9066 数据库管理端口，即 mycat 服务管理控制功能，用于管理mycat的整个集群状态 连接MyCat的管理控制台： 1mysql -h 192.168.200.210 -p 9066 -uroot -p123456 命令 含义 show @@help 查看Mycat管理工具帮助文档 show @@version 查看Mycat的版本 reload @@config 重新加载Mycat的配置文件 show @@datasource 查看Mycat的数据源信息 show @@datanode 查看MyCat现有的分片节点信息 show @@threadpool 查看Mycat的线程池信息 show @@sql 查看执行的SQL show @@sql.sum 查看执行的SQL统计 该管理工具还是通过命令行的方式操作，整体还不是很直观 ③Mycat 监控 Mycat-web(Mycat-eye)是对mycat-server提供监控服务，功能不局限于对mycat-server使用。他通过JDBC连接对Mycat、Mysql监控，监控远程服务器(目前仅限于linux系统)的cpu、内存、网络、磁盘。 Mycat-eye运行过程中需要依赖zookeeper，因此需要先安装zookeeper url访问如：http://192.168.200.210:8082/mycat 浏览器连接后可以看到一个直观的分析： 4.读写分离1）基本概念 读写分离，简单地说是把对数据库的读和写操作分开,以对应不同的数据库服务器。主数据库提供写操作，从数据库提供读操作，这样能有效地减轻单台数据库的压力。 虽然我们实现了主从复制，但是编码的时候我们不太好编码，因为我们想写操作全部交给主节点执行，查询操作全部交给从节点执行，实现真正的读写分离，因此我们可以连接mycat，mycat来完成这部分逻辑判断。 通过MyCat即可轻易实现上述功能，不仅可以支持MySQL，也可以支持Oracle和SQL Server。 2）一主一从①原理 MySQL的主从复制，是基于二进制日志（binlog）实现的。 判断写操作和读操作的逻辑交给mycat ②搭建一主一从结构 主机 角色 用户名 密码 192.168.200.211 master root 1234 192.168.200.212 slave root 1234 具体搭建在主从复制处 ③Mycat 配置读写分离a）scheam.xml writeHost代表的是写操作对应的数据库，readHost代表的是读操作对应的数据库。 所以我们要想实现读写分离，就得配置writeHost关联的是主库，readHost关联的是从库。 而仅仅配置好了writeHost以及readHost还不能完成读写分离，还需要配置一个非常重要的负责均衡的参数 balance，取值有 4 种，具体含义如下： 参数值 含义 0 不开启读写分离机制 , 所有读操作都发送到当前可用的writeHost上（配置了readHost也没用） 1 全部的readHost与备用的writeHost都参与select 语句的负载均衡（主要针对于双主双从模式） 2 所有的读写操作都随机在writeHost , readHost上分发 3 所有的读请求随机分发到writeHost对应的readHost上执行, writeHost不负担读压力 因此要想实现读写分离，取值之能是1,3 在一主一从的情况下1和3效果都是一样的 b）server.xml123456789101112&lt;user name=&quot;root&quot; defaultAccount=&quot;true&quot;&gt;&lt;property name=&quot;password&quot;&gt; 123456 &lt;/property&gt;&lt;property name=&quot;schemas&quot;&gt;SHOPPING,ITCAST,ITCAST_RW&lt;/property&gt; &lt;!-- 表级 DML 权限设置 --&gt; &lt;!-- &lt;privileges check=&quot;true&quot;&gt; &lt;schema name=&quot;DB01&quot; dml=&quot;0110&quot; &gt; &lt;table name=&quot;TB_ORDER&quot; dml=&quot;1110&quot;&gt;&lt;/table&gt; &lt;/schema&gt; &lt;/privileges&gt; --&gt;&lt;/user&gt; 主要就是保证用户能操作配置的schema ④问题 现在确实做到了读写分离，但是当主节点宕机后，虽然还可以进行读操作，但是不能执行写操作了 为了解决该问题，我们考虑双主双从模式 3）双主双从①结构 一个主机 Master1 用于处理所有写请求，它的从机 Slave1 和另一台主机 Master2 还有它的从机 Slave2 负责所有读请求。当 Master1 主机宕机后，Master2 主机负责写请求，Master1 、Master2 互为备机。 所有读请求都交给m1处理，m2,s1,s2处理读请求 编号 IP 预装软件 角色 1 192.168.200.210 MyCat、MySQL MyCat中间件服务器 2 192.168.200.211 MySQL M1 3 192.168.200.212 MySQL S1 4 192.168.200.213 MySQL M2 5 192.168.200.214 MySQL S2 ②搭建双主双从结构a）M1 修改Mysql配置文件 &#x2F;etc&#x2F;my.cnf 12345678#mysql 服务ID，保证整个集群环境中唯一，取值范围：1 – 2^32-1，默认为 1server-id= 1#指定同步的数据库binlog-do-db=db01binlog-do-db=db02binlog-do-db=db03# 在作为从数据库的时候，有写入操作也要更新二进制日志文件log-slave-updates 配置mysql实例的id，保证整个集群环境中唯一，这里是1 指定要同步的数据库，其它的数据库不同步 重启MySQL服务器 1systemctl restart mysqld 创建账户并授权 1234# 创建itcast用户，并设置密码，该用户可在任意主机连接该MySQL服务CREATE USER &#x27;itcast&#x27;@&#x27;%&#x27; IDENTIFIED WITH mysql_native_password BY &#x27;Root@123456&#x27;;# 为 &#x27;itcast&#x27;@&#x27;%&#x27; 用户分配主从复制权限GRANT REPLICATION SLAVE ON *.* TO &#x27;itcast&#x27;@&#x27;%&#x27;; 创建用于进行主从同步的账户并授予主从复制权限 通过指令，查看两台主库的二进制日志坐标 看记录到了哪一个binlog，在该binlog的哪个位置开始 b）M2 修改Mysql配置文件 &#x2F;etc&#x2F;my.cnf 12345678# mysql 服务ID，保证整个集群环境中唯一，取值范围：1 – 2^32-1，默认为 1server-id= 3# 指定同步的数据库binlog-do-db=db01binlog-do-db=db02binlog-do-db=db03# 在作为从数据库的时候，有写入操作也要更新二进制日志文件log-slave-updates 配置mysql实例的id，保证整个集群环境中唯一，这里是3 指定要同步的数据库，其它的数据库不同步 重启MySQL服务器 1systemctl restart mysqld 创建账户并授权 1234# 创建itcast用户，并设置密码，该用户可在任意主机连接该MySQL服务CREATE USER &#x27;itcast&#x27;@&#x27;%&#x27; IDENTIFIED WITH mysql_native_password BY &#x27;Root@123456&#x27;;#为 &#x27;itcast&#x27;@&#x27;%&#x27; 用户分配主从复制权限GRANT REPLICATION SLAVE ON *.* TO &#x27;itcast&#x27;@&#x27;%&#x27;; 创建用于进行主从同步的账户并授予主从复制权限 通过指令，查看两台主库的二进制日志坐标 c）S1 修改配置文件 &#x2F;etc&#x2F;my.cnf 12#mysql 服务ID，保证整个集群环境中唯一，取值范围：1 – 232-1，默认为 1server-id= 2 配置mysql实例的id，保证整个集群环境中唯一，这里是2 这里不需要配置要同步的数据库，因为S1没有从库要同步自己的数据 重新启动MySQL服务器 1systemctl restart mysqld d）S2 修改配置文件 &#x2F;etc&#x2F;my.cnf 12#mysql 服务ID，保证整个集群环境中唯一，取值范围：1 – 232-1，默认为 1server-id= 4 配置mysql实例的id，保证整个集群环境中唯一，这里是4 这里不需要配置要同步的数据库，因为S2没有从库要同步自己的数据 重新启动MySQL服务器 1systemctl restart mysqld e）S1同步M1，S2同步M2 在 slave1(192.168.200.212)上执行 123CHANGE MASTER TO MASTER_HOST=&#x27;192.168.200.211&#x27;, MASTER_USER=&#x27;itcast&#x27;,MASTER_PASSWORD=&#x27;Root@123456&#x27;, MASTER_LOG_FILE=&#x27;binlog.000002&#x27;,MASTER_LOG_POS= 663 ; 指定了M1的ip，使用的用户信息，开始同步的位置 在 slave2(192.168.200.214)上执行 123CHANGE MASTER TO MASTER_HOST=&#x27;192.168.200.213&#x27;, MASTER_USER=&#x27;itcast&#x27;,MASTER_PASSWORD=&#x27;Root@123456&#x27;, MASTER_LOG_FILE=&#x27;binlog.000002&#x27;,MASTER_LOG_POS= 663 ; 指定了M2的ip，使用的用户信息，开始同步的位置 启动两台从库主从复制，查看从库状态 12start slave;show slave status \\G; f）M1和M2相互同步 在 Master1(192.168.200.211)上执行 123CHANGE MASTER TO MASTER_HOST=&#x27;192.168.200.213&#x27;, MASTER_USER=&#x27;itcast&#x27;,MASTER_PASSWORD=&#x27;Root@123456&#x27;, MASTER_LOG_FILE=&#x27;binlog.000002&#x27;,MASTER_LOG_POS= 663 ; 指定了M2的ip，使用的用户信息，开始同步的位置 在 Master2(192.168.200.213)上执行 123CHANGE MASTER TO MASTER_HOST=&#x27;192.168.200.211&#x27;, MASTER_USER=&#x27;itcast&#x27;,MASTER_PASSWORD=&#x27;Root@123456&#x27;, MASTER_LOG_FILE=&#x27;binlog.000002&#x27;,MASTER_LOG_POS= 663 ; 指定了M1的ip，使用的用户信息，开始同步的位置 启动两台从库主从复制，查看从库状态 12start slave;show slave status \\G; ③Mycat 配置读写分离 dataHost中配置了主从关系 以下是dataHost标签详解 当配置好了以后，如果M1宕机了，还是可以进行写操作，M2会负责","categories":[{"name":"mysql","slug":"mysql","permalink":"http://example.com/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://example.com/tags/mysql/"}]},{"title":"Spring Boot探究","slug":"spring Boot原理探究","date":"2022-04-13T04:06:12.000Z","updated":"2022-08-28T14:55:12.092Z","comments":true,"path":"2022/04/13/spring Boot原理探究/","link":"","permalink":"http://example.com/2022/04/13/spring%20Boot%E5%8E%9F%E7%90%86%E6%8E%A2%E7%A9%B6/","excerpt":"bean的加载方式、bean的加载控制、bean依赖属性的配置、SpringBoot自动装配、自定义一个starter","text":"bean的加载方式、bean的加载控制、bean依赖属性的配置、SpringBoot自动装配、自定义一个starter 1.bean加载1）配置文件+&lt;bean/&gt;标签​ 最基本的bean的加载方式其实可以直击spring管控bean的核心思想，就是提供类名，然后spring就可以管理了。所以第一种方式就是给出bean的类名，至于内部嘛就是反射机制加载成class，然后拿到了class就可以得到对象实例。 12345678910111213&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;!--xml方式声明自己开发的bean--&gt; &lt;bean id=&quot;cat&quot; class=&quot;Cat&quot;/&gt; &lt;bean class=&quot;Dog&quot;/&gt; &lt;!--xml方式声明第三方开发的bean--&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;/&gt; &lt;bean class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;/&gt; &lt;bean class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;/&gt;&lt;/beans&gt; 2）配置文件中扫描+注解定义bean​ 哪一个类要受到spring管控加载成bean，就在这个类的上面加一个注解，还可以顺带起一个bean的名字（id）。这里可以使用的注解有@Component以及三个衍生注解@Service、@Controller、@Repository。 123@Component(&quot;tom&quot;)public class Cat &#123;&#125; 123@Servicepublic class Mouse &#123;&#125; ​ 第三方技术在@Bean定义在一个方法上方，当前方法的返回值就可以交给spring管控，记得这个方法所在的类一定要定义在@Component或@Configuration修饰的类中 12345678@Componentpublic class DbConfig &#123; @Bean public DruidDataSource dataSource()&#123; DruidDataSource ds = new DruidDataSource(); return ds; &#125;&#125; 为什么可以用@component？ 我们查看@Configuration 可以看到@Configuration内部也有@Component，所以可以得出当@Configuration修饰的配置类也会当做一个bean加入到spring的容器中 我们可以验证上面的结论 12345678@ComponentScan(&#123;&quot;fla.bean&quot;,&quot;fla.config&quot;&#125;)@Configuration()public class SpringConfig &#123; @Bean public Dog dog()&#123; return new Dog(); &#125;&#125; 1234567public static void main(String[] args) &#123; ApplicationContext ctx = new AnnotationConfigApplicationContext(SpringConfig.class); String[] beanDefinitionNames = ctx.getBeanDefinitionNames(); for (String beanName : beanDefinitionNames) &#123; System.out.println(beanName); &#125;&#125; 123456org.springframework.context.annotation.internalConfigurationAnnotationProcessororg.springframework.context.annotation.internalAutowiredAnnotationProcessororg.springframework.context.annotation.internalCommonAnnotationProcessororg.springframework.context.event.internalEventListenerProcessororg.springframework.context.event.internalEventListenerFactoryspringConfig //可以看到springConfig加入了spring容器 ​ 从上面我们可以看出springConfig加入了spring容器，并且id名字就是类名首字母小写 ​ 注意：没参数的时候都是通过该bean的无参构造方法来初始化的，有参数的时候是使用该bean的有参构造方法，但需要保证传入的参数在IOC容器中可以AutoWire，如果没有的话就先无参构造，然后再在方法中set属性，如下面第一种： 12345678910111213141516171819public class JdbcConfig &#123; @Value(&quot;com.mysql.jdbc.Driver&quot;) private String driver; @Value(&quot;jdbc:mysql://localhost:3306/spring_db&quot;) private String url; @Value(&quot;root&quot;) private String userName; @Value(&quot;password&quot;) private String password; @Bean public DataSource dataSource()&#123; DruidDataSource ds = new DruidDataSource(); ds.setDriverClassName(driver); ds.setUrl(url); ds.setUsername(userName); ds.setPassword(password); return ds; &#125;&#125; 123456789101112131415161718192021222324//定义mybatis专用的配置类@Configurationpublic class MyBatisConfig &#123; //定义创建SqlSessionFactory对应的bean @Bean public SqlSessionFactoryBean sqlSessionFactory(DataSource dataSource)&#123;//自动注入dataSource //SqlSessionFactoryBean是由mybatis-spring包提供的，专用于整合用的对象 SqlSessionFactoryBean sfb = new SqlSessionFactoryBean(); //设置数据源替代原始配置中的environments的配置 sfb.setDataSource(dataSource); //设置类型别名替代原始配置中的typeAliases的配置 sfb.setTypeAliasesPackage(&quot;com.itheima.domain&quot;); return sfb; &#125; //定义加载所有的映射配置：告诉mapper类在哪儿 @Bean public MapperScannerConfigurer mapperScannerConfigurer()&#123; MapperScannerConfigurer msc = new MapperScannerConfigurer(); msc.setBasePackage(&quot;com.itheima.dao&quot;); return msc; &#125;&#125; 注解文件扫描bean注解： 1234567891011121314&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd &quot;&gt; &lt;!--指定扫描加载bean的位置--&gt; &lt;context:component-scan base-package=&quot;com.fla.bean,com.fla.config&quot;/&gt;&lt;/beans&gt; ​ 声明bean的方式是目前企业中较为常见的bean的声明方式，但是也有缺点。方式一中，通过一个配置文件，你可以查阅当前spring环境中定义了多少个或者说多少种bean，但是方式二没有任何一个地方可以查阅整体信息，只有当程序运行起来才能感知到加载了多少个bean。 3）注解方式声明配置类​ 定义一个类并使用@ComponentScan替代方式二中原始xml配置中的包扫描这个动作，其实功能基本相同。 1234567@ComponentScan(&#123;&quot;com.fla.bean&quot;,&quot;com.fla.config&quot;&#125;)public class SpringConfig3 &#123; @Bean public Dao dog()&#123; return new Dao(); &#125;&#125; ①FactroyBean接口​ 补充一个小知识，spring提供了一个接口FactoryBean，也可以用于声明bean，只不过实现了FactoryBean接口的类造出来的对象不是当前类的对象，而是FactoryBean接口泛型指定类型的对象。如下列，造出来的bean并不是DogFactoryBean，而是Dog。可以在对象初始化前做一些事情，下例中的注释位置就是让你自己去扩展要做的其他事情的。 12345678910111213141516public class DogFactoryBean implements FactoryBean&lt;Dog&gt; &#123; @Override public Dog getObject() throws Exception &#123; Dog d = new Dog(); //获得实例对象的时候进行一些其它初始化操作 return d; &#125; @Override public Class&lt;?&gt; getObjectType() &#123; return Dog.class; &#125; @Override public boolean isSingleton() &#123; return true; &#125;&#125; ​ Dog是一个抽象后剥离的特别干净的模型，但是实际使用的时候必须进行一系列的初始化动作。使用这种方式就能够在实例化bean的时候进行初始化操作。 ​ 通常实现了FactoryBean接口的类使用@Bean的形式进行加载，当然你也可以使用@Component去声明DogFactoryBean，只要被扫描加载到即可，但是这种格式加载总觉得怪怪的，指向性不是很明确。 1234567@ComponentScan(&#123;&quot;com.fla.bean&quot;,&quot;com.fla.config&quot;&#125;)public class SpringConfig3 &#123; @Bean public DogFactoryBean dog()&#123; return new DogFactoryBean(); &#125;&#125; ②注解格式导入XML格式配置的bean​ 补充一个小知识，由于早起开发的系统大部分都是采用xml的形式配置bean。新开发的用注解格式，之前开发的是xml格式。spring提供了一个注解可以解决这个问题，@ImportResource，在配置类上直接写上要被融合的xml配置文件名即可。 1234@Configuration@ImportResource(&quot;applicationContext1.xml&quot;)//这样就可以导入原来xml配置中的beanpublic class SpringConfig32 &#123;&#125; ③proxyBeanMethods属性 可以看出@Configuartion具有@Component所有功能，但是@Configuartion具有一个特殊的值proxyBeanMethods，这使得这个注解有一个更加强大的功能，它可以保障配置类中使用方法创建的bean的唯一性。为@Configuration注解设置proxyBeanMethods属性值为true即可，并且此属性默认值为true。 springConfig类（默认为true）： 12345678@ComponentScan(&#123;&quot;fla.bean&quot;,&quot;fla.config&quot;&#125;)@Configuration //默认就是truepublic class SpringConfig &#123; @Bean public Dog dog()&#123; return new Dog(); &#125;&#125; psvm： 1234567891011121314public static void main(String[] args) &#123; ApplicationContext ctx = new AnnotationConfigApplicationContext(SpringConfig.class); SpringConfig springConfig1 = ctx.getBean(SpringConfig.class); SpringConfig springConfig2 = ctx.getBean(SpringConfig.class); System.out.println(&quot;sf1.class=: &quot;+springConfig1.getClass()); System.out.println(&quot;sf2.class=: &quot;+springConfig2.getClass()); System.out.println(&quot;sf1: &quot;+springConfig1); System.out.println(&quot;sf2: &quot;+springConfig2); System.out.println(&quot;sf1.dog: &quot;+springConfig1.dog()); System.out.println(&quot;sf1.dog: &quot;+springConfig1.dog()); System.out.println(&quot;sf2.dog: &quot;+springConfig2.dog()); System.out.println(&quot;sf2.dog: &quot;+springConfig2.dog()); &#125; 结果： 12345678sf1.class=: class fla.config.SpringConfig$$EnhancerBySpringCGLIB$$bd5ede93sf2.class=: class fla.config.SpringConfig$$EnhancerBySpringCGLIB$$bd5ede93sf1: fla.config.SpringConfig$$EnhancerBySpringCGLIB$$bd5ede93@3aefe5e5sf2: fla.config.SpringConfig$$EnhancerBySpringCGLIB$$bd5ede93@3aefe5e5sf1.dog: fla.bean.Dog@149e0f5dsf1.dog: fla.bean.Dog@149e0f5dsf2.dog: fla.bean.Dog@149e0f5dsf2.dog: fla.bean.Dog@149e0f5d 获取的两个springConfig的bean都是cglib动态代理获得的且是同一个 调用其springConfig对象的dog方法获得的bean都是同一个对象 修改springConfig类，设置@Configuration(proxyBeanMethods &#x3D; false) 12345678@ComponentScan(&#123;&quot;fla.bean&quot;,&quot;fla.config&quot;&#125;)@Configuration(proxyBeanMethods = false)public class SpringConfig &#123; @Bean public Dog dog()&#123; return new Dog(); &#125;&#125; 执行的结果： 12345678sf1.class=: class fla.config.SpringConfigsf2.class=: class fla.config.SpringConfigsf1: fla.config.SpringConfig@29ba4338sf2: fla.config.SpringConfig@29ba4338sf1.dog: fla.bean.Dog@57175e74sf1.dog: fla.bean.Dog@7bb58ca3sf2.dog: fla.bean.Dog@c540f5asf2.dog: fla.bean.Dog@770c2e6b 获取的两个springConfig的bean并没有使用动态代理，其class就是本身，但是两次获得的仍然是同一个对象 调用其springConfig对象的dog方法获得的bean不再是同一个对象，每次调用都会new一个新的 4）使用@Import注入bean​ 使用扫描的方式加载bean是常见的bean的加载方式，但是由于扫描的时候不仅可以加载到你要的东西，还有可能加载到各种各样的乱七八糟的东西。 ​ 考虑一个问题：比如你扫描了com.fla.service包，后来因为业务需要，又扫描了com.fla.dao包，你发现com.fla包下面只有service和dao这两个包，直接扫描com.fla。但是十天后加入了一个外部依赖包，里面也有com.fla包，这样就会发生错误。 ​ 所以我们需要一种精准制导的加载方式，使用@Import注解就可以解决你的问题。它可以加载所有的一切，只需要在注解的参数中写上加载的类对应的.class即可。虽然@Import不如@ComponentScan简单，但是它可以指定加载啊，好的命名规范配合@ComponentScan可以解决很多问题，但是@Import注解拥有其重要的应用场景：如果要加载的bean并没有@Component注解（如第三方jar包中的类），那么@ComponentScan就没有用了，但是@Import还是能够直接加载。 springConfig类： 1234@Import(DogConfig.class) //不用@ComponentScan，用@Import精准扫描public class SpringConfig &#123; &#125; DogConfig类： 123456public class DogConfig &#123; @Bean public Dog dog()&#123; return new Dog(); &#125;&#125; 可以看到DogConfig中还定义了Dogbean psvm： 12345678910public static void main(String[] args) &#123; ApplicationContext ctx = new AnnotationConfigApplicationContext(SpringConfig.class); String[] names = ctx.getBeanDefinitionNames(); for(String name:names)&#123; System.out.println(name); &#125; System.out.println(ctx.getBean(SpringConfig.class)); System.out.println(ctx.getBean(DogConfig.class)); System.out.println(ctx.getBean(Dog.class)); &#125; 最终结果： 1234567891011org.springframework.context.annotation.internalConfigurationAnnotationProcessororg.springframework.context.annotation.internalAutowiredAnnotationProcessororg.springframework.context.annotation.internalCommonAnnotationProcessororg.springframework.context.event.internalEventListenerProcessororg.springframework.context.event.internalEventListenerFactoryspringConfigfla.config.DogConfigdogfla.config.SpringConfig@799d4f69fla.config.DogConfig@49c43f4efla.bean.Dog@290dbf45 可以看到除了DogConfig，它里面定义的bean也被注册了 并且加载的bean的class都是其类名，考虑在DogConfig上加入@Configuration，getClass结果如下 fla.config.DogConfig$$EnhancerBySpringCGLIB$$7bc970f3@7219ec67 此时DogConfig是通过动态代理获得的，那么此时dogbean还是唯一的吗 DogConfig类 12345@Configuration@Import(DogConfig.class)public class SpringConfig &#123;&#125; psvm： 1234567891011121314public static void main(String[] args) &#123; ApplicationContext ctx = new AnnotationConfigApplicationContext(SpringConfig.class); String[] names = ctx.getBeanDefinitionNames(); for(String name:names)&#123; System.out.println(name); &#125; DogConfig dogConfig = ctx.getBean(DogConfig.class); System.out.println(dogConfig.dog()); System.out.println(dogConfig.dog()); Dog dog = ctx.getBean(Dog.class); System.out.println(dog); System.out.println(dog.getClass());&#125; 结果： 1234567891011121314org.springframework.context.annotation.internalConfigurationAnnotationProcessororg.springframework.context.annotation.internalAutowiredAnnotationProcessororg.springframework.context.annotation.internalCommonAnnotationProcessororg.springframework.context.event.internalEventListenerProcessororg.springframework.context.event.internalEventListenerFactoryspringConfigfla.config.DogConfigdogfla.config.SpringConfig$$EnhancerBySpringCGLIB$$bb4bfa36@7b227d8dfla.config.DogConfig$$EnhancerBySpringCGLIB$$7bc970f3@7219ec67fla.bean.Dog@45018215fla.bean.Dog@45018215fla.bean.Dog@45018215class fla.bean.Dog 可以明显看到获得的dog都是同一个对象，这是DogConfig的@Configuration起作用了，当去掉后这几个dog就不再是同一个了，尽管SpringConfig有@Configuration，说明@Configuration只能保证本类的bean是唯一的，import是不能保证的 123456fla.config.SpringConfig$$EnhancerBySpringCGLIB$$bb4bfa36@1d082e88fla.config.DogConfig@60704cfla.bean.Dog@6b19b79 //这下面的三个dog都不是同一个fla.bean.Dog@2a32de6cfla.bean.Dog@7692d9ccclass fla.bean.Dog 5）编程形式注册bean​ 下面的方式可以在容器初始化完成后手动加载bean。通过这种方式可以实现编程式控制bean的加载。 1234567public class App5 &#123; public static void main(String[] args) &#123; AnnotationConfigApplicationContext ctx = new AnnotationConfigApplicationContext(SpringConfig.class); //上下文容器对象已经初始化完毕后，手工加载bean ctx.register(Mouse.class); &#125;&#125; 注意：这个方法只有在AnnotationConfigApplicationContext中有实现，所以不能使用下面这种方式 ApplicationContext ctx &#x3D; new AnnotationConfigApplicationContext(SpringConfig.class); ApplicationContext中没有该方法 Cat类： 12345678910public class Cat &#123; int age; public Cat()&#123; &#125; public Cat(int age)&#123; this.age=age; &#125;&#125; 12345678910public class App5 &#123; public static void main(String[] args) &#123; AnnotationConfigApplicationContext ctx = new AnnotationConfigApplicationContext(SpringConfig.class); //上下文容器对象已经初始化完毕后，手工加载bean ctx.registerBean(&quot;tom&quot;, Cat.class,0);//最后是一个可变参数，代表构造方法的参数 ctx.registerBean(&quot;tom&quot;, Cat.class,1); ctx.registerBean(&quot;tom&quot;, Cat.class,2); System.out.println(ctx.getBean(Cat.class)); &#125;&#125; 最后是2的tom被注册了，后面覆盖前面 6）导入实现了ImportSelector接口的类​ 5）是在容器初始化后实现bean的加载控制，该方法可以在容器初始化过程中进行控制。实现ImportSelector接口的类可以设置加载的bean的全路径类名，记得一点，只要能编程就能判定，能判定意味着可以控制程序的运行走向，进而控制一切。 SpringConfig： 1234@Import(MyImportSelector.class) //加载Selectorpublic class SpringConfig &#123; &#125; MyImportSelector：注意要实现ImportSelector接口 123456789101112public class MyImportSelector implements ImportSelector &#123; @Override public String[] selectImports(AnnotationMetadata metadata) &#123; //各种条件的判定，判定完毕后，决定是否装载指定的bean boolean flag = metadata.hasAnnotation(&quot;org.springframework.context.annotation.Configuration&quot;); if(flag)&#123; return new String[]&#123;&quot;com.fla.bean.Dog&quot;&#125;; &#125; return new String[]&#123;&quot;com.fla.bean.Cat&quot;&#125;; &#125;&#125; 注意参数中有一个AnnotationMetadata，这里的MetaData指的是元数据，那么元数据代表什么呢 可以看到是SpringConfig通过@Import(MyImportSelector.class)加载了Selector，那么对该MyImportSelector来说，加载它的SpringConfig就是它的元数据 既然metaData代表SpringConfig，那么我们就可以通过metaData获取SpringConfig中的各类信息 如上方就是判断元数据即SpringConfig中是否有@Configuration注解 String[]数组中写明要加载的类的全路径类名，获得的bean的id名是如下格式 fla.bean.Catfla.bean.Mouse 若该类有注解如@Component(“cat”)，那么生成的bean的id名是cat而不是fla.bean.Cat 7）导入实现了ImportBeanDefinitionRegistrar接口的类​ 6）中提供了给定类全路径类名控制bean加载的形式，但bean的加载不是一个简简单单的对象，spring中定义了一个叫做BeanDefinition的东西，它才是控制bean初始化加载的核心。BeanDefinition接口中给出了若干种方法，可以控制bean的相关属性。说个最简单的，创建的对象是单例还是非单例，在BeanDefinition中定义了scope属性就可以控制这个。方式6）没有开放出足够的对bean的控制操作，但此方式我们就能对bean有更多操作。我们可以通过定义一个类，然后实现ImportBeanDefinitionRegistrar接口的方式定义bean，并且还可以让你对bean的初始化进行更加细粒度的控制。 SpringConfig： 1234@Import(&#123;MySelector.class, MyRegistrar.class&#125;)//加载Selector和 MyRegistrar.classpublic class SpringConfig &#123; &#125; MyRegistrar：注意要实现ImportBeanDefinitionRegistrar接口 12345678910@Override public void registerBeanDefinitions(AnnotationMetadata metaData, BeanDefinitionRegistry registry) &#123; //1.首先获得一个BeanDefinition，因为下面的方法需要该方法需要 AbstractBeanDefinition beanDefinition = BeanDefinitionBuilder.rootBeanDefinition(Cat.class).getBeanDefinition(); //2.不同于6），可以在初始化之前可以对bean的属性做更多细腻的控制，如下面就可以设置是否是单例 beanDefinition.setScope(&quot;singleton&quot;); //3.通过该方法传入beanDefinition来初始化一个bean registry.registerBeanDefinition(&quot;cat11&quot;,beanDefinition); &#125; 7）与6）的不同: 都是在bean的初始化过程中相关控制，但是7）更接近bean的初始化过程，通过BeanDefinition来初始化一个bean 7）可以在初始化之前可以对bean的属性做更多细腻的控制，如上面就设置是否是单例 psvm public static void main(String[] args) &#123; ApplicationContext ctx = new AnnotationConfigApplicationContext(SpringConfig.class); String[] names = ctx.getBeanDefinitionNames(); for (String name : names) &#123; System.out.println(name); &#125; &#125; 1234567891011* 结果： * ```tex org.springframework.context.annotation.internalCommonAnnotationProcessor org.springframework.context.event.internalEventListenerProcessor org.springframework.context.event.internalEventListenerFactory springConfig fla.bean.Cat //6）selector初始化的 fla.bean.Mouse cat11 //7）Registrar初始化的 可以看出容器中有两个Cat的实例，分别是selector初始化的和Registrar初始化的 8）导入实现了BeanDefinitionRegistryPostProcessor接口的类​ 考虑一个问题：这么多种方式，它们之间如果有冲突怎么办？谁能有最终裁定权？ ​ BeanDefinitionRegistryPostProcessor，BeanDefinition代表了是bean定义，Registry注册，Post后置，Processor处理器，全称bean定义后处理器，在所有bean注册后，它把最后一道关，说白了，它说了算，这下消停了，它是最后一个运行的。 12345678public class MyPostProcessor implements BeanDefinitionRegistryPostProcessor &#123; @Override public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) throws BeansException &#123; BeanDefinition beanDefinition = BeanDefinitionBuilder.rootBeanDefinition(BookServiceImpl4.class).getBeanDefinition(); registry.registerBeanDefinition(&quot;bookService&quot;,beanDefinition); &#125;&#125; ​ 总体上来说，上面介绍了各种各样的bean的注册加载初始化方式，方式很多，spring源码中大量运用各种方式。 ​ 比如说框架会默认加载一些bean，如果我们要自定义的话就可以使用这种方式，在beanDefinition注册后进行覆盖 总结 bean的定义由前期xml配置逐步演化成注解配置，本质是一样的，都是通过反射机制加载类名后创建对象，对象就是spring管控的bean @Import注解可以指定加载某一个类作为spring管控的bean，如果被加载的类中还具有@Bean相关的定义，会被一同加载 spring开放出了若干种可编程控制的bean的初始化方式，通过分支语句由固定的加载bean转成了可以选择bean是否加载或者选择加载哪一种bean 9）6、7、8比较①当都指定统一个类，但取的id名字不同时都指定CatService1这个类但是取不同名字 Service1类： 123@Service(&quot;catService&quot;) //有注解并起了名字public class CatService1 implements CatService &#123;&#125; 注意注解中取了名字 Myselector类： 123456public class MySelector implements ImportSelector &#123; @Override public String[] selectImports(AnnotationMetadata metadata) &#123; return new String[]&#123;&quot;fla.service.impl.CatService1&quot;&#125;; &#125;&#125; MyRegistrar类： 12345678public class MyRegistrar implements ImportBeanDefinitionRegistrar &#123; @Override public void registerBeanDefinitions(AnnotationMetadata metaData, BeanDefinitionRegistry registry) &#123; AbstractBeanDefinition beanDefinition = BeanDefinitionBuilder.rootBeanDefinition(CatService1.class).getBeanDefinition(); beanDefinition.setScope(&quot;singleton&quot;); registry.registerBeanDefinition(&quot;cat11&quot;,beanDefinition); &#125;&#125; MyProcessor类： 123456789101112public class MyProcessor implements BeanDefinitionRegistryPostProcessor &#123; @Override public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) throws BeansException &#123; //1.首先获得一个BeanDefinition，因为下面的方法需要该方法需要 AbstractBeanDefinition beanDefinition = BeanDefinitionBuilder.rootBeanDefinition(CatService1.class).getBeanDefinition(); //2.不同于6），可以在初始化之前可以对bean的属性做更多细腻的控制，如下面就可以设置是否是单例 beanDefinition.setScope(&quot;singleton&quot;); //3.通过该方法传入beanDefinition来初始化一个bean registry.registerBeanDefinition(&quot;cat12&quot;,beanDefinition); &#125;&#125; SpringConfig类： 1234@Import(&#123;CatService1.class,MySelector.class,MyRegistrar.class,MyProcessor.class&#125;)public class SpringConfig &#123;&#125; 注意：这里直接import了CatService1，相当于容器本来就有一个CatService1.class的实例对象（id名根据其@Service(“catService”) 指定的） psvm 1234567891011public static void main(String[] args) &#123; ApplicationContext ctx = new AnnotationConfigApplicationContext(SpringConfig2.class); String[] names = ctx.getBeanDefinitionNames(); for (String name : names) &#123; System.out.println(name); &#125; System.out.println(&quot;catService: &quot;+ctx.getBean(&quot;catService&quot;).getClass()); System.out.println(&quot;cat11:&quot; +ctx.getBean(&quot;cat11&quot;).getClass()); System.out.println(&quot;cat12:&quot;+ctx.getBean(&quot;cat12&quot;).getClass()); &#125; 最终结果： 12345678910111213org.springframework.context.annotation.internalConfigurationAnnotationProcessororg.springframework.context.annotation.internalAutowiredAnnotationProcessororg.springframework.context.annotation.internalCommonAnnotationProcessororg.springframework.context.event.internalEventListenerProcessororg.springframework.context.event.internalEventListenerFactoryspringConfig2catServicefla.processor.MyProcessorcat11cat12catService: class fla.service.impl.CatService1cat11:class fla.service.impl.CatService1cat12:class fla.service.impl.CatService1 从该注解可知当对同一个类取不同id名的时候，这几种方式生成的bean的class都是一样的，但id名不同，所以也是不同的bean，不会覆盖 @Import({CatService1.class,MySelector.class）方式其实是差不多的，知识后者可以对bean的初始话过程进行控制 ②当都指定统不同类，但取的id名字向同时Service1类： 123@Service(&quot;catService&quot;) //有注解并起了名字public class CatService1 implements CatService &#123;&#125; 注意注解中取了名字，而其它类都没import且没有@Service(“catService”) Myselector类： 123456public class MySelector implements ImportSelector &#123; @Override public String[] selectImports(AnnotationMetadata metadata) &#123; return new String[]&#123;&quot;fla.service.impl.CatService1&quot;&#125;; &#125;&#125; MyRegistrar类： 12345678public class MyRegistrar implements ImportBeanDefinitionRegistrar &#123; @Override public void registerBeanDefinitions(AnnotationMetadata metaData, BeanDefinitionRegistry registry) &#123; AbstractBeanDefinition beanDefinition = BeanDefinitionBuilder.rootBeanDefinition(CatService2.class).getBeanDefinition(); beanDefinition.setScope(&quot;singleton&quot;); registry.registerBeanDefinition(&quot;catService&quot;,beanDefinition); &#125;&#125; 取得id名是catService，指定的是CatService2.class MyProcessor类： 123456789101112public class MyProcessor implements BeanDefinitionRegistryPostProcessor &#123; @Override public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) throws BeansException &#123; //1.首先获得一个BeanDefinition，因为下面的方法需要该方法需要 AbstractBeanDefinition beanDefinition = BeanDefinitionBuilder.rootBeanDefinition(CatService1.class).getBeanDefinition(); //2.不同于6），可以在初始化之前可以对bean的属性做更多细腻的控制，如下面就可以设置是否是单例 beanDefinition.setScope(&quot;singleton&quot;); //3.通过该方法传入beanDefinition来初始化一个bean registry.registerBeanDefinition(&quot;cat12&quot;,beanDefinition); &#125;&#125; 取得id名是catService，指定的是CatService3.class SpringConfig类： 1234@Import(&#123;CatService1.class,MySelector.class,MyRegistrar.class,MyProcessor.class&#125;)public class SpringConfig &#123;&#125; 注意：这里直接import了CatService1，相当于容器本来就有一个CatService1.class的实例对象（id名根据其@Service(“catService”) 指定的），即容器中本来就有一个id名叫”catService”的对象，且它的class是CatService1.class psvm： 123456789public static void main(String[] args) &#123; ApplicationContext ctx = new AnnotationConfigApplicationContext(SpringConfig2.class); String[] names = ctx.getBeanDefinitionNames(); for (String name : names) &#123; System.out.println(name); &#125; System.out.println(); System.out.println(&quot;catService: &quot;+ctx.getBean(&quot;catService&quot;).getClass()); &#125; 最终结果： 12345678910org.springframework.context.annotation.internalConfigurationAnnotationProcessororg.springframework.context.annotation.internalAutowiredAnnotationProcessororg.springframework.context.annotation.internalCommonAnnotationProcessororg.springframework.context.event.internalEventListenerProcessororg.springframework.context.event.internalEventListenerFactoryspringConfig2catServicefla.processor.MyProcessorcatService: class fla.service.impl.CatService3 // 可以看出是processor最后做了决定 从结果看出是processor最后进行了裁定 如果没有processor，得到的结果是catService: class fla.service.impl.CatService2 说明是registrar做了决定 由此我们可以得出一个优先级 @Service&#x3D;Selector&lt;Registra&lt;Processor ③5）和8）对比MyProcessor类： 123456789101112public class MyProcessor implements BeanDefinitionRegistryPostProcessor &#123; @Override public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) throws BeansException &#123; //1.首先获得一个BeanDefinition，因为下面的方法需要该方法需要 AbstractBeanDefinition beanDefinition = BeanDefinitionBuilder.rootBeanDefinition(CatService1.class).getBeanDefinition(); //2.不同于6），可以在初始化之前可以对bean的属性做更多细腻的控制，如下面就可以设置是否是单例 beanDefinition.setScope(&quot;singleton&quot;); //3.通过该方法传入beanDefinition来初始化一个bean registry.registerBeanDefinition(&quot;cat12&quot;,beanDefinition); &#125;&#125; 取得id名是catService，指定的是CatService1.class psvm： 123456public static void main(String[] args) &#123; AnnotationConfigApplicationContext ctx = new AnnotationConfigApplicationContext(SpringConfig2.class); ctx.registerBean(&quot;catService&quot;,CatService4.class); System.out.println(); System.out.println(&quot;catService: &quot;+ctx.getBean(&quot;catService&quot;).getClass()); &#125; 在bean都初始化结束后，我们手动的又注册了一个bean，名字&quot;catService&quot;，class是CatService4.class 结果： catService: class fla.service.impl.CatService4 结果是可以预见的，processor只是在容器bean初始化的整体过程中做了初始化决定，而我们在得到容器后又注册了一个具有相同id名的bean，肯定是目前这个将processor决定的所覆盖 初始化容器的最后阶段processor做裁定 容器初始化完毕后我们再注册自然会对容器中的bean进行覆盖 2.bean的加载控制1）编程控制eg：selector做控制 1234567891011121314151617public class MyImportSelector implements ImportSelector &#123; @Override public String[] selectImports(AnnotationMetadata importingClassMetadata) &#123; try &#123; //1.我们首先通过该路径去获得字节码，如果能够加载（项目中有mouse，不代表容器中有mouse） Class&lt;?&gt; clazz = Class.forName(&quot;com.fla.bean.Mouse&quot;); //2.如果clazz!=null，说明项目中有mouse，所以可以加载cat的bean if(clazz != null) &#123; return new String[]&#123;&quot;com.fla.bean.Cat&quot;&#125;; &#125; &#125; catch (ClassNotFoundException e) &#123; //2.报错说明com.fla.bean.Mouse这个路径中并没有mouse，直接返回 return new String[0]; &#125; return null; &#125;&#125; 现在我们就做到了这样一个效果 项目环境中有mouse，那么我们就加载cat 项目环境中有mouse，那么我们就不加载cat ​ 这样的逻辑可能会被反复用到，比如项目中有insect，我们就加载bird，否则不加载，这样的逻辑是可以抽取出来的，spring boot就将这样的逻辑抽取出来并设置成了若干注解供使用。这里只是那selector来做了一个示范，用registrar和processor是一样的 2）注解控制①@ConditionalOnClass ​ 注解控制虚拟机中有加载指定的类才加载对应的bean，即没有项目环境中有这个类 1234567public class AnimalConfig &#123; @ConditionalOnClass(name=&quot;com.fla.bean.Mouse&quot;) @Bean(&quot;tom&quot;) public Cat cat()&#123; return new Cat(); &#125;&#125; 这其实就是和1）中的效果是一样的，那这样有什么用呢？看下面这段代码： 1234567public class SpringConfig &#123; @Bean @ConditionalOnClass(name=&quot;com.mysql.jdbc.Driver&quot;) public DruidDataSource dataSource()&#123; return new DruidDataSource(); &#125;&#125; ​ 如果环境中有mysql的driver（pom里面导入了，说明要使用这个技术），那么就配置一个druid配置源，没有就不配置，因为不会用到相关技术 ②@ConditionalOnMissingClass ​ 注解控制虚拟机中没有加载指定的类才加载对应的bean，即没有项目环境中没有这个类 12345@Bean@ConditionalOnMissingClass(&quot;com.fla.bean.Dog&quot;)public Cat tom()&#123; return new Cat();&#125; ③@ConditionalOnBean ​ 可以判定是否加载了指定名称的bean。比如当前容器中已经提供了jdbcTemplate对应的bean，就没有必要再加载一个全新的jdbcTemplate的bean，这样容器中有的话就不用自己加载了。 12345@Bean@ConditionalOnBean(name=&quot;jerry&quot;)public Cat tom()&#123; return new Cat();&#125; ④@ConditionalOnWebApplication 下例是判定当前容器环境是否是web环境 12345@Bean@ConditionalOnWebApplicationpublic Cat tom()&#123; return new Cat();&#125; ⑤@ConditionalOnNotWebApplication 下面是判定容器环境是否是非web环境。 12345@Bean@ConditionalOnNotWebApplicationpublic Cat tom()&#123; return new Cat();&#125; ​ 总结 springboot定义了若干种控制bean加载的条件设置注解，由spring固定加载bean变成了可以根据情况选择性的加载bean 条件还可以做并且的逻辑关系，写2个就是2个条件都成立，写多个就是多个条件都成立 123456@Bean@ConditionalOnClass(name = &quot;com.fla.bean.Wolf&quot;)@ConditionalOnMissingClass(&quot;com.fla.bean.Mouse&quot;)public Cat tom()&#123; return new Cat();&#125; 除了bean在config类中作为方法返回参数进行加载，自定以的bean类也可以使用该注解，只需要被@Import或者@ComponentScan到即可 12345@Component@ConditionalOnClass(name=&quot;fla.bean.Mouse&quot;)public class Cat &#123;&#125; 3.bean依赖属性的配置​ bean在运行的时候，实现对应的业务逻辑时有可能需要开发者提供一些设置值，有就是属性了。如果使用构造方法将参数固定，灵活性不足，这个时候就可以使用前期学习的bean的属性配置相关的知识进行灵活的配置了。先通过yml配置文件，设置bean运行需要使用的配置信息。 ①在yml配置bean所需要的的相关属性 1234567cartoon: cat: name: &quot;图多盖洛&quot; age: 5 mouse: name: &quot;泰菲&quot; age: 1 ②然后定义一个封装属性的专用类，加载配置属性，读取对应前缀相关的属性值 12345678//@Component 这里一定不要写这个，因为写这个就代表100%是一个bean，而在CartoonCatAndMouse中配置了、@EnableConfigurationProperties，这样这个类就会有两个 bean在容器中@ConfigurationProperties(prefix = &quot;cartoon&quot;)@Data //必须加上，因为没有自己写get和set方法，boot不能从yml中加载属性public class CartoonProperties &#123; private Cat cat; private Mouse mouse;&#125; 当yml中配置的cat有值的时候就会将值注入进该属性类的cat中，如果没有再yml中配置cat，那么这里的cat就是null ③最后在使用的位置注入对应的配置即可 1234567//加载该类的时候将CartoonProperties.class当做一个bean交给spring管理，将这两个类绑定起来@EnableConfigurationProperties(CartoonProperties.class) public class CartoonCatAndMouse&#123; //自动装载属性类，因为此时属性类已经交给spring管理并从yml注入属性了 @Autowired private CartoonProperties cartoonProperties;&#125; 注意： 在CartoonCatAndMouse中我们要设定@EnableConfigurationProperties(CartoonProperties.class)，这样在需要用到该bean的时候，就会把其中指定的CartoonProperties.class当做一个bean来进行相关属性的加载，这样做就将CartoonCatAndMouse和CartoonProperties进行了一个绑定，只要用到前者，那么后者就会被当做一个bean并加载。 CartoonProperties专门作为CartoonCatAndMouse的属性类，目的是解耦，因为若我们在yml中没有配置，直接在CartoonCatAndMouse中加载属性的话，就会报错，因为没有配置相关属性。 当我们为该类配置了一个属性类，如果属性类中加载了yml的值那就用该值，否则我们就用默认值，这样就完成了解耦 4.自动装配1）基本思路阶段一：准备阶段 springboot的开发人员先大量收集Spring开发者的编程习惯，整理开发过程每一个程序经常使用的技术列表，形成一个技术集A 收集常用技术(技术集A)的使用参数，不管你用什么常用设置，我用什么常用设置，统统收集起来整理一下，得到开发过程中每一个技术的常用设置，形成每一个技术对应的设置集B 阶段二：加载阶段 springboot初始化Spring容器基础环境，读取用户的配置信息，加载用户自定义的bean和导入的其他坐标，形成初始化环境 springboot将技术集A包含的所有技术在SpringBoot启动时默认全部加载，这时肯定加载的东西有一些是无效的，没有用的 springboot会对技术集A中每一个技术约定出启动这个技术对应的条件，并设置成按条件加载，由于开发者导入了一些bean和其他坐标，也就是与初始化环境，这个时候就可以根据这个初始化环境与springboot的技术集A进行比对了，哪个匹配上加载哪个 因为有些技术不做配置就无法工作，所以springboot开始对设置集B下手了。它统计出各个国家各个行业的开发者使用某个技术时最常用的设置是什么，然后把这些设置作为默认值直接设置好，并告诉开发者当前设置我已经给你搞了一套，你要用可以直接用，这样可以减少开发者配置参数的工作量 但是默认配置不一定能解决问题，于是springboot开放修改设置集B的接口，可以由开发者根据需要决定是否覆盖默认配置 2）源码分析①加载技术 通过@SpringBootApplication我们可以看到该注解主要包含①②③三部分功能，我们需要对该三部分功能做一个分析，当拆解了这三部分注解及其子注解后，我们可以看到如下结构 ①中首先我们可以看到 12@Configuration @Component ​ 这样两个注解，@Configuration中包含了@Component，@Component能够使被注解的bean作为组件被spring扫描进容器，而@Configuration则多了一个proxyBeanMethods()属性，默认设置为true，当值&#x3D;true的时候，这个类中方法返回的bean就永远都是同一个对象，且该类和方法返回的bean都是以cglib动态代理的方式生成的。若将值设置为false，那么该类和类中方法返回的bean的class都是该类的全限定类名，并不会以cglib的方式生成，同时，该类中**@bean类方法返回的bean就不再是同一个，每次返回的都是不同的bean**。 至于@Indexed，它与spring的加速启动有关 再来看③ 12345678@ComponentScan( excludeFilters = &#123;@ComponentScan.Filter( type = FilterType.CUSTOM, classes = &#123;TypeExcludeFilter.class&#125; ), @ComponentScan.Filter( type = FilterType.CUSTOM, classes = &#123;AutoConfigurationExcludeFilter.class&#125; )&#125; ​ 这一段注解实际上就是说明了一件事，当扫描的时候会根据一定的条件，选择性的进行扫描，有些会扫描到，有些并不会扫描，这其实就是加载bean过程的一个扫描策略 重点在②中 12@AutoConfigurationPackage @Import(&#123;AutoConfigurationPackages.Registrar.class&#125;) 这里的功能主要由子注解 @Import(&#123;AutoConfigurationPackages.Registrar.class&#125;)实现，点进Registrar.class ​ 我们可以清晰地看到，该类继承了ImportBeanDefinitionRegistrar，通过前面的知识可以知道，ImportBeanDefinitionRegistrar可以根据BeanDefinition来加载类，同时可以在BeanDefinition中设置如scope等的属性，并且可以在使用BeanDefinition初始化bean的时候做一些控制。 关注这一段参数，通过右键evalue计算我们可以得出，这一段参数实际上就是得到了包名（应到类所在包），我们继续进入该方法 对于else中的代码，我们很熟悉，其实就是类似于下面的代码 12345678910@Override public void registerBeanDefinitions(AnnotationMetadata metaData, BeanDefinitionRegistry registry) &#123; //1.首先获得一个BeanDefinition，因为下面的方法需要该方法需要 AbstractBeanDefinition beanDefinition = BeanDefinitionBuilder.rootBeanDefinition(Cat.class).getBeanDefinition(); //2.不同于6），可以在初始化之前可以对bean的属性做更多细腻的控制，如下面就可以设置是否是单例 beanDefinition.setScope(&quot;singleton&quot;); //3.通过该方法传入beanDefinition来初始化一个bean registry.registerBeanDefinition(&quot;cat11&quot;,beanDefinition); &#125; 第一个参数是bean的id名，第二个参数是这个bean的definition，点进第二个参数 第一行主要是加载字节码信息，第二行主要是设置权限，重点在第三行 这里其实就做了一件事，spring需要知道扫哪些包，所以需要把这些包添加记录在一个地方。 为什么是add呢？因为这里可以设定扫不同的包，所以这里时添加，我们可以看③这个注解 这里就可以设置扫描哪些包 到此，②中的第一个注解就结束了，现在来查看②中的第二个注解**@Import({AutoConfigurationImportSelector.class})** 点进这个注解，我们可以看到它实现了很多接口 这里主要可以分为三类： Aware结尾的接口 当我们在某个bean中实现该接口，该接口就可以拿到applicationContext这个对象并进行一些操作，如sout容器中所有bean的name 1234567891011121314private ApplicationContext applicationContext;@Overridepublic void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; this.applicationContext=applicationContext;&#125;public void f2()&#123; String[] names = applicationContext.getBeanDefinitionNames(); for (String name:names)&#123; System.out.println(name); &#125;&#125; 注意：我们在psvm中很容易实现，但是在bean类中需要以这样的方式拿到applicationContext这个对象 所以4个ware结束的接口就可以保证目前boot环境中能使用这种方式 Ordered ​ bean的加载是需要顺序要求的，若50号bean依赖30号，那30先50后没问题，但是50先30后就会出错了，这时候就需要实现Ordered接口来为每个bean设置优先级，保证bean的加载符合一定的顺序 DeferredImportSelector：Deferre代表延迟，说明实现这个接口会延迟加载一些东西，我们点进去再看 我们可以看到该接口中定义了一个接口叫Group，并且该接口继承了ImportedSelector，细看Group，其中包含一个process方法 我们返回DeferredImportSelector，查看其实现的process方法： 在这里我们重点关注该方法： 123456789101112131415161718protected AutoConfigurationImportSelector.AutoConfigurationEntry getAutoConfigurationEntry(AnnotationMetadata annotationMetadata) &#123; //1.这里是判断注解是否有效 if (!this.isEnabled(annotationMetadata)) &#123; return EMPTY_ENTRY; &#125; else &#123; //2.在这里获得了一些attributes，这些attributes就是@EnableAutoConfiguration中的2个属性 AnnotationAttributes attributes = this.getAttributes(annotationMetadata); //3.这里涉及到一个getCandidateConfigurations传入了attributes List&lt;String&gt; configurations = this.getCandidateConfigurations(annotationMetadata, attributes); configurations = this.removeDuplicates(configurations); Set&lt;String&gt; exclusions = this.getExclusions(annotationMetadata, attributes); this.checkExcludedClasses(configurations, exclusions); configurations.removeAll(exclusions); configurations = this.getConfigurationClassFilter().filter(configurations); this.fireAutoConfigurationImportEvents(configurations, exclusions); return new AutoConfigurationImportSelector.AutoConfigurationEntry(configurations, exclusions); &#125;&#125; 1.这里首先判断了注解是否有效，Metadata其实就是代表了写了注解的类，也就是引导类，这里是在判断引导类中的注解是否有效 2.中获得的属性就是②**@EnableAutoConfiguration中的exclude和excludeName** 通过名字我们就可以知道这是在排除一些东西，实际上这里就是attributes就是在装配的时候剔除的技术集，这些剔除的技术就是我们传入的参数 3.根据getCandidateConfigurations这个方法名我们可以知道，这个方法就是用来加载候选的config的，我们进入这个方法看看 在这个方法中我们可以看到其通过调用了一个loadFactoryNames方法获得了一个集合，显然这就是配置的集合 我们再进入loadFactoryNames方法中： 这里得到了类加载器，最后return调用了一个**(List)loadSpringFactories(classLoaderToUse)**方法，我们继续查看 在这个方法中我们可以清晰地看到有这样一个信息，boot加载了外部资源，那么这个位置在哪儿呢？ ​ 我们查看spring-boot-starter对应的spring-boot-autoconfigure的jar包，这里面就有我们所需要的东西 查看这里面的内容： 这段配置就是要加载的技术集，我们继续刚才的方法 至此我们就知道spring boot在读取了这些配置信息后将其存入了configurations这个list中 以上所有的操作都是为了一个目的： 将所有的技术集从jar包的配种中加载 ②默认配置当我们找到spring-boot-starter对应的spring-boot-autoconfigurejar包 我们查看该类，整个类的定义就非常熟悉了： 123456789101112131415161718192021222324252627282930@Configuration( proxyBeanMethods = false//1.)@ConditionalOnClass(&#123;RedisOperations.class&#125;)//2.@EnableConfigurationProperties(&#123;RedisProperties.class&#125;)//3.@Import(&#123;LettuceConnectionConfiguration.class, JedisConnectionConfiguration.class&#125;)//4.public class RedisAutoConfiguration &#123; public RedisAutoConfiguration() &#123; &#125; @Bean @ConditionalOnMissingBean(//5. name = &#123;&quot;redisTemplate&quot;&#125; ). @ConditionalOnSingleCandidate(RedisConnectionFactory.class)//6. public RedisTemplate&lt;Object, Object&gt; redisTemplate(RedisConnectionFactory redisConnectionFactory) &#123; RedisTemplate&lt;Object, Object&gt; template = new RedisTemplate(); template.setConnectionFactory(redisConnectionFactory); return template; &#125; @Bean @ConditionalOnMissingBean @ConditionalOnSingleCandidate(RedisConnectionFactory.class) public StringRedisTemplate stringRedisTemplate(RedisConnectionFactory redisConnectionFactory) &#123; StringRedisTemplate template = new StringRedisTemplate(); template.setConnectionFactory(redisConnectionFactory); return template; &#125;&#125; 1.我们可以看到这个类上加了@Configuration注解，并且proxyBeanMethods &#x3D; false，说明该类方法返回的多个bean就不再是同一个类，是多例的 2.可以看到本类的加载时又条件的，这个条件我们也可以很容易的想到，即在pom文件中导入了redis相关的jar包，当boot检测到环境中导入了redis的jar包，说明需要该技术，那么就会加载该redis的默认配置类 3.redisConfiguration作为配置类，需要加载一些属性，所以通过这样的方式来做到解耦，因此当加载该类的时候，@EnableConfigurationProperties({RedisProperties.class})使属性类被视作一个bean进行加载 4.根据名字我们也可以看出，这是关于redis的两种客户端实现，我们用到了redis技术自然需要用到redis对应的客户端技术 5.这里说了一件事，如果我们自己没有定义一个id名叫redisTemplate的bean，即容器中不存在，boot才会加载该redisTemplate 6.boot要自动加载RedisTemplate还需要一个条件，即容器中只有一个指定的bean也就是RedisConnectionFactory，boot才会自动加载RedisTemplate，这是一个接口，我们查看其实现类 这里由lettuce和jedis两种实现，而我们默认的是lettuce实现 通过redis的加载配置，我们对boot自动装配的实现有了一个及本认识： 当我们加载完技术集A后，我们会加载默认配置，这就相当于我们自己配置config配置类，配置类中的属性另写在properties类中，通过**EnableConfigurationProperties({RedisProperties.class})**来进行一个绑定。 加载的条件通过**@Conditiona….**来进行控制 3）自定义自动装配目录结构： CatRun类： 这就相当于我们想要自动加载的技术 1234567891011121314151617@Service//这里代表只有环境中有redis对应的核心技术我们才加载@ConditionalOnClass(name = &quot;org.springframework.data.redis.core.RedisOperations&quot;)@EnableConfigurationProperties(CatRunProperties.class)public class CatRun &#123; @Autowired private CatRunProperties cr; public void f1()&#123; String[] names = applicationContext.getBeanDefinitionNames(); for (String name:names)&#123; System.out.println(name); &#125; &#125;&#125; CatRunProperties： 这就是我们想要加载的技术的属性类 12345@ConfigurationProperties(prefix = &quot;catrun&quot;)@Datapublic class CatRunProperties &#123; private Cat cat;&#125; META-INF&#x2F;spring.factories配置： 12org.springframework.boot.autoconfigure.EnableAutoConfiguration=\\com.fla.service.CatRun 注意格式，以下是一个模板： 1234# Auto Configureorg.springframework.boot.autoconfigure.EnableAutoConfiguration=\\org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration,\\org.springframework.boot.autoconfigure.aop.AopAutoConfiguration 这样我们就自定义了一个自动装配，并且这个类需要redis技术存在环境中的时候才会自动装配 4）变更自动装配方式一：通过yaml配置设置排除指定的自动配置类 1234spring: autoconfigure: exclude: - org.springframework.boot.autoconfigure.task.TaskExecutionAutoConfiguration 方式二：通过注解参数排除自动配置类 1@EnableAutoConfiguration(excludeName = &quot;&quot;,exclude = &#123;&#125;) 我们可以直接在引导类的注解**@SpringBootApplication中定义，因为该注解就包含在@SpringBootApplication**中 方式三：排除坐标（应用面较窄） ​ 如果当前自动配置中包含有更多的自动配置功能，也就是一个套娃的效果。此时可以通过检测条件的控制来管理自动配置是否启动。例如web程序启动时会自动启动tomcat服务器，可以通过排除坐标的方式，让加载tomcat服务器的条件失效。不过需要提醒一点，把tomcat排除掉，记得再加一种可以运行的服务器。 123456789101112131415161718&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;!--web起步依赖环境中，排除Tomcat起步依赖，匹配自动配置条件--&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!--添加Jetty起步依赖，匹配自动配置条件--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jetty&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; ​ 这种方法思路很简单，因为自动加载tomcat肯定有一个**@conditiona…**来判断环境中是否有tomcat的jar（相关类），我们排除这些jar，那么tomcat的自动装配就不会进行了。 5.自定义一个starter定义一个ip访问统计操作，每发送一次请求，会统计相应ip数量，结构如下： 1）IpAutoConfiguration123456789@EnableScheduling//@EnableConfigurationProperties(IpCountProperties.class)@Import(&#123;IpCountProperties.class, SpringMvcConfig.class&#125;)public class IpAutoConfiguration &#123; @Bean public IpCountService ipCount()&#123; return new IpCountService(); &#125;&#125; 注意这里并没有使用**@EnableConfigurationProperties(IpCountProperties.class)而是使用了@Import({IpCountProperties.class, SpringMvcConfig.class})**，下面会分析为什么这样使用 2）IpCountProperties该类是service的属性类，yml注入属性 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354@Component(&quot;ipCountProperties&quot;)@ConfigurationProperties(prefix = &quot;iptool.count&quot;)public class IpCountProperties &#123; /** * 日志显示周期 */ private Long cycle = 5L; /** * 是否周期内重置数据 */ private Boolean cycleReset = false; /** * 日志输出模式 detail：详细模式 simple：极简模式 */ private String model = LogModel.DETAIL.value; public enum LogModel&#123; DETAIL(&quot;detail&quot;), SIMPLE(&quot;simple&quot;); private String value; LogModel(String value) &#123; this.value = value; &#125; public String getValue() &#123; return value; &#125; &#125; public Long getCycle() &#123; return cycle; &#125; public void setCycle(Long cycle) &#123; this.cycle = cycle; &#125; public Boolean getCycleReset() &#123; return cycleReset; &#125; public void setCycleReset(Boolean cycleReset) &#123; this.cycleReset = cycleReset; &#125; public String getModel() &#123; return model; &#125; public void setModel(String model) &#123; this.model = model; &#125;&#125; 该类定义了三个主要属性cycle，cycleReset，model，这些参数都有默认值 @ConfigurationProperties(prefix &#x3D; “iptool.count”)代表该类可通过yml注入 注意：这里单独使用了@Component(“ipCountProperties”)，这是因为在IpCountService里的cron表达式注入需要 3）IpCountService该类为完成ip统计的操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class IpCountService &#123; private Map&lt;String,Integer&gt; ipCountMap = new HashMap&lt;String,Integer&gt;(); @Autowired //当前的request对象的注入工作由使用当前starter的工程提供自动装配 private HttpServletRequest httpServletRequest; @Autowired private IpCountProperties ipProperties; /** * 计数操作 */ public void count()&#123; //每次调用当前操作，就记录当前访问的IP，然后累加访问次数 //1.获取当前操作的IP地址 String ip = httpServletRequest.getRemoteAddr(); //2.根据IP地址从Map取值，并递增 ipCountMap.put(ip,ipCountMap.getOrDefault(ip, 0)+1); &#125; /** * 打印操作 */ @Scheduled(cron = &quot;0/#&#123;ipCountProperties.cycle&#125; * * * * ?&quot;) public void print()&#123; if(ipProperties.getModel().equals(IpCountProperties.LogModel.DETAIL.getValue()))&#123; System.out.println(&quot; IP访问监控&quot;); System.out.println(&quot;+-----ip-address-----+--num--+&quot;); for (Map.Entry&lt;String, Integer&gt; entry : ipCountMap.entrySet()) &#123; String key = entry.getKey(); Integer value = entry.getValue(); System.out.println(String.format(&quot;|%18s |%5d |&quot;,key,value)); &#125; System.out.println(&quot;+--------------------+-------+&quot;); &#125;else if(ipProperties.getModel().equals(IpCountProperties.LogModel.SIMPLE.getValue()))&#123; System.out.println(&quot; IP访问监控&quot;); System.out.println(&quot;+-----ip-address-----+&quot;); for (String key: ipCountMap.keySet()) &#123; System.out.println(String.format(&quot;|%18s |&quot;,key)); &#125; System.out.println(&quot;+--------------------+&quot;); &#125; if(ipProperties.getCycleReset())&#123; ipCountMap.clear(); &#125; &#125;&#125; 本类中的属性都来自自动注入的ipProperties，但是cron表达式这里有个问题，因为表达式的注入不能直接通过java代码获得，如果这里用@Value即**cron = &quot;0/$(&quot;ip-tool.count.cycle&quot;) * * * * ?&quot;**表示直接从yml中获得，这样会带来一个问题：这种方式根本没用到注入进ipProperties的cycle值，并且如果用户没在yml配置该属性，那么这里就没有值会报错。 正确的做法应该是从ipProperties的bean中获得注入的属性，如果我们使用**@EnableConfigurationProperties(IpCountProperties.class)使IpProperties加载成了一个bean，会出现一个问题，即boot根据其命名规则生成的名字是这样的：tools.ip-cn.itcast.properties.IpProperties，并不是我们想象的ipProperties，那么我们用该id名在cron里面：cron = &quot;0/#&#123;tools.ip-cn.itcast.properties.IpProperties.cycle&#125; * * * * ?&quot;**，这样boot就会把tools当做一个bean，因为这是后面的内容当做bean的属性，这是错误的。 我们要解决上面问题，所以我们需要自己给bean取名字，那么我们就要用到，@Component并且在IpAutoConfiguration使用@Import扫描到它，这样我们的问题就解决了 4）拦截器开发①SpringMvcConfig这是web配置拦截器的类 123456789101112131415@Configuration(&quot;springMvcConfig&quot;)public class SpringMvcConfig implements WebMvcConfigurer &#123; @Override public void addInterceptors(InterceptorRegistry registry) &#123; //这里获得bean直接调用了ipCountInterceptor()方法，该方法需要保证每次ipCountInterceptor()都是同一个对象， // 所以需要加上@Configuration（true）注解，保证每次获得的ipCountInterceptor()都是同一个 registry.addInterceptor(ipCountInterceptor()).addPathPatterns(&quot;/**&quot;); &#125; @Bean public IpCountInterceptor ipCountInterceptor()&#123; return new IpCountInterceptor(); &#125;&#125; 拦截器需要用到web的技术，我们在这里需要获得拦截器并定义其顺序，因为这里使用方法获取的，所以要保证每次通过该方法获取的拦截器都是同一个bean，所以需要@Configuration(&quot;springMvcConfig&quot;)，默认使用代理&#x3D;true 注意：保证该config被扫描到，在IpAutoConfiguration中要@Import ②IpCountInterceptor拦截器的具体定义类 12345678910public class IpCountInterceptor implements HandlerInterceptor &#123; @Autowired private IpCountService ipCountService; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; ipCountService.count(); return true; &#125;&#125; 5）Meta-INF&#x2F;spring.factories12org.springframework.boot.autoconfigure.EnableAutoConfiguration=\\ cn.fla.ipcountspringboostarter.autoConfig.IpAutoConfiguration 这样在boot从这里读取到IpAutoConfiguration后，会根据注解有序地扫描到我们定义的bean 6）yml的提示需要在Meta-INF下提供这样一个提示文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&#123; &quot;groups&quot;: [ &#123; &quot;name&quot;: &quot;ip-tool.count&quot;, #这是yml的prefix &quot;type&quot;: &quot;cn.fla.properties.IpProperties&quot;, &quot;sourceType&quot;: &quot;cn.fla.properties.IpProperties&quot; &#125; ], &quot;properties&quot;: [ &#123; &quot;name&quot;: &quot;ip-tool.count.cycle&quot;, #这是yml的key &quot;type&quot;: &quot;java.lang.Long&quot;, #类型 &quot;description&quot;: &quot;日志显示周期&quot;, #描述 &quot;sourceType&quot;: &quot;cn.fla.properties.IpProperties&quot;, #属性类的全路径 &quot;defaultValue&quot;: 5 #默认值 &#125;, &#123; &quot;name&quot;: &quot;ip-tool.count.cycle-reset&quot;, &quot;type&quot;: &quot;java.lang.Boolean&quot;, &quot;description&quot;: &quot;是否周期内重置数据&quot;, &quot;sourceType&quot;: &quot;cn.fla.properties.IpProperties&quot;, &quot;defaultValue&quot;: false &#125;, &#123; &quot;name&quot;: &quot;ip-tool.count.model&quot;, &quot;type&quot;: &quot;java.lang.String&quot;, &quot;description&quot;: &quot;日志输出模式 detail：详细模式 simple：极简模式&quot;, &quot;sourceType&quot;: &quot;cn.fla.properties.IpProperties&quot; &#125; ], &quot;hints&quot;: [ # 这样就是弹出提示，弹出的提示是根据我们在properties中写的注解显示的 &#123; &quot;name&quot;: &quot;ip-tool.count.model&quot;, &quot;values&quot;: [ &#123; &quot;value&quot;: &quot;detail&quot;, &quot;description&quot;: &quot;详细模式.&quot; &#125;, &#123; &quot;value&quot;: &quot;simple&quot;, &quot;description&quot;: &quot;极简模式.&quot; &#125; ] &#125; ]&#125; yml中的配置格式如下：","categories":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"http://example.com/categories/Spring-Boot/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"http://example.com/tags/Spring-Boot/"}]},{"title":"Spring Boot实用开发","slug":"spring Boot实用开发","date":"2022-04-12T04:06:12.000Z","updated":"2022-08-18T04:30:03.138Z","comments":true,"path":"2022/04/12/spring Boot实用开发/","link":"","permalink":"http://example.com/2022/04/12/spring%20Boot%E5%AE%9E%E7%94%A8%E5%BC%80%E5%8F%91/","excerpt":"程序打包、配置优先级、多环境开发、日志、热部署、数据注入（松散绑定）、测试相关、数据库整合、缓存整合、定时任务整合、邮件、监控","text":"程序打包、配置优先级、多环境开发、日志、热部署、数据注入（松散绑定）、测试相关、数据库整合、缓存整合、定时任务整合、邮件、监控 1.程序打包1）不带插件打包当我们**注释掉**如下pluging： 12345678&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 执行mvn打包后会得到如下的package 而当我们运行会发现无法运行，出现以下情况 2）带插件打包当我们**添加**如下pluging： 12345678&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 执行mvn打包后会得到如下的package 当我们执行该jar，发现可以正常执行： 3）两者对比①大小对比当我们查看两者的大小，可以看出带插件package后会大很多 当我们解压jar包中的内容 插件打包： 不带插件打包： ​ 通过以上对比，我们知道了，带插件打包为了让boot的jar独立运行，会将程序依赖的jar包一起打包，正是这些jar包导致了package出来的jar的大小较大 ②启动对比​ 在带插件package的最外层有一个org文件夹，其中有如下内容 进入目录：org\\springframework\\boot\\loader，在里面可以找到一个JarLauncher.class的文件，这显然是用于启动的class ​ 然后回到两个程序包的最外层目录，查看名称相同的文件夹META-INF下都有一个叫做MANIFEST.MF的文件，但是大小不同，打开文件，比较内容区别 小容量文件的MANIFEST.MF 12345Manifest-Version: 1.0Implementation-Title: springboot_08_ssmpImplementation-Version: 0.0.1-SNAPSHOTBuild-Jdk-Spec: 1.8Created-By: Maven Jar Plugin 3.2.0 大容量文件的MANIFEST.MF 123456789101112Manifest-Version: 1.0Spring-Boot-Classpath-Index: BOOT-INF/classpath.idxImplementation-Title: springboot_08_ssmpImplementation-Version: 0.0.1-SNAPSHOTSpring-Boot-Layers-Index: BOOT-INF/layers.idxStart-Class: com.itheima.SSMPApplication //引导类名Spring-Boot-Classes: BOOT-INF/classes/Spring-Boot-Lib: BOOT-INF/lib/Build-Jdk-Spec: 1.8Spring-Boot-Version: 2.5.4Created-By: Maven Jar Plugin 3.2.0Main-Class: org.springframework.boot.loader.JarLauncher //jar中需要执行的类，该类会找到引导类并执行 ​ 大文件中明显比小文件中多了几行信息，其中最后一行信息是Main-Class: org.springframework.boot.loader.JarLauncher。如果使用java -jar执行此程序包，将执行Main-Class属性配置的类，这个类恰巧就是前面看到的那个文件。SpringBoot打包程序中出现Spring框架的东西是为这里服务的。而这个org.springframework.boot.loader.JarLauncher类内部要查找Start-Class属性中配置的类，并执行对应的类。这个属性在当前配置中也存在，对应的就是我们的引导类类名。 现在我们就明白了这一组设定的作用： SpringBoot程序添加配置后会打出一个特殊的包，包含Spring框架部分功能，原始工程内容，原始工程依赖的jar包 首先读取MANIFEST.MF文件中的Main-Class属性，用来标记执行java -jar命令后运行的类 JarLauncher类执行时会找到Start-Class属性，也就是启动类类名 运行启动类时会运行当前工程的内容 运行当前工程时会使用依赖的jar包，从lib目录中查找 ​ SpringBoot打出来了包为了能够独立运行，将所有需要使用的资源全部都添加到了这个包里，这就是为什么这个jar包能独立运行的原因。 4）相关命令①windows12345678910# 查询端口netstat -ano# 查询指定端口netstat -ano |findstr &quot;端口号&quot;# 根据进程PID查询进程名称tasklist |findstr &quot;进程PID号&quot;# 根据PID杀死任务taskkill /F /PID &quot;进程PID号&quot;# 根据进程名称杀死任务，不常用，因为同一个进程名常常对应多个服务taskkill -f -t -im &quot;进程名称&quot; ​ ②linux123456#根据条件查询端口信息ps -ef | grep &quot;java -jar&quot; #后台执行并指定日志文件nohup java -jar &#x27;jar名&#x27; &gt; &#x27;文件名&#x27; 2&gt;&amp;1 &amp;#结束进程kiil -9 &#x27;pid&#x27; ​ 2.配置1）临时属性配置①命令行​ yml配置： 123456789server: port: 8080spring: datasource: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:3306/ssm_db username: root password: root ​ SpringBoot提供了灵活的配置方式，如果你发现你的项目中有个别属性需要重新配置，可以使用临时属性的方式快速修改某些配置。方法也特别简单，在启动的时候添加上对应参数就可以了。 1java –jar springboot.jar –-server.port=80 ​ 上面的命令是启动SpringBoot程序包的命令，在命令输入完毕后，空一格，然后输入两个-号。下面按照属性名&#x3D;属性值的形式添加对应参数就可以了。记得，这里的格式不是yaml中的书写格式，当属性存在多级名称时，中间使用点分隔，和properties文件中的属性格式完全相同。 ​ 如果你发现要修改的属性不止一个，可以按照上述格式继续写，属性与属性之间使用空格分隔。 1java –jar springboot.jar –-server.port=80 --spring.datasource.password=1234 ②idea中​ 打开SpringBoot引导类的运行界面，在里面找到配置项。其中Program arguments对应的位置就是添加临时属性的，可以加几个试试效果。 运行main方法的时候，arg所接收的参数就是这个位置配置的参数，通过这个args就可以获取到参数： 123public static void main(String[] args) &#123; SpringApplication.run(SSMPApplication.class,args);//提供args&#125; ​ Idea中配置的临时参数就是通过这个位置传递到我们的程序中的，如果不用这个args是不是就断开了外部传递临时属性的入口。我们可以使用下面的调用方式，这样外部临时属性就无法进入到SpringBoot程序中了。 123public static void main(String[] args) &#123; SpringApplication.run(SSMPApplication.class);//不提供args&#125; ​ 我们还可以通过以下的方式进行测试，这时候程序会认为参数是arg而不是args 12345public static void main(String[] args) &#123; String[] arg = new String[1]; arg[0] = &quot;--server.port=8082&quot;; SpringApplication.run(SSMPApplication.class, arg);&#125; ③属性加载优先级https://docs.spring.io/spring-boot/docs/current/reference/html/spring-boot-features.html#boot-features-external-config Default properties (specified by setting SpringApplication.setDefaultProperties). @PropertySource annotations on your @Configuration classes. Please note that such property sources are not added to the Environment until the application context is being refreshed. This is too late to configure certain properties such as logging.* and spring.main.* which are read before refresh begins. Config data (such as application.properties files). A RandomValuePropertySource that has properties only in random.*. OS environment variables. Java System properties (System.getProperties()). JNDI attributes from java:comp/env. ServletContext init parameters. ServletConfig init parameters. Properties from SPRING_APPLICATION_JSON (inline JSON embedded in an environment variable or system property). Command line arguments. properties attribute on your tests. Available on @SpringBootTest and the test annotations for testing a particular slice of your application. @TestPropertySource annotations on your tests. Devtools global settings properties in the $HOME/.config/spring-boot directory when devtools is active. 其中，3就是程序内配置，11就是命令行配置，优先级↓（由上至下升高） 2）4级配置文件​ SpringBoot提供了配置文件和临时属性的方式来对程序进行配置，spring提供了4级配置文件： 类路径下配置文件（resources目录中的application.yml文件） 类路径下config目录下配置文件 程序包所在目录中配置文件 程序包所在目录中config目录下配置文件 ​ 它们的优先级如下： file ：config&#x2F;application.yml 【最高】 file ：application.yml classpath：config&#x2F;application.yml classpath：application.yml 【最低】 4级配置文件，相同的配置低级会被高级的覆盖，不同配置会一起生效，其使用环境如下 项目类路径配置文件：服务于开发人员本机开发与测试 项目类路径config目录中配置文件：服务于项目经理整体调控 工程路径配置文件：服务于运维人员配置涉密线上环境 工程路径config目录中配置文件：服务于运维经理整体调控 3）自定义配置文件​ 自定义配置文件方式有如下两种： 方式一：使用临时属性设置配置文件名，注意仅仅是名称，不要带扩展名 ​ 参数名：spring.config.name 方式二：使用临时属性设置配置文件路径，这个是全路径名 ​ 参数名：spring.config.location ​ 也可以设置加载多个配置文件 ​ 多个配置文件，优先级最高的是最后设置的这个配置文件 注意：使用的属性一个是pring.config.name，另一个是spring.config.location， 总结 配置文件可以修改名称，通过启动参数设定 配置文件可以修改路径，通过启动参数设定 微服务开发中配置文件通过配置中心进行设置 3.多环境开发1）配置多环境①单一文件​ 每个环境用—进行隔开 123456789101112131415161718192021222324spring: profiles: active: pro # 启动pro---spring: config: activate: on-profile: proserver: port: 80---spring: config: activate: on-profile: proserver: port: 81---spring: config: activate: on-profile: proserver: port: 82 ​ 其中关于环境名称定义上述格式是标准格式，过时格式如下： 12spring: profiles: test ②多文件主配置文件 123spring: profiles: active: pro # 启动pro application-pro.yaml 12server: port: 80 application-dev.yaml 12server: port: 81 ​ 文件的命名规则为：application-环境名.yml。 ​ 在配置文件中，如果某些配置项所有环境都一样，可以将这些项写入到主配置中，只有哪些有区别的项才写入到环境配置文件中。 主配置文件中设置公共配置（全局） 环境分类配置文件中常用于设置冲突属性（局部） 作用： 可以使用独立配置文件定义环境属性 独立配置文件便于线上系统维护更新并保障系统安全性 2）配置拆分将所有的配置根据功能对配置文件中的信息进行拆分，并制作成独立的配置文件，命名规则如下 application-devDB.yml application-devRedis.yml application-devMVC.yml ①include方式​ 使用include属性在激活指定环境的情况下，同时对多个环境进行加载使其生效，多个环境间使用逗号分隔 1234spring: profiles: active: dev include: devDB,devRedis,devMVC ​ 相当于加载dev配置时，再加载对应的3组配置，结构上很清晰，用了什么，对应的名称是什么 注意：加载的顺序是**devDB,devRedis,devMVC,dev**，dev是在最后才加载的 覆盖的顺序是后面覆盖前面（优先级高–&gt;低） ②group方式（主流）​ include的设置有一个问题，比如要切换dev环境为pro时，include也要修改，因为include属性只能使用一次。 ​ SpringBoot从2.4版开始使用group属性替代include属性，降低了配置书写量。 1234567spring: profiles: active: dev group: &quot;dev&quot;: devDB,devRedis,devMVC &quot;pro&quot;: proDB,proRedis,proMVC &quot;test&quot;: testDB,testRedis,testMVC ​ 现在再来看，如果切换dev到pro，只需要改以下active 注意：加载的顺序是**dev,devDB,devRedis,devMVC**，dev是最开始就加载了 覆盖的顺序是后面覆盖前面（优先级高–&gt;低） 3）maven控制环境​ 目前我们的环境还是由spring控制的，我们需要手动修改active，我们应该通过maven来控制这里选择的版本 1234567spring: profiles: active: dev group: &quot;dev&quot;: devDB,devRedis,devMVC &quot;pro&quot;: proDB,proRedis,proMVC &quot;test&quot;: testDB,testRedis,testMVC maven中设置多环境（使用属性方式区分环境） 123456789101112131415161718&lt;profiles&gt; &lt;profile&gt; &lt;id&gt;env_dev&lt;/id&gt; &lt;properties&gt; &lt;profile.active&gt;dev&lt;/profile.active&gt; &lt;!--profile.active是自己定义的变量，方便我们取值--&gt; &lt;/properties&gt; &lt;activation&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;!--默认启动环境--&gt; &lt;/activation&gt; &lt;/profile&gt; &lt;profile&gt; &lt;id&gt;env_pro&lt;/id&gt; &lt;properties&gt; &lt;profile.active&gt;pro&lt;/profile.active&gt; &lt;/properties&gt; &lt;/profile&gt;&lt;/profiles&gt; SpringBoot中读取maven设置值 123spring: profiles: active: @profile.active@ ​ 上面的@属性名@就是读取maven中配置的属性值的语法格式，这样我们就通过了maven所定义的变量来控制环境了 4.日志1）使用日志步骤①：添加日志对象log 12345678910111213141516@RestController@RequestMapping(&quot;/books&quot;)public class BookController extends BaseClass&#123; //添加日志对象 private static final Logger log = LoggerFactory.getLogger(BookController.class); @GetMapping public String getById()&#123; //写日志 log.debug(&quot;debug...&quot;); log.info(&quot;info...&quot;); log.warn(&quot;warn...&quot;); log.error(&quot;error...&quot;); return &quot;springboot is running...2&quot;; &#125;&#125; ​ 上述代码中log对象就是用来记录日志的对象，下面的log.debug，log.info这些操作就是写日志的API了。 步骤②：设置日志输出级别 ​ 日志设置好以后可以根据设置选择哪些参与记录。这里是根据日志的级别来设置的。日志的级别分为6种，分别是： TRACE：运行堆栈信息，使用率低 DEBUG：程序员调试代码使用 INFO：记录运维过程数据 WARN：记录运维过程报警数据 ERROR：记录错误堆栈信息 FATAL：灾难信息，合并计入ERROR ​ 一般情况下，开发时候使用DEBUG，上线后使用INFO，运维信息记录使用WARN即可。下面就设置一下日志级别： 12# 开启debug模式，输出调试信息，常用于检查系统运行状况debug: true ​ 这么设置太简单粗暴了，日志系统通常都提供了细粒度的控制 1234567# 开启debug模式，输出调试信息，常用于检查系统运行状况debug: true# 设置日志级别，root表示根节点，即整体应用日志级别logging: level: root: debug ​ 还可以再设置更细粒度的控制 步骤③：设置日志组，控制指定包对应的日志输出级别，也可以直接控制指定包对应的日志输出级别 123456789101112logging: # 设置日志组 group: # 自定义组名，设置当前组中所包含的包 ebank: com.fla.controller,com.fla.service level: # 1.root相当于最大的group，整个项目都会记录到 root: warn # 2.为对应组设置日志级别（常用） ebank: debug # 3.为对包设置日志级别 com.itheima.controller: debug ​ 注意：记得导包 2）log对象创建1private static final Logger log = LoggerFactory.getLogger(BookController.class); 由于每个需要记录日志的类我们都需要一个这样的log对象，这样重复的地方我们应想办法解决 ①继承BaseClass： 123456789public class Baseclass &#123; private class clazz; public static Logger Log; public Baseclass()&#123; clazz = this.getclass(); log = LoggerFactory.getLogger(clazz); &#125;&#125; 继承后： 12345@RestController@RequestMapping(&quot;/books&quot;)public class BookController extends BaseClass&#123; private static final Logger log = LoggerFactory.getLogger(BookController.class); //这一句可以不写了&#125; 这样BookController在bean实例化的时候就会执行构造函数，获得当前的class，并生成log对象。 ②lombok​ 导入lombok后使用注解搞定，日志对象名为log，基于lombok提供的@Slf4j注解为类快速添加日志对象： 123456@Slf4j //这个注解替代了下面那一行@RestController@RequestMapping(&quot;/books&quot;)public class BookController extends BaseClass&#123; private static final Logger log = LoggerFactory.getLogger(BookController.class); //@Slf4j代替了&#125; 3）日志格式​ 日志已经能够记录了，但是目前记录的格式是SpringBoot给我们提供的，如果想自定义控制就需要自己设置了。先分析一下当前日志的记录格式。 ​ 对于单条日志信息来说，日期，触发位置，记录信息是最核心的信息。级别用于做筛选过滤，PID与线程名用于做精准分析。 ​ 我们可以手动diy该配置 123logging: pattern: console: &quot;%d %clr(%p) --- [%16t] %clr(%-40.40c)&#123;cyan&#125; : %m %n&quot; 4）日志文件日志信息不仅能显示在控制台上，还应该将其打印到文件中进行保存： 记录日志到文件中格式非常简单，设置日志文件名即可。 123logging: file: name: server.log ​ 虽然使用上述格式可以将日志记录下来了，但是面对线上的复杂情况，一个文件记录肯定是不能够满足运维要求的，通常会每天记录日志文件，同时为了便于维护，还要限制每个日志文件的大小。下面是日志文件的常用配置方式： 12345logging: logback: rollingpolicy: max-file-size: 3KB #定义每个文件的大小 file-name-pattern: server.%d&#123;yyyy-MM-dd&#125;.%i.log #定义每个文件的名字 ​ 以上格式是基于logback日志技术设置每日日志文件的设置格式，要求容量到达3KB以后就转存信息到第二个文件中。文件命名规则中的%d标识日期，%i是一个递增变量，用于区分日志文件。 5.热部署1）手动热部署步骤①：导入开发者工具对应的坐标 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;!-- 通过来devtools实现--!&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; 步骤②：构建项目，可以使用快捷键激活此功能 ​ 对应的快捷键快捷键如下： 1&lt;CTR&gt;L+&lt;F9&gt; ​ 以上过程就实现了springboot工程的热部署，其底层的工作如下： 重启与重载 ​ 一个springboot项目在运行时实际上是分两个过程进行的，根据加载的东西不同，划分成base类加载器与restart类加载器。 base类加载器：用来加载jar包中的类，jar包中的类和配置文件由于不会发生变化，因此不管加载多少次，加载的内容不会发生变化 restart类加载器：用来加载开发者自己开发的类、配置文件、页面等信息，这一类文件受开发者影响 ​ 当springboot项目启动时，base类加载器执行，加载jar包中的信息后，restart类加载器执行，加载开发者制作的内容。当执行构建项目后，由于jar中的信息不会变化，因此base类加载器无需再次执行，所以仅仅运行restart类加载即可，也就是将开发者自己制作的内容重新加载就行了，这就完成了一次热部署的过程，也可以说热部署的过程实际上是重新加载restart类加载器中的信息。 2）自动热部署自动热部署其实就是设计一个开关，打开这个开关后，IDE工具就可以自动热部署。因此这个操作和IDE工具有关，以下以idea为例设置idea中启动热部署 步骤①：设置自动构建项目 ​ 打开【File】-&gt;【settings…】-&gt;【Compile】-&gt;【Build project automatically】 步骤②：允许在程序运行时进行自动构建 ​ 使用快捷键【Ctrl】+【Alt】+【Shit】+【&#x2F;】打开维护面板，选择第1项【Registry…】 ​ 在选项中搜索comple，然后勾选对应项即可 ​ 这样程序在运行的时候就可以进行自动构建了，实现了热部署的效果。 注意： ​ 如果每敲一个字母，服务器就重新构建一次，这有点太频繁了，所以idea设置当idea工具失去焦点5秒后进行热部署。其实就是从idea工具中切换到其他工具时进行热部署，比如改完程序需要到浏览器上去调试，这个时候idea就自动进行热部署操作。 3）参与范围配置参与热部署监控的文件范围配置 ​ 通过修改项目中的文件，可以发现其实并不是所有的文件修改都会激活热部署的，原因在于在开发者工具中有一组配置，当满足了配置中的条件后，才会启动热部署，配置中默认不参与热部署的目录信息如下 &#x2F;META-INF&#x2F;maven &#x2F;META-INF&#x2F;resources &#x2F;resources &#x2F;static &#x2F;public &#x2F;templates ​ 以上目录中的文件如果发生变化，是不参与热部署的。如果想修改配置，可以通过application.yml文件进行设定哪些文件不参与热部署操作 12345spring: devtools: restart: # 设置不参与热部署的文件或文件夹 exclude: static/**,public/**,config/application.yml 4）关闭热部署​ 线上环境运行时是不可能使用热部署功能的，所以需要强制关闭此功能，通过配置可以关闭此功能。 1234spring: devtools: restart: enabled: false ​ 如果当心配置文件层级过多导致相符覆盖最终引起配置失效，可以提高配置的层级，在更高层级中配置关闭热部署。例如在启动容器前通过系统属性设置关闭热部署功能。（高优先级配置文件覆盖低优先级文件，4级配置） 1234567@SpringBootApplicationpublic class SSMPApplication &#123; public static void main(String[] args) &#123; System.setProperty(&quot;spring.devtools.restart.enabled&quot;,&quot;false&quot;);//设置系统参数 SpringApplication.run(SSMPApplication.class); &#125;&#125; 6.配置数据注入相关1）注入数据①注入自定义bean​ @ConfigurationProperties，此注解的作用是用来为bean绑定属性的。开发者可以在yml配置文件中以对象的格式添加若干属性 1234servers: ip-address: 192.168.0.1 port: 2345 timeout: -1 ​ 然后再开发一个用来封装数据的实体类，注意要提供属性对应的setter方法（这里是lombok提供） 1234567@Component@Data //lombokpublic class ServerConfig &#123; private String ipAddress; private int port; private long timeout;&#125; ​ 使用@ConfigurationProperties注解就可以将配置中的属性值关联到开发的模型类上 12345678@Component@Data@ConfigurationProperties(prefix = &quot;servers&quot;)public class ServerConfig &#123; private String ipAddress; private int port; private long timeout;&#125; ​ 这样加载对应bean的时候就可以直接加载配置属性值了。 ②注入第三方beana）@ConfigurationProperties 使用@ConfigurationProperties注解其实可以为第三方bean加载属性 步骤①：使用@Bean注解定义第三方bean 12345@Beanpublic DruidDataSource datasource()&#123; DruidDataSource ds = new DruidDataSource(); return ds;&#125; 步骤②：在yml中定义要绑定的属性，注意datasource此时全小写 12datasource: driverClassName: com.mysql.jdbc.Driver 步骤③：使用@ConfigurationProperties注解为第三方bean进行属性绑定，注意前缀是全小写的datasource 123456@Bean@ConfigurationProperties(prefix = &quot;datasource&quot;)public DruidDataSource datasource()&#123; DruidDataSource ds = new DruidDataSource(); return ds;&#125; ​ 操作方式完全一样，只不过@ConfigurationProperties注解不仅能添加到类上，还可以添加到方法上，添加到类上是为spring容器管理的当前类的对象绑定属性，添加到方法上是为spring容器管理的当前方法的返回值对象绑定属性，其实本质上都一样。 b）@EnableConfigurationProperties() ​ 这里出现了一个新的问题，目前我们定义bean不是通过@Component就是通过@Bean定义，使用@ConfigurationProperties注解可以为bean进行属性绑定，那在一个业务系统中，哪些bean通过注解@ConfigurationProperties去绑定属性了呢？ ​ 因为这个注解不仅可以写在类上，还可以写在方法上，所以找起来就比较麻烦了。为了解决这个问题，spring提供了一个全新的注解，专门标注使用@ConfigurationProperties注解绑定属性的bean是哪些，这个注解叫做@EnableConfigurationProperties。 步骤①：在配置类上开启@EnableConfigurationProperties注解，并标注要使用@ConfigurationProperties注解绑定属性的类 1234@SpringBootApplication@EnableConfigurationProperties(ServerConfig.class) //在引导类上配置public class Springboot13ConfigurationApplication &#123;&#125; 步骤②：在对应的类上直接使用@ConfigurationProperties进行属性绑定 1234567@Data@ConfigurationProperties(prefix = &quot;servers&quot;)public class ServerConfig &#123; private String ipAddress; private int port; private long timeout;&#125; 注意： ​ 现在绑定属性的ServerConfig类并没有声明@Component注解。当使用@EnableConfigurationProperties注解时，spring会默认将其标注的类定义为bean，因此无需再次声明@Component注解了。 ​ 一个小问题，使用@ConfigurationProperties注解时，会出现一个提示信息 出现这个提示后只需要添加一个坐标此提醒就消失了 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt;&lt;/dependency&gt; 总结 使用@ConfigurationProperties可以为使用@Bean声明的第三方bean绑定属性 当使用@EnableConfigurationProperties声明进行属性绑定的bean后，无需使用@Component注解再次进行bean声明 2）宽松绑定&#x2F;松散绑定在进行属性绑定时，可能会遇到如下情况，为了进行标准命名，开发者会将属性名严格按照驼峰命名法书写，在yml配置文件中将datasource修改为dataSource，如下： 12dataSource: driverClassName: com.mysql.jdbc.Driver ​ 此时程序可以正常运行，然后又将代码中的前缀datasource修改为dataSource，如下： 123456@Bean@ConfigurationProperties(prefix = &quot;dataSource&quot;)//这里完全按照yml中的写法，但还是报错了public DruidDataSource datasource()&#123; DruidDataSource ds = new DruidDataSource(); return ds;&#125; ​ 此时就发生了编译错误，而且并不是idea工具导致的，运行后依然会出现问题，配置属性名dataSource是无效的 12345678Configuration property name &#x27;dataSource&#x27; is not valid: Invalid characters: &#x27;S&#x27; Bean: datasource Reason: Canonical names should be kebab-case (&#x27;-&#x27; separated), lowercase alpha-numeric characters and must start with a letterAction:Modify &#x27;dataSource&#x27; so that it conforms to the canonical names requirements. ​ 这里涉及到有关属性名称的宽松绑定，也可以称为宽松绑定。 ​ 实际上是springboot进行编程时人性化设计的一种体现，即配置文件中的命名格式与变量名的命名格式可以进行格式上的最大化兼容，几乎主流的命名格式都支持，例如： ​ 在ServerConfig中的ipAddress属性名 123456@Component@Data@ConfigurationProperties(prefix = &quot;servers&quot;)public class ServerConfig &#123; private String ipAddress;&#125; ​ 可以与下面的配置属性名规则全兼容 12345servers: ipAddress: 192.168.0.2 # 驼峰模式 ip_address: 192.168.0.2 # 下划线模式 ip-address: 192.168.0.2 # 烤肉串模式 IP_ADDRESS: 192.168.0.2 # 常量模式 ​ 也可以说，以上4种模式最终都可以匹配到ipAddress这个属性名。原因就是在进行匹配时，配置中的名称要去掉中划线和下划线后，忽略大小写的情况下去与java代码中的属性名进行忽略大小写的等值匹配，以上4种命名去掉下划线中划线忽略大小写后都是一个词ipaddress，java代码中的属性名忽略大小写后也是ipaddress，这样就可以进行等值匹配了，这就是为什么这4种格式都能匹配成功的原因。不过springboot官方推荐使用烤肉串模式，也就是中划线模式。 ​ 再来看开始出现的编程错误信息中的Reason 12345678Configuration property name &#x27;dataSource&#x27; is not valid: Invalid characters: &#x27;S&#x27; Bean: datasource Reason: Canonical names should be kebab-case (&#x27;-&#x27; separated), lowercase alpha-numeric characters and must start with a letter //原因希望prefix使用烤肉串模式Action:Modify &#x27;dataSource&#x27; so that it conforms to the canonical names requirements. ​ 其中Reason描述了报错的原因，规范的名称应该是烤肉串(kebab)模式(case)，即使用-分隔，使用小写字母数字作为标准字符，且必须以字母开头。然后再看我们写的名称dataSource，就不满足上述要求。在书写前缀时，这个词不是随意支持的，必须使用上述标准。 注意： 以上规则仅针对springboot中**@ConfigurationProperties**注解进行属性绑定时有效，对@Value注解进行属性映射无效。 所以@Value(ipAddress)是不能获取yml配置中ipaddress的，必须要完全一致 ​ 3）计量数据​ 如下写明了timeout的时间是240，但是这个240具体的单位无法确定（默认时间是ms，大小是B）： 1234servers: ip-address: 192.168.0.1 port: 2345 timeout: 240 ​ 除了加强约定之外，springboot充分利用了JDK8中提供的全新的用来表示计量单位的新数据类型，从根本上解决这个问题。以下模型类中添加了两个JDK8中新增的类，分别是Duration和DataSize 12345678910@Component@Data@ConfigurationProperties(prefix = &quot;servers&quot;)public class ServerConfig &#123; @DurationUnit(ChronoUnit.HOURS) //定义的单位是hour private Duration serverTimeOut; @DataSizeUnit(DataUnit.MEGABYTES) //定义的单位是MB private DataSize dataSize;&#125; Duration：表示时间间隔，可以通过@DurationUnit注解描述时间单位，例如上例中描述的单位为小时（ChronoUnit.HOURS） DataSize：表示存储空间，可以通过@DataSizeUnit注解描述存储空间单位，例如上例中描述的单位为MB（DataUnit.MEGABYTES） ​ 使用上述两个单位就可以有效避免因沟通不同步或文档不健全导致的信息不对称问题，从根本上解决了问题，避免产生误读。 Druation常用单位如下： DataSize常用单位如下： 4）数据校验​ 目前我们在进行属性绑定时可以通过松散绑定规则在书写时放飞自我了，但是在书写时由于无法感知模型类中的数据类型，就会出现类型不匹配的问题，比如代码中需要int类型，配置中给了非法的数值，例如写一个“a”，这种数据肯定无法有效的绑定，还会引发错误。 SpringBoot给出了强大的数据校验功能，可以有效的避免此类问题的发生。在JAVAEE的JSR303规范中给出了具体的数据校验标准，开发者可以根据自己的需要选择对应的校验框架，此处使用Hibernate提供的校验框架来作为实现进行数据校验。书写应用格式非常固定，话不多说，直接上步骤 步骤①：开启校验框架 JSR303是一个接口，相当于定义了一套规范（类似JDBC） hibernate校验器实现了JSR303这个接口，是它的实现类（类似mysql驱动实现JDBC） 12345678910&lt;!--1.导入JSR303规范--&gt;&lt;dependency&gt; &lt;groupId&gt;javax.validation&lt;/groupId&gt; &lt;artifactId&gt;validation-api&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!--使用hibernate框架提供的校验器做实现--&gt;&lt;dependency&gt; &lt;groupId&gt;org.hibernate.validator&lt;/groupId&gt; &lt;artifactId&gt;hibernate-validator&lt;/artifactId&gt;&lt;/dependency&gt; 步骤②：在需要开启校验功能的类上使用注解@Validated开启校验功能 1234567@Component@Data@ConfigurationProperties(prefix = &quot;servers&quot;)//开启对当前bean的属性注入校验@Validatedpublic class ServerConfig &#123;&#125; 步骤③：对具体的字段设置校验规则 1234567891011@Component@Data@ConfigurationProperties(prefix = &quot;servers&quot;)//开启对当前bean的属性注入校验@Validatedpublic class ServerConfig &#123; //设置具体的规则 @Max(value = 8888,message = &quot;最大值不能超过8888&quot;) @Min(value = 202,message = &quot;最小值不能低于202&quot;) private int port;&#125; ​ 通过设置数据格式校验，就可以有效避免非法数据加载，其实使用起来还是挺轻松的，基本上就是一个格式。 5）数据转换问题​ 先把问题描述一下，连接数据库正常操作，但是运行程序后显示的信息是密码错误。 1java.sql.SQLException: Access denied for user &#x27;root&#x27;@&#x27;localhost&#x27; (using password: YES) ​ 其配置是这样写的： 123456spring: datasource: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:3306/ssm_db?serverTimezone=UTC username: root password: 0127 //密码是对的，但是没用&quot;&quot;包裹 ​ 在整数相关知识中有这么一句话，支持二进制，八进制，十六进制 ​ 0（0-7）：8进制 ​ 0x（0-9，a-f）16进制 0127在开发者眼中是一个字符串“0127”，但是在springboot看来，这就是一个数字，而且是一个八进制的数字。当后台使用String类型接收数据时，如果配置文件中配置了一个整数值，他是先安装整数进行处理，读取后再转换成字符串。0127撞上了八进制的格式，所以最终以十进制数字87的结果存在了。 注意： 字符串标准书写加上引号包裹，养成习惯 遇到0开头的数据多注意 7.测试1）临时属性​ 测试过程本身并不是一个复杂的过程，但是很多情况下测试时需要模拟一些线上情况，或者模拟一些特殊情况。如果当前环境按照线上环境已经设定好了，例如是下面的配置 123env: maxMemory: 32GB minMemory: 16GB ​ 但是你现在想测试对应的兼容性，需要测试如下配置 123env: maxMemory: 16GB minMemory: 8GB ​ 这个时候我们不能每次测试的时候都去修改源码application.yml中的配置进行测试，每次测试前改过来，每次测试后改回去，这太麻烦了。这时候我们需要在测试环境中创建一组临时属性，去覆盖我们源码中设定的属性，这样测试用例就相当于是一个独立的环境，能够独立测试。 ①properties临时属性 ​ springboot已经为我们开发者早就想好了这种问题该如何解决，并且提供了对应的功能入口。在测试用例程序中，可以通过对注解@SpringBootTest添加属性来模拟临时属性，具体如下： 123456789101112//properties属性可以为当前测试用例添加临时的属性配置@SpringBootTest(properties = &#123;&quot;test.prop=testValue1&quot;&#125;)public class PropertiesAndArgsTest &#123; @Value(&quot;$&#123;test.prop&#125;&quot;) private String msg; @Test void testProperties()&#123; System.out.println(msg); &#125;&#125; ​ 使用注解@SpringBootTest的properties属性就可以为当前测试用例添加临时的属性，覆盖源码配置文件中对应的属性值进行测试。 ②args临时参数 ​ 除了上述这种情况，在前面讲解使用命令行启动springboot程序时讲过，通过命令行参数也可以设置属性值。而且线上启动程序时，通常都会添加一些专用的配置信息。作为运维人员他们才不懂java，更不懂这些配置的信息具体格式该怎么写，作为开发者提供了对应的书写内容后，需要提前测试一下这些配置信息是否有效呢，通过注解@SpringBootTest的另一个属性args来进行设定。 123456789101112//args属性可以为当前测试用例添加临时的命令行参数@SpringBootTest(args=&#123;&quot;--test.prop=testValue2&quot;&#125;)public class PropertiesAndArgsTest &#123; @Value(&quot;$&#123;test.prop&#125;&quot;) private String msg; @Test void testProperties()&#123; System.out.println(msg); &#125;&#125; ​ 使用注解@SpringBootTest的args属性就可以为当前测试用例模拟命令行参数并进行测试。（idea中也可以添加） 加载顺序 原来的配置文件-&gt;properties属性-&gt;args属性（后面覆盖前面） 2）加载测试专用配置​ 学习过Spring的知识，我们都知道，其实一个spring环境中可以设置若干个配置文件或配置类，若干个配置信息可以同时生效。现在我们的需求就是在测试环境中再添加一个配置类，然后启动测试环境时，生效此配置就行了。其实做法和spring环境中加载多个配置信息的方式完全一样。具体操作步骤如下： 步骤①：在测试包test中创建config下专用的测试环境配置类 1234567@Configurationpublic class MsgConfig &#123; @Bean public String msg()&#123; return &quot;bean msg&quot;; &#125;&#125; ​ 上述配置仅用于演示当前实验效果，实际开发可不能这么注入String类型的数据 步骤②：在启动测试环境时，导入测试环境专用的配置类，使用@Import注解即可实现 123456789101112@SpringBootTest@Import(&#123;MsgConfig.class&#125;) //通过import注解导入专门配置public class ConfigurationTest &#123; @Autowired private String msg; @Test void testConfiguration()&#123; System.out.println(msg); &#125;&#125; ​ 到这里就通过@Import属性实现了基于开发环境的配置基础上，对配置进行测试环境的追加操作，实现了1+1的配置环境效果。这样我们就可以实现每一个不同的测试用例加载不同的bean的效果，丰富测试用例的编写，同时不影响开发环境的配置。 3）Web环境模拟测试 如何在测试类中启动web测试 如何在测试类中发送web请求 ①测试类中启动web环境​ 每一个springboot的测试类上方都会标准@SpringBootTest注解，而注解带有一个属性，叫做webEnvironment。通过该属性就可以设置在测试用例中启动web环境，具体如下： 123@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT)public class WebTest &#123; &#125; ​ 测试类中启动web环境时，可以指定启动的Web环境对应的端口，springboot提供了4种设置值，分别如下： MOCK：根据当前设置确认是否启动web环境，例如使用了Servlet的API就启动web环境，属于适配性的配置 DEFINED_PORT：使用自定义的端口作为web服务器端口 RANDOM_PORT：使用随机端口作为web服务器端口 NONE：不启动web环境（默认值） ​ 通过上述配置，现在启动测试程序时就可以正常启用web环境了，建议测试时使用RANDOM_PORT，避免代码中因为写死设定引发线上功能打包测试时由于端口冲突导致意外现象的出现。就是说你程序中写了用8080端口，结果线上环境8080端口被占用了，结果你代码中所有写的东西都要改，这就是写死代码的代价。现在用随机端口就可以测试出来有没有这种问题的隐患了。 ​ ②测试类中发送请求对于测试类中发送请求，使用@AutoConfigureMockMvc定义的操作来发送请求： 步骤①：在测试类中开启web虚拟调用功能，通过注解@AutoConfigureMockMvc实现此功能的开启 12345@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT)//开启虚拟MVC调用@AutoConfigureMockMvcpublic class WebTest &#123;&#125; 步骤②：定义发起虚拟调用的对象MockMVC，通过自动装配的形式初始化对象 123456789101112@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT)//开启虚拟MVC调用@AutoConfigureMockMvcpublic class WebTest &#123; //1.第一种方式注入 @Autowired private MockMvc mvc; @Test void testWeb(@Autowired MockMvc mvc) &#123;//第二种方式注入 &#125;&#125; 步骤③：使用工具类MockMvcRequestBuilders创建一个虚拟请求对象MockHttpServletRequestBuilder，封装请求的路径，并使用MockMVC对象发送对应请求 12345678910111213141516@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT)//开启虚拟MVC调用@AutoConfigureMockMvcpublic class WebTest &#123; @Test void testWeb(@Autowired MockMvc mvc) throws Exception &#123; //http://localhost:8080/books //创建虚拟请求，当前访问/books //1.通过工具类获得builder MockHttpServletRequestBuilder builder = MockMvcRequestBuilders.get(&quot;/books&quot;); //2.mvc发送请求需要传入一个builder mvc.perform(builder); &#125;&#125; ​ 执行测试程序，现在就可以正常的发送&#x2F;books对应的请求了，注意访问路径不要写http://localhost:8080/books，因为前面的服务器IP地址和端口使用的是当前虚拟的web环境，无需指定，仅指定请求的具体路径即可。 ③web环境响应结果比对​ 发送完web请求后我们应该验证比对响应结果是否使我们想要的结果： 响应状态匹配 123456789101112@Testvoid testStatus(@Autowired MockMvc mvc) throws Exception &#123; MockHttpServletRequestBuilder builder = MockMvcRequestBuilders.get(&quot;/books&quot;); ResultActions action = mvc.perform(builder); //设定预期值 与真实值进行比较，成功测试通过，失败测试失败 //定义本次调用的预期值 StatusResultMatchers status = MockMvcResultMatchers.status(); //预计本次调用时成功的：状态200 ResultMatcher ok = status.isOk(); //添加预计值到本次调用过程中进行匹配 action.andExpect(ok);&#125; 响应体匹配（非json数据格式） 1234567891011@Testvoid testBody(@Autowired MockMvc mvc) throws Exception &#123; MockHttpServletRequestBuilder builder = MockMvcRequestBuilders.get(&quot;/books&quot;); ResultActions action = mvc.perform(builder); //设定预期值 与真实值进行比较，成功测试通过，失败测试失败 //定义本次调用的预期值 ContentResultMatchers content = MockMvcResultMatchers.content(); ResultMatcher result = content.string(&quot;springboot2&quot;); //添加预计值到本次调用过程中进行匹配 action.andExpect(result);&#125; 响应体匹配（json数据格式，开发中的主流使用方式） 1234567891011@Testvoid testJson(@Autowired MockMvc mvc) throws Exception &#123; MockHttpServletRequestBuilder builder = MockMvcRequestBuilders.get(&quot;/books&quot;); ResultActions action = mvc.perform(builder); //设定预期值 与真实值进行比较，成功测试通过，失败测试失败 //定义本次调用的预期值 ContentResultMatchers content = MockMvcResultMatchers.content(); ResultMatcher result = content.json(&quot;&#123;\\&quot;id\\&quot;:1,\\&quot;name\\&quot;:\\&quot;springboot2\\&quot;,\\&quot;type\\&quot;:\\&quot;springboot\\&quot;&#125;&quot;); //添加预计值到本次调用过程中进行匹配 action.andExpect(result);&#125; 响应头信息匹配 1234567891011@Testvoid testContentType(@Autowired MockMvc mvc) throws Exception &#123; MockHttpServletRequestBuilder builder = MockMvcRequestBuilders.get(&quot;/books&quot;); ResultActions action = mvc.perform(builder); //设定预期值 与真实值进行比较，成功测试通过，失败测试失败 //定义本次调用的预期值 HeaderResultMatchers header = MockMvcResultMatchers.header(); ResultMatcher contentType = header.string(&quot;Content-Type&quot;, &quot;application/json&quot;); //添加预计值到本次调用过程中进行匹配 action.andExpect(contentType);&#125; 多种信息匹配eg 1234567891011121314151617@Testvoid testGetById(@Autowired MockMvc mvc) throws Exception &#123; MockHttpServletRequestBuilder builder = MockMvcRequestBuilders.get(&quot;/books&quot;); ResultActions action = mvc.perform(builder); StatusResultMatchers status = MockMvcResultMatchers.status(); ResultMatcher ok = status.isOk(); action.andExpect(ok); HeaderResultMatchers header = MockMvcResultMatchers.header(); ResultMatcher contentType = header.string(&quot;Content-Type&quot;, &quot;application/json&quot;); action.andExpect(contentType); ContentResultMatchers content = MockMvcResultMatchers.content(); ResultMatcher result = content.json(&quot;&#123;\\&quot;id\\&quot;:1,\\&quot;name\\&quot;:\\&quot;springboot\\&quot;,\\&quot;type\\&quot;:\\&quot;springboot\\&quot;&#125;&quot;); action.andExpect(result);&#125; 4）数据层测试回滚​ 测试程序可以完美的进行表现层、业务层、数据层接口对应的功能测试，但是测试用例开发完成后，在打包的阶段由于test生命周期属于必须被运行的生命周期，如果跳过会给系统带来极高的安全隐患，所以测试用例必须执行。但是新的问题就呈现了，测试用例如果测试时产生了事务提交就会在测试过程中对数据库数据产生影响，进而产生垃圾数据。这个过程不是我们希望发生的，作为开发者测试用例该运行运行，但是过程中产生的数据不要在我的系统中留痕。 ​ springboot给出了最简解决方案：在原始测试用例中添加注解@Transactional即可实现当前测试用例的事务不提交。当程序运行后，只要注解@Transactional出现的位置存在注解@SpringBootTest，springboot就会认为这是一个测试程序，无需提交事务，所以也就可以避免事务的提交。 1234567891011121314151617@SpringBootTest@Transactional@Rollback(true) //对于test来说，springBoot默认回滚数据public class DaoTest &#123; @Autowired private BookService bookService; @Test void testSave()&#123; Book book = new Book(); book.setName(&quot;springboot3&quot;); book.setType(&quot;springboot3&quot;); book.setDescription(&quot;springboot3&quot;); bookService.save(book); &#125;&#125; ​ 如果开发者想提交事务，也可以，再添加一个@RollBack的注解，设置回滚状态为false即可正常提交事务。 ​ @Transactional是spring的声明式事务，我们也可以在service层使用 ​ 5）测试用例数据设定​ 对于测试用例的数据写固定值肯定是不合理的，springboot提供了在配置中使用随机值的机制，确保每次运行程序加载的数据都是随机的。具体如下： 12345678testcase: book: id: $&#123;random.int&#125; id2: $&#123;random.int(10)&#125; type: $&#123;random.int!5,10!&#125; name: $&#123;random.value&#125; uuid: $&#123;random.uuid&#125; publishTime: $&#123;random.long&#125; ​ 当前配置就可以在每次运行程序时创建一组随机数据，避免每次运行时数据都是固定值的尴尬现象发生，有助于测试功能的进行。数据的加载按照之前加载数据的形式，使用@ConfigurationProperties注解即可 123456789101112//test.domain下的bean数据@Component@Data@ConfigurationProperties(prefix = &quot;testcase.book&quot;)public class BookCase &#123; private int id; private int id2; private int type; private String name; private String uuid; private long publishTime;&#125; ​ 对于随机值的产生，还有一些小的限定规则，比如产生的数值性数据可以设置范围等，具体如下： ${random.int}表示随机整数 ${random.int(10)}表示10以内的随机数 ${random.int(10,20)}表示10到20的随机数 其中()可以是任意字符，例如[]，!!均可 8.数据层方案1）sql①dataSourcespringboot提供了3款内嵌数据源技术，分别如下： HikariCP Tomcat提供DataSource Commons DBCP druid配置： 1234567spring: datasource: druid: url: jdbc:mysql://localhost:3306/ssm_db?serverTimezone=UTC driver-class-name: com.mysql.cj.jdbc.Driver username: root password: root druid删掉就是默认的hikari： 123456spring: datasource: url: jdbc:mysql://localhost:3306/ssm_db?serverTimezone=UTC driver-class-name: com.mysql.cj.jdbc.Driver username: root password: root 也可以写上是对hikari做的配置，但是url地址要单独配置，如下： 1234567spring: datasource: url: jdbc:mysql://localhost:3306/ssm_db?serverTimezone=UTC #hikari的url是写在外面的 hikari: driver-class-name: com.mysql.cj.jdbc.Driver username: root password: root 可以对hikari做进一步的配置，可以继续配置其独立的属性。例如： 12345678spring: datasource: url: jdbc:mysql://localhost:3306/ssm_db?serverTimezone=UTC hikari: driver-class-name: com.mysql.cj.jdbc.Driver username: root password: root maximum-pool-size: 50 ​ ②template步骤①：导入jdbc对应的坐标，记得是starter 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt;&lt;/dependency 步骤②：自动装配JdbcTemplate对象 123456@SpringBootTestclass Springboot15SqlApplicationTests &#123; @Test void testJdbcTemplate(@Autowired JdbcTemplate jdbcTemplate)&#123; &#125;&#125; 步骤③：使用JdbcTemplate实现查询操作（非实体类封装数据的查询操作） 123456@Testvoid testJdbcTemplate(@Autowired JdbcTemplate jdbcTemplate)&#123; String sql = &quot;select * from tbl_book&quot;; List&lt;Map&lt;String, Object&gt;&gt; maps = jdbcTemplate.queryForList(sql); System.out.println(maps);&#125; 步骤④：使用JdbcTemplate实现查询操作（实体类封装数据的查询操作） 123456789101112131415161718@Testvoid testJdbcTemplate(@Autowired JdbcTemplate jdbcTemplate)&#123; String sql = &quot;select * from tbl_book&quot;; RowMapper&lt;Book&gt; rm = new RowMapper&lt;Book&gt;() &#123; @Override public Book mapRow(ResultSet rs, int rowNum) throws SQLException &#123; Book temp = new Book(); temp.setId(rs.getInt(&quot;id&quot;)); temp.setName(rs.getString(&quot;name&quot;)); temp.setType(rs.getString(&quot;type&quot;)); temp.setDescription(rs.getString(&quot;description&quot;)); return temp; &#125; &#125;; List&lt;Book&gt; list = jdbcTemplate.query(sql, rm); System.out.println(list);&#125; 步骤⑤：使用JdbcTemplate实现增删改操作 12345@Testvoid testJdbcTemplateSave(@Autowired JdbcTemplate jdbcTemplate)&#123; String sql = &quot;insert into tbl_book values(3,&#x27;springboot1&#x27;,&#x27;springboot2&#x27;,&#x27;springboot3&#x27;)&quot;; jdbcTemplate.update(sql);&#125; 如果想对JdbcTemplate对象进行相关配置，可以在yml文件中进行设定，具体如下： 123456spring: jdbc: template: query-timeout: -1 # 查询超时时间 max-rows: 500 # 最大行数 fetch-size: -1 # 缓存行数 2）nosql①redisTemplate步骤①：导入springboot整合redis的starter坐标 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; ​ 上述坐标可以在创建模块的时候通过勾选的形式进行选择，归属NoSQL分类中 步骤②：进行基础配置 1234spring: redis: host: localhost port: 6379 ​ 步骤③：使用springboot整合redis的专用客户端接口操作，此处使用的是RedisTemplate 12345678910111213141516171819202122232425262728@SpringBootTestclass Springboot16RedisApplicationTests &#123; @Autowired private RedisTemplate redisTemplate; @Test void set() &#123; ValueOperations ops = redisTemplate.opsForValue(); ops.set(&quot;age&quot;,41); &#125; @Test void get() &#123; ValueOperations ops = redisTemplate.opsForValue(); Object age = ops.get(&quot;name&quot;); System.out.println(age); &#125; @Test void hset() &#123; HashOperations ops = redisTemplate.opsForHash(); ops.put(&quot;info&quot;,&quot;b&quot;,&quot;bb&quot;); &#125; @Test void hget() &#123; HashOperations ops = redisTemplate.opsForHash(); Object val = ops.get(&quot;info&quot;, &quot;b&quot;); System.out.println(val); &#125;&#125; ​ 在操作redis时，需要先确认操作何种数据，根据数据种类得到操作接口。例如使用opsForValue()获取string类型的数据操作接口，使用opsForHash()获取hash类型的数据操作接口，剩下的就是调用对应api操作了。各种类型的数据操作接口如下： ②StringRedisTemplate​ 有这样一个问题，当我们在java程序中进行如下set，然后再在cmd中get(age)，发现存储的并不是41 12345@Test void set() &#123; ValueOperations ops = redisTemplate.opsForValue(); ops.set(&quot;age&quot;,41); &#125; ​ 这是由于redis内部不提供java对象的存储格式，因此当操作的数据以对象的形式存在时，会转换成序列化的字符串格式后进行操作。 所以这里的41会经过转码成一串string，这就和原来的string不一致了 ​ springboot整合redis时提供了专用的API接口StringRedisTemplate，我们查看RedisTemplate，可以看到他可以接收一个泛型&lt;k,v&gt;的，当我们没有设置泛型，其默认就是&lt;Object,Object&gt;，而我们要正确操作redis就需要&lt;String,String&gt;，而StringRedisTemplate就实现了这个泛型 1234567891011@SpringBootTestpublic class StringRedisTemplateTest &#123; @Autowired private StringRedisTemplate stringRedisTemplate; @Test void get()&#123; ValueOperations&lt;String, String&gt; ops = stringRedisTemplate.opsForValue(); String name = ops.get(&quot;name&quot;); System.out.println(name); &#125;&#125; ③redis客户端选择​ springboot整合redis技术提供了多种客户端兼容模式，默认提供的是lettucs客户端技术，也可以根据需要切换成指定客户端技术，例如jedis客户端技术，切换成jedis客户端技术操作步骤如下： 步骤①：导入jedis坐标 1234&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt;&lt;/dependency&gt; jedis坐标受springboot管理，无需提供版本号 步骤②：配置客户端技术类型，设置为jedis 12345spring: redis: host: localhost port: 6379 client-type: jedis //将操作redis的客户端换成jedis 步骤③：根据需要设置对应的配置 1234567891011spring: redis: host: localhost port: 6379 client-type: lettuce lettuce: pool: max-active: 16 jedis: pool: max-active: 16 lettcus与jedis区别 jedis连接Redis服务器是直连模式，当多线程模式下使用jedis会存在线程安全问题，解决方案可以通过配置连接池使每个连接专用，这样整体性能就大受影响（即每个线程单独使用一个连接） lettcus基于Netty框架进行与Redis服务器连接，底层设计中采用StatefulRedisConnection。 StatefulRedisConnection自身是线程安全的，可以保障并发访问安全问题，所以一个连接可以被多线程复用。当然lettcus也支持多连接实例一起工作 https://www.zhihu.com/question/53124685 9.缓存整合​ spring boot定义了一个缓存的规范，并对一些缓存做了内部整合，而有一些并没有被boot整合，需要我们自己来整合。以下所有的整合方式都用模拟手机验证码的存储和校验来作为例子。 输入手机号获取验证码，组织文档以短信形式发送给用户（页面模拟） 输入手机号和验证码验证结果 ①定义验证码对应的实体类，封装手机号与验证码两个属性，验证码pojo 12345@Datapublic class SMSCode &#123; private String tele; private String code;&#125; ②定义验证码功能的业务层接口与实现类 service 和 serviceImple，此处缓存不需要dao来和数据库交互 1234567891011121314151617181920212223public interface SMSCodeService &#123; public String sendCodeToSMS(String tele); public boolean checkCode(SMSCode smsCode);&#125;@Servicepublic class SMSCodeServiceImpl implements SMSCodeService &#123; @Autowired private CodeUtils codeUtils; @CachePut(value = &quot;smsCode&quot;, key = &quot;#tele&quot;) public String sendCodeToSMS(String tele) &#123; String code = codeUtils.generator(tele); return code; &#125; public boolean checkCode(SMSCode smsCode) &#123; //取出内存中的验证码与传递过来的验证码比对，如果相同，返回true String code = smsCode.getCode(); String cacheCode = codeUtils.get(smsCode.getTele()); return code.equals(cacheCode); &#125;&#125; 1）simple缓存simple是boot内置的默认缓存 步骤①：导入springboot提供的缓存技术对应的starter 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-cache&lt;/artifactId&gt;&lt;/dependency&gt; 步骤②：启用缓存，在引导类上方标注注解@EnableCaching配置springboot程序中可以使用缓存 12345678@SpringBootApplication//开启缓存功能@EnableCachingpublic class Springboot19CacheApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(Springboot19CacheApplication.class, args); &#125;&#125; 步骤③：设置操作的数据是否使用缓存 12345678910@Servicepublic class BookServiceImpl implements BookService &#123; @Autowired private BookDao bookDao; @Cacheable(value=&quot;cacheSpace&quot;,key=&quot;#id&quot;) public Book getById(Integer id) &#123; return bookDao.selectById(id); &#125;&#125; ​ 在业务方法上面使用注解@Cacheable声明当前方法的返回值放入缓存中，其中要指定缓存的存储位置，以及缓存中保存当前方法返回值对应的名称。上例中value属性描述缓存的存储位置，可以理解为是一个存储空间名，key属性描述了缓存中保存数据的名称，使用#id读取形参中的id值作为缓存名称。 ​ 使用@Cacheable注解后，执行当前操作，如果发现对应名称在缓存中没有数据，就正常读取数据，然后放入缓存；如果对应名称在缓存中有数据，就终止当前业务方法执行，直接返回缓存中的数据。 步骤④：定义验证码功能的业务层接口与实现类 1234567891011121314151617181920212223public interface SMSCodeService &#123; public String sendCodeToSMS(String tele); public boolean checkCode(SMSCode smsCode);&#125;@Servicepublic class SMSCodeServiceImpl implements SMSCodeService &#123; @Autowired private CodeUtils codeUtils; @CachePut(value = &quot;smsCode&quot;, key = &quot;#tele&quot;)//该注解仅仅是在缓存中放入值 public String sendCodeToSMS(String tele) &#123; String code = codeUtils.generator(tele); return code; &#125; public boolean checkCode(SMSCode smsCode) &#123; //取出内存中的验证码与传递过来的验证码比对，如果相同，返回true String code = smsCode.getCode(); String cacheCode = codeUtils.get(smsCode.getTele()); return code.equals(cacheCode); &#125;&#125; ​ 获取验证码后，当验证码失效时必须重新获取验证码，因此在获取验证码的功能上不能使用@Cacheable注解，@Cacheable注解是缓存中没有值则放入值，缓存中有值则取值。此处的功能仅仅是生成验证码并放入缓存，并不具有从缓存中取值的功能，因此不能使用@Cacheable注解，应该使用仅具有向缓存中保存数据的功能，使用@CachePut注解即可。 @Cacheable(value,key)：value指的是缓存空间的名字，key则是某条缓存的key（key的值和参数对应）。执行该方法的时候首先会根据key查找缓存，没查到则把该方法的return作为key对应的value存入 @CachePut(value,key)：并不会去查找有没有对应的key缓存，而是每次都put（相同则覆盖） 更多：https://www.cnblogs.com/fashflying/p/6908028.html ​ 注意：以下验证方法的写法是错误的 1234567891011121314151617181920212223@Servicepublic class SMSCodeServiceImpl implements SMSCodeService &#123; @Autowired private CodeUtils codeUtils; @CachePut(value = &quot;smsCode&quot;, key = &quot;#tele&quot;)//该注解仅仅是在缓存中放入值，key的值对应方法参数的值 public String sendCodeToSMS(String tele) &#123; String code = codeUtils.generator(tele); return code; &#125; public boolean checkCode(SMSCode smsCode) &#123; //取出内存中的验证码与传递过来的验证码比对，如果相同，返回true String code = smsCode.getCode(); String cacheCode = get(smsCode.getTele());//在这里调用下面的get方法 return code.equals(cacheCode); &#125; @Cacheable(value = &quot;smsCode&quot;, key = &quot;#tele&quot;) public String get(String tele)&#123; return null; &#125;&#125; ​ 这样写是错误的，把get方法写在controller中后，当checkCode在调用get的时候，get并不会去执行@Cacheable相应的操作，因为springboot在这时并没有管理该方法。正确的方式应该是将get方法写在CodeUtils中，将CodeUtils作为一个bean来交给spring管理，那么调用其方法的时候，方法上的注解也会被加载，如下面的步骤⑤ 步骤⑤：定义验证码的生成策略与根据手机号读取验证码的功能 1234567891011121314@Componentpublic class CodeUtils &#123; private String [] patch = &#123;&quot;000000&quot;,&quot;00000&quot;,&quot;0000&quot;,&quot;000&quot;,&quot;00&quot;,&quot;0&quot;,&quot;&quot;&#125;; //生成验证码 public String generator(String tele)&#123; ...... &#125; @Cacheable(value = &quot;smsCode&quot;,key=&quot;#tele&quot;) public String get(String tele)&#123; return null; &#125;&#125; get的时候去寻找key&#x3D;tele的缓存，如果找到则返回该缓存的value。 return null的问题 https://blog.csdn.net/difffate/article/details/64124272 根据设定是否可以缓存null 步骤⑥：定义验证码功能的web层接口，一个方法用于提供手机号获取验证码，一个方法用于提供手机号和验证码进行校验 1234567891011121314151617@RestController@RequestMapping(&quot;/sms&quot;)public class SMSCodeController &#123; @Autowired private SMSCodeService smsCodeService; @GetMapping public String getCode(String tele)&#123; String code = smsCodeService.sendCodeToSMS(tele); return code; &#125; @PostMapping public boolean checkCode(SMSCode smsCode)&#123; return smsCodeService.checkCode(smsCode); &#125;&#125; 2）Ehcache缓存步骤①：导入Ehcache的坐标 1234&lt;dependency&gt; &lt;groupId&gt;net.sf.ehcache&lt;/groupId&gt; &lt;artifactId&gt;ehcache&lt;/artifactId&gt;&lt;/dependency&gt; ​ 此处为什么不是导入Ehcache的starter，而是导入技术坐标呢？其实springboot整合缓存技术做的是通用格式，不管你整合哪种缓存技术，只是实现变化了，操作方式一样。这也体现出springboot技术的优点，统一同类技术的整合方式。 步骤②：配置缓存技术实现使用Ehcache 12345spring: cache: type: ehcache ehcache: config: ehcache.xml ​ 配置缓存的类型type为ehcache，此处需要说明一下，当前springboot可以整合的缓存技术中包含有ehcach（向memcache就没有，需要自己整合），所以可以这样书写。其实这个type不可以随便写的，不是随便写一个名称就可以整合的。 ​ 由于ehcache的配置有独立的配置文件格式，因此还需要指定ehcache的配置文件，以便于读取相应配置 12345678910111213141516171819202122232425262728293031323334&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;ehcache xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:noNamespaceSchemaLocation=&quot;http://ehcache.org/ehcache.xsd&quot; updateCheck=&quot;false&quot;&gt; &lt;diskStore path=&quot;D:\\ehcache&quot; /&gt; &lt;!--默认缓存策略 --&gt; &lt;!-- external：是否永久存在，设置为true则不会被清除，此时与timeout冲突，通常设置为false--&gt; &lt;!-- diskPersistent：是否启用磁盘持久化--&gt; &lt;!-- maxElementsInMemory：最大缓存数量--&gt; &lt;!-- overflowToDisk：超过最大缓存数量是否持久化到磁盘--&gt; &lt;!-- timeToIdleSeconds：最大不活动间隔，设置过长缓存容易溢出，设置过短无效果，可用于记录时效性数据，例如验证码--&gt; &lt;!-- timeToLiveSeconds：最大存活时间--&gt; &lt;!-- memoryStoreEvictionPolicy：缓存清除策略--&gt; &lt;defaultCache eternal=&quot;false&quot; diskPersistent=&quot;false&quot; maxElementsInMemory=&quot;1000&quot; overflowToDisk=&quot;false&quot; timeToIdleSeconds=&quot;60&quot; timeToLiveSeconds=&quot;60&quot; memoryStoreEvictionPolicy=&quot;LRU&quot; /&gt; &lt;!--注意这里要定义保存的空间 --&gt; &lt;cache name=&quot;smsCode&quot; eternal=&quot;false&quot; diskPersistent=&quot;false&quot; maxElementsInMemory=&quot;1000&quot; overflowToDisk=&quot;false&quot; timeToIdleSeconds=&quot;10&quot; timeToLiveSeconds=&quot;10&quot; memoryStoreEvictionPolicy=&quot;LRU&quot; /&gt;&lt;/ehcache&gt; ​ 注意前面的案例中，设置了数据保存的位置是smsCode 12345@CachePut(value = &quot;smsCode&quot;, key = &quot;#tele&quot;)public String sendCodeToSMS(String tele) &#123; String code = codeUtils.generator(tele); return code;&#125; ​ 这个设定需要保障ehcache中有一个缓存空间名称叫做smsCode的配置，所以Ehcahce的配置中也要配置该缓存空间。 ​ 可以看出换了缓存后并没有影响缓存造作@Cacheable和@CachePut 3）redis缓存​ 具体操作基本和Ehcache一致，加坐标，改缓存实现类型为redis，做redis的配置。差别之处只有一点，redis的配置可以在yml文件中直接进行配置，无需制作独立的配置文件。 步骤①：导入redis的坐标 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 步骤②：配置缓存技术实现使用redis 123456spring: redis: host: localhost port: 6379 cache: type: redis ​ 如果需要对redis作为缓存进行配置，注意不是对原始的redis进行配置，而是配置redis作为缓存使用相关的配置，隶属于spring.cache.redis节点下，注意不要写错位置了。 1234567891011spring: redis: host: localhost port: 6379 cache: type: redis redis: use-key-prefix: false key-prefix: sms_ cache-null-values: false # 默认配置不能缓存null值 time-to-live: 10s 总结 springboot使用redis作为缓存实现需要导入redis的坐标 修改设置，配置缓存供应商为redis，并提供对应的缓存配置 4）Memcached缓存变更缓存为Memcached ​ 由于memcached未被springboot收录为缓存解决方案，因此使用memcached需要通过手工硬编码的方式来使用（即@Cacheable和@CachePut没用了） ​ memcached目前提供有三种客户端技术，分别是Memcached Client for Java、SpyMemcached和Xmemcached，其中性能指标各方面最好的客户端是Xmemcached，本次整合就使用这个作为客户端实现技术了。下面开始使用Xmemcached： 步骤①：导入xmemcached的坐标 12345&lt;dependency&gt; &lt;groupId&gt;com.googlecode.xmemcached&lt;/groupId&gt; &lt;artifactId&gt;xmemcached&lt;/artifactId&gt; &lt;version&gt;2.4.7&lt;/version&gt;&lt;/dependency&gt; 步骤②：配置memcached，制作memcached的配置类并返回Xmemcached 123456789@Configurationpublic class XMemcachedConfig &#123; @Bean public MemcachedClient getMemcachedClient() throws IOException &#123; MemcachedClientBuilder memcachedClientBuilder = new XMemcachedClientBuilder(&quot;localhost:11211&quot;); MemcachedClient memcachedClient = memcachedClientBuilder.build(); return memcachedClient; &#125;&#125; ​ memcached默认对外服务端口11211。 ​ 配置属性的注入 定义配置类，加载必要的配置属性，读取配置文件中memcached节点信息 12345678@Component@ConfigurationProperties(prefix = &quot;memcached&quot;)@Datapublic class XMemcachedProperties &#123; private String servers; private int poolSize; private long opTimeout;&#125; 定义memcached节点信息 1234memcached: servers: localhost:11211 poolSize: 10 opTimeout: 3000 在memcached配置类中加载信息 12345678910111213@Configurationpublic class XMemcachedConfig &#123; @Autowired private XMemcachedProperties props; @Bean public MemcachedClient getMemcachedClient() throws IOException &#123; MemcachedClientBuilder memcachedClientBuilder = new XMemcachedClientBuilder(props.getServers()); memcachedClientBuilder.setConnectionPoolSize(props.getPoolSize()); memcachedClientBuilder.setOpTimeout(props.getOpTimeout()); MemcachedClient memcachedClient = memcachedClientBuilder.build(); return memcachedClient; &#125;&#125; 步骤③：使用xmemcached客户端操作缓存，注入MemcachedClient对象 123456789101112131415161718192021222324252627@Servicepublic class SMSCodeServiceImpl implements SMSCodeService &#123; @Autowired private CodeUtils codeUtils; @Autowired private MemcachedClient memcachedClient; public String sendCodeToSMS(String tele) &#123; String code = codeUtils.generator(tele); try &#123; memcachedClient.set(tele,10,code); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return code; &#125; public boolean checkCode(SMSCode smsCode) &#123; String code = null; try &#123; code = memcachedClient.get(smsCode.getTele()).toString(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return smsCode.getCode().equals(code); &#125;&#125; ​ 总结： memcached安装后需要启动对应服务才可以对外提供缓存功能，安装memcached服务需要基于windows系统管理员权限 由于springboot没有提供对memcached的缓存整合方案，需要采用手工编码的形式创建xmemcached客户端操作缓存 导入xmemcached坐标后，创建memcached配置类，注册MemcachedClient对应的bean，用于操作缓存 初始化MemcachedClient对象所需要使用的属性可以通过自定义配置属性类的形式加载 思考 ​ redis需要安装独立的服务器，连接时需要输入对应的服务器地址，这种是远程缓存。Ehcache是一个典型的内存级缓存，因为它什么也不用安装，启动后导入jar包就有缓存功能了。考虑是否能够同时使用者两种类型的缓存，j2cache就可以做到。 10.定时任务整合1）Quartz​ Quartz技术是一个比较成熟的定时任务框架，有点繁琐，配置略微复杂。springboot对其进行整合后，简化了一系列的配置，将很多配置采用默认设置，整体变得较简单。 工作（Job）：用于定义具体执行的工作 工作明细（JobDetail）：用于描述定时工作相关的信息 触发器（Trigger）：描述了工作明细与调度器的对应关系 调度器（Scheduler）：用于描述触发工作的执行规则，通常使用cron表达式定义规则 ​ 简单说就是你定时干什么事情，这就是工作，工作不可能就是一个简单的方法，还要设置一些明细信息。工作啥时候执行，设置一个调度器，可以简单理解成设置一个工作执行的时间。工作和调度都是独立定义的，它们两个怎么配合到一起呢？用触发器。完了，就这么多。下面开始springboot整合Quartz。 步骤①：导入springboot整合Quartz的starter 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-quartz&lt;/artifactId&gt;&lt;/dependency&gt; 步骤②：定义任务Bean，按照Quartz的开发规范制作，继承QuartzJobBean，这就是Job 123456public class MyJob extends QuartzJobBean &#123; @Override protected void executeInternal(JobExecutionContext context) throws JobExecutionException &#123; System.out.println(&quot;quartz task run...&quot;); &#125;&#125; 步骤③：创建Quartz配置类，定义工作明细（JobDetail）与触发器的（Trigger）bean（包含调度器（Scheduler）） 1234567891011121314151617181920212223242526@Configurationpublic class QuartzConfig &#123; //这是工作对应的工作明细 @Bean public JobDetail printJobDetail()&#123; //绑定具体的工作 return JobBuilder. newJob(MyJob.class).//这里还有很多其他参数，如取别名 storeDurably(). build(); &#125; //这是对应工作明细绑定的触发器 @Bean public Trigger printJobTrigger()&#123; //Cron表达式，也就是scheduler ScheduleBuilder schedBuilder = CronScheduleBuilder.cronSchedule(&quot;0/5 * * * * ?&quot;);//每5s执行一次 //绑定对应的工作明细 return TriggerBuilder. newTrigger(). forJob(printJobDetail()).//传入工作明细 withSchedule(schedBuilder).//传入调度器 build(); &#125;&#125; ​ 工作明细中要设置对应的具体工作，使用newJob()操作传入对应的工作任务类型即可。 ​ 触发器需要绑定任务，使用forJob()操作传入绑定的工作明细对象。此处可以为工作明细设置名称然后使用名称绑定，也可以直接调用对应方法绑定。触发器中最核心的规则是执行时间，此处使用调度器定义执行时间，执行时间描述方式使用的是cron表达式。 总结 springboot整合Quartz就是将Quartz对应的核心对象交给spring容器管理，包含两个对象，JobDetail和Trigger对象 JobDetail对象描述的是工作的执行信息，需要绑定一个QuartzJobBean类型的对象 Trigger对象定义了一个触发器，需要为其指定绑定的JobDetail是哪个，同时要设置执行周期调度器 2）Task​ spring根据定时任务的特征，将定时任务的开发简化到了极致，Task是Quartz的一个轻量级实现。 步骤①：开启定时任务功能，在引导类上开启定时任务功能的开关，使用注解@EnableScheduling 12345678@SpringBootApplication//开启定时任务功能@EnableSchedulingpublic class Springboot22TaskApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(Springboot22TaskApplication.class, args); &#125;&#125; 步骤②：定义Bean，在对应要定时执行的操作上方，使用注解@Scheduled定义执行的时间，执行时间的描述方式还是cron表达式 1234567@Componentpublic class MyBean &#123; @Scheduled(cron = &quot;0/1 * * * * ?&quot;) public void print()&#123; System.out.println(Thread.currentThread().getName()+&quot; :spring task run...&quot;); &#125;&#125; ​ ​ 如何想对定时任务进行相关配置，可以通过配置文件进行 123456789spring: task: scheduling: pool: size: 1 # 任务调度线程池大小 默认 1 thread-name-prefix: ssm_ # 调度线程名称前缀 默认 scheduling- shutdown: await-termination: false # 线程池关闭时等待所有任务完成 await-termination-period: 10s # 调度线程关闭前最大等待时间，确保最后一定关闭 总结 spring task需要使用注解@EnableScheduling开启定时任务功能 为定时执行的的任务设置执行周期，描述方式cron表达式 task相关：http://www.blogjava.net/bolo/archive/2015/03/12/423408.html 11.邮件文件发送协议（计网知识）： SMTP（Simple Mail Transfer Protocol）：简单邮件传输协议，用于发送电子邮件的传输协议 POP3（Post Office Protocol - Version 3）：用于接收电子邮件的标准协议 IMAP（Internet Mail Access Protocol）：互联网消息协议，是POP3的替代协议 1）简单邮件即使用JavaMailSender 步骤①：导入springboot整合javamail的starter 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-mail&lt;/artifactId&gt;&lt;/dependency&gt; 步骤②：配置邮箱的登录信息 12345spring: mail: host: smtp.163.com username: test@163.com password: test ​ java程序仅用于发送邮件，邮件的功能还是邮件供应商提供的，所以这里是用别人的邮件服务，要配置对应信息。 ​ host配置的是提供邮件服务的主机协议，当前程序仅用于发送邮件，因此配置的是smtp的协议。 ​ password并不是邮箱账号的登录密码，是邮件供应商提供的一个加密后的密码，也是为了保障系统安全性。不然外部人员通过地址访问下载了配置文件，直接获取到了邮件密码就会有极大的安全隐患。有关该密码的获取每个邮件供应商提供的方式都不一样，此处略过。可以到邮件供应商的设置页面找POP3或IMAP这些关键词找到对应的获取位置。 步骤③：发送邮件 1234567891011121314151617181920212223242526272829public interface IMailService &#123; void sendMail();&#125;@Servicepublic class SendMailServiceImpl implements SendMailService &#123; @Autowired private JavaMailSender javaMailSender; //发送人 private String from = &quot;test@qq.com&quot;; //接收人 private String to = &quot;test@126.com&quot;; //标题 private String subject = &quot;测试邮件&quot;; //正文 private String context = &quot;测试邮件正文内容&quot;; @Override public void sendMail() &#123; SimpleMailMessage message = new SimpleMailMessage(); message.setFrom(from+&quot;(摩西)&quot;);//若from后面加字&quot;(xx)&quot;，收件人邮箱from就是xx message.setTo(to); message.setSubject(subject); message.setText(context); javaMailSender.send(message); &#125;&#125; ​ 将发送邮件的必要信息（发件人、收件人、标题、正文）封装到SimpleMailMessage对象中，可以根据规则设置发送人昵称等。 2）发送多组件邮件​ 邮件携带附件、复杂正文（html） ​ 发送简单邮件仅需要提供对应的4个基本信息就可以了，如果想发送复杂的邮件，需要更换邮件对象。使用MimeMessage可以发送特殊的邮件。 发送网页正文邮件 1234567891011121314151617181920212223242526272829@Servicepublic class SendMailServiceImpl2 implements SendMailService &#123; @Autowired private JavaMailSender javaMailSender; //发送人 private String from = &quot;test@qq.com&quot;; //接收人 private String to = &quot;test@126.com&quot;; //标题 private String subject = &quot;测试邮件&quot;; //正文 private String context = &quot;&lt;img src=&#x27;ABC.JPG&#x27;/&gt;&lt;a href=&#x27;https://www.itcast.cn&#x27;&gt;点开有惊喜&lt;/a&gt;&quot;; public void sendMail() &#123; try &#123; MimeMessage message = javaMailSender.createMimeMessage(); MimeMessageHelper helper = new MimeMessageHelper(message,true);//第二个boolean代表支持多组件：附件等 helper.setFrom(to+&quot;(小甜甜)&quot;); helper.setTo(from); helper.setSubject(subject); helper.setText(context,true); //此处设置正文支持html解析 javaMailSender.send(message); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; helper.setText(context,true);这里的第二个参数为true代表正文支持html解析 发送带有附件的邮件 123456789101112131415161718192021222324252627282930313233343536@Servicepublic class SendMailServiceImpl2 implements SendMailService &#123; @Autowired private JavaMailSender javaMailSender; //发送人 private String from = &quot;test@qq.com&quot;; //接收人 private String to = &quot;test@126.com&quot;; //标题 private String subject = &quot;测试邮件&quot;; //正文 private String context = &quot;测试邮件正文&quot;; public void sendMail() &#123; try &#123; MimeMessage message = javaMailSender.createMimeMessage(); MimeMessageHelper helper = new MimeMessageHelper(message,true); //第二个boolean代表支持多组件：附件等 helper.setFrom(to+&quot;(摩西)&quot;); helper.setTo(from); helper.setSubject(subject); helper.setText(context); //添加附件 File f1 = new File(&quot;springboot_23_mail-0.0.1-SNAPSHOT.jar&quot;); File f2 = new File(&quot;resources\\\\logo.png&quot;); helper.addAttachment(f1.getName(),f1); helper.addAttachment(&quot;kamenlaida.png&quot;,f2); //注意这里要加后缀.png才能预览 javaMailSender.send(message); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; MimeMessageHelper helper &#x3D; new MimeMessageHelper(message,true);这里第二个boolean代表支持多组件：附件等 总结： MimeMsg需要MimeMsgHelper来设置属性，但最后send的任然是javaMailSender.send(mimeMsg) 发送html解析正文需要：helper.setText(context,true); &#x2F;&#x2F;此处设置正文支持html解析 发送附件需要：MimeMessageHelper helper &#x3D; new MimeMessageHelper(message,true); &#x2F;&#x2F;第二个boolean代表支持多组件：附件等 12.监控现在有3个服务支撑着一个程序的运行，每个服务都有自己的运行状态。 ​ 此时被监控的信息就要在三个不同的程序中去查询并展示，但是三个服务是服务于一个程序的运行的，如果不能合并到一个平台上展示，监控工作量巨大，而且信息对称性差，要不停的在三个监控端查看数据。如果将业务放大成30个，300个，3000个呢？看来必须有一个单独的平台，将多个被监控的服务对应的监控指标信息汇总在一起，这样更利于监控工作的开展。 ​ 新的程序专门用来监控，新的问题就出现了，是被监控程序主动上报信息还是监控程序主动获取信息？如果监控程序不能主动获取信息，这就意味着监控程序有可能看到的是很久之前被监控程序上报的信息，万一被监控程序宕机了，监控程序就无法区分究竟是好久没发信息了，还是已经下线了。所以监控程序必须具有主动发起请求获取被监控服务信息的能力。 ​ 如果监控程序要监控服务时，主动获取对方的信息。那监控程序如何知道哪些程序被自己监控呢？不可能在监控程序中设置我监控谁，这样互联网上的所有程序岂不是都可以被监控到，这样的话信息安全将无法得到保障。合理的做法只能是在被监控程序启动时上报监控程序，告诉监控程序你可以监控我了。看来需要在被监控程序端做主动上报的操作，这就要求被监控程序中配置对应的监控程序是谁。 ​ 被监控程序可以提供各种各样的指标数据给监控程序看，但是每一个指标都代表着公司的机密信息，并不是所有的指标都可以给任何人看的，所以被监控指标的是否开放出来给监控系统看，也需要做详细的设定。 1）Spring Boot Admin​ Spring Boot Admin是一个开源的web程序，它通过请求获得actuator的相关端点信息，在web上进行呈现。 ①监控端监控端也就是服务端开发 步骤①：导入springboot admin对应的starter，版本与当前使用的springboot版本保持一致，并将其配置成web工程 12345678910&lt;dependency&gt; &lt;groupId&gt;de.codecentric&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-admin-starter-server&lt;/artifactId&gt; &lt;version&gt;2.5.4&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; ​ 上述过程可以通过创建项目时使用勾选的形式完成。 步骤②：在引导类上添加注解@EnableAdminServer，声明当前应用启动后作为SpringBootAdmin的服务器使用 1234567@SpringBootApplication@EnableAdminServerpublic class Springboot25AdminServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(Springboot25AdminServerApplication.class, args); &#125;&#125; ②被监控端被监控端也就是客户端开发，客户端程序开发其实和服务端开发思路基本相似，多了一些配置而已。 步骤①：导入springboot admin对应的starter，版本与当前使用的springboot版本保持一致，并将其配置成web工程 12345678910&lt;dependency&gt; &lt;groupId&gt;de.codecentric&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-admin-starter-client&lt;/artifactId&gt; &lt;version&gt;2.5.4&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; ​ 上述过程也可以通过创建项目时使用勾选的形式完成，不过一定要小心，端口配置成不一样的，否则会冲突。 步骤②：设置当前客户端将信息上传到哪个服务器上（监控端），通过yml文件配置 12345spring: boot: admin: client: url: http://localhost:8080 # 这里的port是监控端的port ​ 步骤③开放指定信息给服务器看 配置如下： 123456789server: port: 80spring: boot: admin: client: url: http://localhost:8080 //这里定义了作为客户端，应将数据发送给哪个服务器 ​ springbootadmin的客户端默认开放了13组信息给服务器，但是这些信息除了一个之外，其他的信息都不让通过HTTP请求查看。所以你看到的信息基本上就没什么内容了，只能看到一个内容，就是下面的健康信息。 ​ 但是即便如此我们看到健康信息中也没什么内容，原因在于健康信息中有一些信息描述了你当前应用使用了什么技术等信息，如果无脑的对外暴露功能会有安全隐患。通过配置就可以开放所有的健康信息明细查看了。 12345678910111213server: port: 80spring: boot: admin: client: url: http://localhost:8080 //这里定义了作为客户端，应将数据发送给哪个服务器management: endpoint: health: show-details: always # 在没开启之前这里的默认值是never ​ 这样我们就可以查看所有的健康信息了，但目前除了健康信息，其他信息都查阅不了。原因在于其他12种信息是默认不提供给服务器通过HTTP请求查阅的，所以需要开启查阅的内容项，使用*表示查阅全部。 12345678910111213141516171819server: port: 80spring: boot: admin: client: url: http://localhost:8080 //这里定义了作为客户端，应将数据发送给哪个服务器management: endpoint: health: show-details: always # 在没开启之前这里的默认值是never endpoints: web: exposure: include: &quot;*&quot; # include: health 默认只开启了health ​ 这样就可以在web端看到这些信息了 ③配置多个客户端​ 可以通过配置客户端的方式在其他的springboot程序中添加客户端坐标，这样当前服务器就可以监控多个客户端程序了。每个客户端展示不同的监控信息。 类加载面板中可以查阅到开发者自定义的类，如左图 ​ 映射中可以查阅到当前应用配置的所有请求 ​ 性能指标中可以查阅当前应用独有的请求路径统计数据 ​ 总结 开发监控服务端需要导入坐标，然后在引导类上添加注解@EnableAdminServer，并将其配置成web程序即可 开发被监控的客户端需要导入坐标，然后配置服务端服务器地址，并做开放指标的设定即可 在监控平台中可以查阅到各种各样被监控的指标，前提是客户端开放了被监控的指标 2）监控原理​ 监控平台中显示的信息实际上是通过对被监控的应用发送请求得到的。打开被监控应用的pom文件，其中导入了springboot admin的对应的client，在这个资源中导入了一个名称叫做actuator的包。被监控的应用之所以可以对外提供&#x2F;actuator&#x2F;…请求路径，就是因为添加了这个包。 ​ Actuator，可以称为端点，描述了一组监控信息，SpringBootAdmin提供了多个内置端点，通过访问端点就可以获取对应的监控信息，也可以根据需要自定义端点信息。通过发送请求路径**&#x2F;actuator可以访问应用所有端点信息，如果端点中还有明细信息可以发送请求&#x2F;actuator&#x2F;端点名称**来获取详细信息。以下列出了所有端点信息说明： ID 描述 默认启用 auditevents 暴露当前应用程序的审计事件信息。 是 beans 显示应用程序中所有 Spring bean 的完整列表。 是 caches 暴露可用的缓存。 是 conditions 显示在配置和自动配置类上评估的条件以及它们匹配或不匹配的原因。 是 configprops 显示所有 @ConfigurationProperties 的校对清单。 是 env 暴露 Spring ConfigurableEnvironment 中的属性。 是 flyway 显示已应用的 Flyway 数据库迁移。 是 health 显示应用程序健康信息 是 httptrace 显示 HTTP 追踪信息（默认情况下，最后 100 个 HTTP 请求&#x2F;响应交换）。 是 info 显示应用程序信息。 是 integrationgraph 显示 Spring Integration 图。 是 loggers 显示和修改应用程序中日志记录器的配置。 是 liquibase 显示已应用的 Liquibase 数据库迁移。 是 metrics 显示当前应用程序的指标度量信息。 是 mappings 显示所有 @RequestMapping 路径的整理清单。 是 scheduledtasks 显示应用程序中的调度任务。 是 sessions 允许从 Spring Session 支持的会话存储中检索和删除用户会话。当使用 Spring Session 的响应式 Web 应用程序支持时不可用。 是 shutdown 正常关闭应用程序。 否 threaddump 执行线程 dump。 是 heapdump 返回一个 hprof 堆 dump 文件。 是 jolokia 通过 HTTP 暴露 JMX bean（当 Jolokia 在 classpath 上时，不适用于 WebFlux）。 是 logfile 返回日志文件的内容（如果已设置 logging.file 或 logging.path 属性）。支持使用 HTTP Range 头来检索部分日志文件的内容。 是 prometheus 以可以由 Prometheus 服务器抓取的格式暴露指标。 是 ​ 上述端点每一项代表被监控的指标，如果对外开放则监控平台可以查询到对应的端点信息，如果未开放则无法查询对应的端点信息。通过配置可以设置端点是否对外开放功能。使用enable属性控制端点是否对外开放。其中health端点为默认端点，不能关闭。 123456management: endpoint: health: # 端点名称 show-details: always info: # 端点名称 enabled: true # 是否开放 ​ 为了方便开发者快速配置端点，springboot admin设置了13个较为常用的端点作为默认开放的端点，如果需要控制默认开放的端点的开放状态，可以通过配置设置，如下： 123management: endpoints: enabled-by-default: true # 是否开启默认端点，默认值true ​ 上述端点开启后，就可以通过端点对应的路径查看对应的信息了。但是此时还不能通过HTTP请求查询此信息，还需要开启通过HTTP请求查询的端点名称，使用“*”可以简化配置成开放所有端点的WEB端HTTP请求权限。 12345management: endpoints: web: exposure: include: &quot;*&quot; ​ 整体上来说，对于端点的配置有两组信息，一组是endpoints开头的，对所有端点进行配置，一组是endpoint开头的，对具体端点进行配置。 12345678910111213management: endpoint: # 具体端点的配置 health: show-details: always info: enabled: true endpoints: # 全部端点的配置 web: exposure: include: &quot;*&quot; enabled-by-default: true #开启springboot admin设置了13个较为常用的端点作为默认开放的端点 enabled-by-default只是在说客户端开启了这13个端点，并不代表web端有权限查看 因为除了web端，java还可以通过以jconsole的形式进行监控查看，jconsle并不是通过http技术来获得数据的，而是通过jmx技术获得数据的，所以jconsle拥有查看这些开发端点的所有权限 而web并没有查看这些开发端点所有权限，需要单独配置 3）自定义端点信息​ 端点描述了被监控的信息，除了系统默认的指标，还可以自行添加显示的指标，下面就通过3种不同的端点的指标自定义方式来学习端点信息的二次开发。 ①INFO端点​ info端点描述了当前应用的基本信息，没有配置的时候里面没有信息，可以通过两种形式快速配置info端点的信息 配置形式 在yml文件中通过设置info节点的信息就可以快速配置端点信息 1234info: appName: @project.artifactId@ # 从pom中获取数据 version: @project.version@ author: fla 配置完毕后，对应信息显示在监控平台上 也可以通过请求端点信息路径获取对应json信息 编程形式 通过配置的形式只能添加固定的数据，如果需要动态数据还可以通过配置bean的方式为info端点添加信息，此信息与配置信息共存 注：需要实现InfoContributor接口，方法需要传入Info.Builder builder（自动装配） 1234567891011@Componentpublic class InfoConfig implements InfoContributor &#123; @Override public void contribute(Info.Builder builder) &#123; builder.withDetail(&quot;runTime&quot;,System.currentTimeMillis()); //添加单个信息 Map infoMap = new HashMap(); infoMap.put(&quot;buildTime&quot;,&quot;2006&quot;); builder.withDetails(infoMap); //添加一组信息 &#125;&#125; ②Health端点​ health端点描述当前应用的运行健康指标，即程序中各个组件的运行情况（mysql、redis等），即应用的运行是否成功。通过编程的形式可以扩展指标信息。 注：需要继承AbstractHealthIndicator，方法需要传入Health.Builder builder（自动装配） 1234567891011121314151617@Componentpublic class HealthConfig extends AbstractHealthIndicator &#123; @Override protected void doHealthCheck(Health.Builder builder) throws Exception &#123; boolean condition = true; if(condition) &#123; builder.status(Status.UP); //设置运行状态为启动状态 builder.withDetail(&quot;runTime&quot;, System.currentTimeMillis()); Map infoMap = new HashMap(); infoMap.put(&quot;buildTime&quot;, &quot;2006&quot;); builder.withDetails(infoMap); &#125;else&#123; builder.status(Status.OUT_OF_SERVICE); //设置运行状态为不在服务状态 builder.withDetail(&quot;上线了吗？&quot;,&quot;你做梦&quot;); &#125; &#125;&#125; ​ 当任意一个组件状态不为UP时，整体应用对外服务状态为非UP状态。 ③Metrics端点​ metrics端点描述了性能指标，除了系统自带的监控性能指标，还可以自定义性能指标。 这里是在某个service中new了一个构造方法，并加了参数，在加载这个bean的时候就会加载MeterRegistry meterRegistry 注意： counter &#x3D; meterRegistry.counter(“用户付费操作次数：”);里面添加了key值，counter则是value 1234567891011121314151617@Servicepublic class BookServiceImpl extends ServiceImpl&lt;BookDao, Book&gt; implements IBookService &#123; @Autowired private BookDao bookDao; private Counter counter; public BookServiceImpl(MeterRegistry meterRegistry)&#123; counter = meterRegistry.counter(&quot;用户付费操作次数：&quot;); &#125; @Override public boolean spend(Integer id) &#123; counter.increment(); return bookDao.spend(id) &gt; 0; &#125;&#125; ​ 在性能指标中就出现了自定义的性能指标监控项 ④自定义端点​ 可以根据业务需要自定义端点，方便业务监控 @Endpoint(id&#x3D;”pay”,enableByDefault &#x3D; true) id是&#x2F;actuators&#x2F;xx中的xx enableByDefault &#x3D; true 代表默认开启，否则就需要在配置中开启了 12345678management: endpoint: # 具体端点的配置 health: show-details: always info: enabled: true pay: enabled: true 必须@ReadOperation 定义一个方法，方法名随便，这个方法是在请求&#x2F;actuators&#x2F;pay后执行的操作 可以通过map来返回jason数据 123456789101112@Component@Endpoint(id=&quot;pay&quot;,enableByDefault = true)public class PayEndpoint &#123; @ReadOperation public Object getPay()&#123; Map payMap = new HashMap(); payMap.put(&quot;level 1&quot;,&quot;300&quot;); payMap.put(&quot;level 2&quot;,&quot;291&quot;); payMap.put(&quot;level 3&quot;,&quot;666&quot;); return payMap; &#125;&#125; ​ 由于此端点数据spirng boot admin无法预知该如何展示，所以通过admin界面无法看到此数据，通过HTTP请求路径可以获取到当前端点的信息，但是需要先开启当前端点对外功能，或者设置当前端点为默认开发的端点。","categories":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"http://example.com/categories/Spring-Boot/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"http://example.com/tags/Spring-Boot/"}]},{"title":"RabbitMQ","slug":"rabbitMQ - 副本","date":"2022-04-10T07:29:48.000Z","updated":"2022-09-28T07:55:01.723Z","comments":true,"path":"2022/04/10/rabbitMQ - 副本/","link":"","permalink":"http://example.com/2022/04/10/rabbitMQ%20-%20%E5%89%AF%E6%9C%AC/","excerpt":"rabbitMQ的相关知识：ttl、死信队列、消息可靠性、消息补偿、集群等","text":"rabbitMQ的相关知识：ttl、死信队列、消息可靠性、消息补偿、集群等 1.springBoot整合1）依赖①导入springboot整合amqp的starter，amqp协议默认实现为rabbitmq方案 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt; 2）配置①yml配置1234567spring: rabbitmq: host: localhost port: 5672 virtual-host: / #虚拟机配置 username: xxx password: xxx ②RabbitMQ配置类direct模式： 1234567891011121314151617181920212223@Configurationpublic class RabbitConfigDirect &#123; @Bean public Queue directQueue()&#123; return new Queue(&quot;direct_queue&quot;); &#125; @Bean public Queue directQueue2()&#123; return new Queue(&quot;direct_queue2&quot;); &#125; @Bean public DirectExchange directExchange()&#123; return new DirectExchange(&quot;directExchange&quot;); &#125; @Bean public Binding bindingDirect()&#123; return BindingBuilder.bind(directQueue()).to(directExchange()).with(&quot;direct&quot;); &#125; @Bean public Binding bindingDirect2()&#123; return BindingBuilder.bind(directQueue2()).to(directExchange()).with(&quot;direct2&quot;); &#125;&#125; 这种方式的queue和exchange都是直接new的 binding的时候的参数是通过方法调用的方式获得的，这里有@Configuration注解所以可以保证是同一个bean topic模式： 1234567891011121314151617181920212223@Configurationpublic class RabbitConfigTopic &#123; @Bean public Queue topicQueue()&#123; return new Queue(&quot;topic_queue&quot;); &#125; @Bean public Queue topicQueue2()&#123; return new Queue(&quot;topic_queue2&quot;); &#125; @Bean public TopicExchange topicExchange()&#123; return new TopicExchange(&quot;topicExchange&quot;); &#125; @Bean public Binding bindingTopic()&#123; return BindingBuilder.bind(topicQueue()).to(topicExchange()).with(&quot;topic.*.id&quot;); &#125; @Bean public Binding bindingTopic2()&#123; return BindingBuilder.bind(topicQueue2()).to(topicExchange()).with(&quot;topic.orders.*&quot;); &#125;&#125; 这种方式的queue和exchange都是直接new的 binding的时候的参数是通过方法调用的方式获得的，这里有@Configuration注解所以可以保证是同一个bean 123456789101112131415161718192021222324252627@Configurationpublic class RabbitMQConfig &#123; //交换机名称 public static final String ITEM_TOPIC_EXCHANGE = &quot;item_topic_exchange&quot;; //队列名称 public static final String ITEM_QUEUE = &quot;item_queue&quot;; //声明交换机 @Bean(&quot;itemTopicExchange&quot;) public Exchange topicExchange()&#123; return ExchangeBuilder.topicExchange(ITEM_TOPIC_EXCHANGE).durable(true).build(); &#125; //声明队列 @Bean(&quot;itemQueue&quot;) public Queue itemQueue()&#123; return QueueBuilder.durable(ITEM_QUEUE).build(); &#125; //绑定队列和交换机 @Bean public Binding itemQueueExchange(@Qualifier(&quot;itemQueue&quot;) Queue queue, @Qualifier(&quot;itemTopicExchange&quot;) Exchange exchange)&#123; return BindingBuilder.bind(queue).to(exchange).with(&quot;item.#&quot;).noargs(); &#125;&#125; 这里binding是通过参数注入的方式来绑定，queue和exchange都取了别名的 并且queue,exchange,binding都是通过builder来创建的 3）使用①生产者使用AmqpTemplate操作RabbitMQ 1234567891011121314@Servicepublic class MessageServiceRabbitmqTopicImpl implements MessageService &#123; //@Autowired //private RabbitTemplate rabbitTemplate; @Autowired private AmqpTemplate amqpTemplate; @Override public void sendMessage(String id) &#123; System.out.println(&quot;待发送短信的订单已纳入处理队列（rabbitmq topic），id：&quot;+id); amqpTemplate.convertAndSend(&quot;topicExchange&quot;,&quot;topic.orders.id&quot;,id); &#125;&#125; 发送消息后，根据当前提供的routingKey与绑定交换机时设定的routingKey进行匹配，规则匹配成功消息才会进入到对应的队列中。 ②消费者使用消息监听器在服务器启动后，监听指定队列 1234567891011@Componentpublic class MessageListener &#123; @RabbitListener(queues = &quot;topic_queue&quot;) //使用@RabbitListener监听指定队列 public void receive(String id)&#123; System.out.println(&quot;已完成短信发送业务(rabbitmq topic 1)，id：&quot;+id); &#125; @RabbitListener(queues = &quot;topic_queue2&quot;) public void receive2(String id)&#123; System.out.println(&quot;已完成短信发送业务(rabbitmq topic 22222222)，id：&quot;+id); &#125;&#125; 使用注解@RabbitListener定义当前方法监听RabbitMQ中指定名称的消息队列 2.TTL1）设置队列的TTL12345678910111213141516171819202122232425@Configurationpublic class DirectRabbitConfig &#123; @Bean public DirectExchange ttlDirectExchange() &#123; return new DirectExchange(&quot;ttl_direct_exchange&quot;, true, false); &#125; //队列 起名：TestDirectQueue @Bean public Queue ttlDirectQueue() &#123; // durable:是否持久化,默认是false,持久化队列：会被存储在磁盘上，当消息代理重启时仍然存在，暂存队列：当前连接有效 // exclusive:默认也是false，只能被当前创建的连接使用，而且当连接关闭后队列即被删除。此参考优先级高于durable // autoDelete:是否自动删除，当没有生产者或者消费者使用此队列，该队列会自动删除。 //一般设置一下队列的持久化就好,其余两个就是默认false HashMap&lt;String, Object&gt; args = new HashMap&lt;&gt;(); args.put(&quot;x-message-ttl&quot;, 5000); return new Queue(&quot;ttl.direct.queue&quot;, true, args); &#125; @Bean public Binding bindingDirect1() &#123; return BindingBuilder.bind(ttlDirectQueue()).to(ttlDirectExchange()).with(&quot;ttl&quot;); &#125;&#125; x-message-ttl参数设置队列过期时间，然后传入Queue中，注意时间一定是int型 如果没有配置死信队列，那么过期消息会被丢弃 配置了死信队列，过期消息会被放入死信队列中 2）设置消息的TTLrabbitMQ配置 1234567891011121314151617181920212223@Configurationpublic class DirectRabbitConfig &#123; @Bean public DirectExchange ttlDirectExchange() &#123; return new DirectExchange(&quot;ttlMessage_direct_exchange&quot;, true, false); &#125; //队列 起名：TestDirectQueue @Bean public Queue ttlDirectQueue() &#123; // durable:是否持久化,默认是false,持久化队列：会被存储在磁盘上，当消息代理重启时仍然存在，暂存队列：当前连接有效 // exclusive:默认也是false，只能被当前创建的连接使用，而且当连接关闭后队列即被删除。此参考优先级高于durable // autoDelete:是否自动删除，当没有生产者或者消费者使用此队列，该队列会自动删除。 //一般设置一下队列的持久化就好,其余两个就是默认false return new Queue(&quot;ttlMessage.direct.queue&quot;, true, args); &#125; @Bean public Binding bindingDirect1() &#123; return BindingBuilder.bind(ttlDirectQueue()).to(ttlDirectExchange()).with(&quot;ttlMessage&quot;); &#125;&#125; 可以看到交换机和队列的配置就是普通队列的配置 消费者： 12345678910111213141516171819202122public void makeorderTtlMessage(String userid, String productid, int num) &#123; // 1.:根据商品id查询库存是否充足 // 2:保存订单 String orderId = UUID.randomUUID() .toString (); System.out.println(&quot;订单生产成功:&quot; + orderId); //3:通过Mg来完成消息的分发 //参数1:交换机 参数2:路由key/queue队列名称参数 3:消息内容 String exchangeName = &quot;ttl_direct _exchange &quot;; String routingkey = &quot;ttlmessage&quot;; //给消息设置过期时间 MessagePostProcessor messagePostProcessor = new MessagePostProcessor() &#123; @Override public Message postProcessMessage(Message message) throws AmqpException &#123; message.getMessageProperties().setExpiration(&quot;5000&quot;);//这里就是字符串 message.getMessageProperties().setContentEncoding(&quot;UTF-8&quot;); return message; &#125;; //传入了messagePostProcessor来为消息设置ttl等属性 rabbitTemplate.convertandsend(exchangeName，routingkey,orderId,messagePostProcessor); &#125;&#125; 创建MessagePostProcessor来为消息设置属性，最后在发送的时候作为参数传入 setExpiration时候使用字符串 3）队列ttl和消息ttl的差别 队列ttl在设置私信队列以后，过期消息会被放入死信队列 消息ttl，消息过期后该消息会被直接丢弃 如果队列ttl和消息ttl都设置了，以两者最小时间为准 控制台对比 3.死信队列1）基本概念DLX，全称为Dead-Letter-Exchange , 可以称之为死信交换机，也有人称之为死信邮箱。当消息在一个队列中变成死信(dead message)之后，它能被重新发送到另一个交换机中，这个交换机就是DLX ，绑定DLX的队列就称之为死信队列。消息变成死信，可能是由于以下的原因： 消息被拒绝 消费者出现异常 消息过期 ttl过期 队列达到最大长度 DLX也是一个正常的交换机，和一般的交换机没有区别，它能在任何的队列上被指定，实际上就是设置某一个队列的属性。当这个队列中存在死信时，Rabbitmq就会自动地将这个消息重新发布到设置的DLX上去，进而被路由到另一个队列，即死信队列。要想使用死信队列，只需要在定义队列的时候设置队列参数 x-dead-letter-exchange 指定交换机即可。 2）example①普通队列设置死信队列1234567891011121314151617181920212223242526272829@Configurationpublic class DirectRabbitConfig &#123; @Bean public DirectExchange ttlDirectExchange() &#123; return new DirectExchange(&quot;ttl_direct_exchange&quot;, true, false); &#125; @Bean public Queue ttlDirectQueue() &#123; // durable:是否持久化,默认是false,持久化队列：会被存储在磁盘上，当消息代理重启时仍然存在，暂存队列：当前连接有效 // exclusive:默认也是false，只能被当前创建的连接使用，而且当连接关闭后队列即被删除。此参考优先级高于durable // autoDelete:是否自动删除，当没有生产者或者消费者使用此队列，该队列会自动删除。 //一般设置一下队列的持久化就好,其余两个就是默认false HashMap&lt;String, Object&gt; args = new HashMap&lt;&gt;(); args.put ( &quot;x-message-ttl&quot;，5000);//这里一定是int类型， args.put ( &quot;x-max-length&quot;,5);//这里一定是int类型, args.put ( &quot;x-dead-letter-exchange&quot;,&quot;dead direct_exchange&quot;);//配置死信交换机 args.put ( &quot;x-dead-letter-routing-key&quot; , &quot;dead&quot; ) ; //fanout不需要配置 //name,durable,exclusive,autoDelete,args return new Queue (&quot;ttl.direct.queue&quot;,true,false,false,args); &#125; @Bean public Binding bindingDirect1() &#123; return BindingBuilder.bind(ttlDirectQueue()).to(ttlDirectExchange()).with(&quot;ttl&quot;); &#125;&#125; 通过x-dead-letter-exchange和死信交换机的name配置私信交换机 通过x-max-length来配置队列的最大长度 通过x-dead-letter-routing-key来配置路由key，这里是direct模式需要配置，如果是fanout直接广播死信交换机，那么不需要配置 注意：队列配置好后再添加一些配置，不会覆盖！！！只会报错！！！ 自己试验的时候可以在控制台删除后再创建 生产环境下再new一个queue，然后消费者导向新的queue ②死信队列配置123456789101112131415161718@Configurationpublic class DeadRabbitConfig &#123; @Bean public DirectExchange deadDirectExchange() &#123; return new DirectExchange(&quot;dead_direct_exchange&quot;, true, false); &#125; @Bean public Queue deadDirectQueue() &#123; return new Queue(&quot;dead.direct.queue&quot;, true); &#125; @Bean public Binding deadBinding() &#123; return BindingBuilder.bind(deadDirectQueue()).to(deadDirectExchange()).with(&quot;dead&quot;); &#125;&#125; 可以看到死信队列配置就是配置一个普通的队列，作为一个接盘侠来接收死信 4.持久化1）基本概念 持久化就把信息写入到磁盘的过程，把消息默认放在内存中是为了加快传输和消费的速度，存入磁盘是保证消息数据的持久化。 非持久消息：是指当内存不够用的时候，会把消息和数据转移到磁盘，但是重启以后非持久化队列消息就丢失。 不论是持久化的消息还是非持久化的消息都可以写入到磁盘中，只不过非持久的是等内存不足的情况下才会被写入到磁盘中。 重启rabbit-server服务，会发现持久化队列依然在，而非持久队列会丢失。 2）RabbitMQ持久化分类①队列持久化队列的持久化是定义队列时的durable参数来实现的，Durable为true时，队列才会持久化。 123456// 参数1：名字 // 参数2：是否持久化，// 参数3：独占的queue， // 参数4：不使用时是否自动删除，// 参数5：其他参数channel.queueDeclare(queueName,true,false,false,null); 其中参数2：设置为true,就代表的是持久化的含义。即durable&#x3D;true。持久化的队列在web控制台中有一个D 的标记 ②消息持久化消息持久化是通过消息的属性deliveryMode来设置是否持久化，在发送消息时通过basicPublish的参数传入。 12345678// 参数1：交换机的名字// 参数2：队列或者路由key// 参数3：是否进行消息持久化// 参数4：发送消息的内容channel.basicPublish(exchangeName, routingKey, MessageProperties.PERSISTENT_TEXT_PLAIN, //消息持久化message.getBytes()); ③RabbitMQ交换机持久化和队列一样，交换机也需要在定义的时候设置持久化的标识，否则在rabbit-server服务重启以后将丢失。 1234// 参数1：交换机的名字// 参数2：交换机的类型，topic/direct/fanout/headers// 参数3：是否持久化channel.exchangeDeclare(exchangeName,exchangeType,true); 5.内存磁盘的监控1）内存警告当内存使用超过配置的阈值或者磁盘空间剩余空间对于配置的阈值时，RabbitMQ会暂时阻塞客户端的连接，并且停止接收从客户端发来的消息，以此避免服务器的崩溃，客户端与服务端的心态检测机制也会失效。如下图： 当出现blocking或blocked话说明到达了阈值和以及高负荷运行了。 2）内存控制参考帮助文档：https://www.rabbitmq.com/configure.html当出现警告的时候，可以通过配置去修改和调整 ①命令的方式12rabbitmqctl set_vm_memory_high_watermark &lt;fraction&gt;rabbitmqctl set_vm_memory_high_watermark absolute 50MB fraction&#x2F;value 为内存阈值。默认情况是：0.4&#x2F;2GB，代表的含义是：当RabbitMQ的内存超过40%时，就会产生警告并且阻塞所有生产者的连接。通过此命令修改阈值在Broker重启以后将会失效，通过修改配置文件方式设置的阈值则不会随着重启而消失，但修改了配置文件一样要重启broker才会生效。 ②配置文件方式rabbitmq.conf 123456#默认#vm_memory_high_watermark.relative = 0.4# 使用relative相对值进行设置fraction,建议取值在04~0.7之间，不建议超过0.7vm_memory_high_watermark.relative = 0.6# 使用absolute的绝对值的方式，但是是KB,MB,GB对应的命令如下vm_memory_high_watermark.absolute = 2GB 3）RabbitMQ的内存换页在某个Broker节点及内存阻塞生产者之前，它会尝试将队列中的消息换页到磁盘以释放内存空间，持久化和非持久化的消息都会写入磁盘中，其中持久化的消息本身就在磁盘中有一个副本，所以在转移的过程中持久化的消息会先从内存中清除掉。 默认情况下，内存到达的阈值是50%时就会换页处理。也就是说，在默认情况下该内存的阈值是0.4的情况下，当内存超过0.4*0.5&#x3D;0.2时，会进行换页动作。 比如有1000MB内存，当内存的使用率达到了400MB,已经达到了极限，但是因为配置的换页内存0.5，这个时候会在达到极限400mb之前，会把内存中的200MB进行转移到磁盘中。从而达到稳健的运行。 可以通过设置 vm_memory_high_watermark_paging_ratio 来进行调整 12vm_memory_high_watermark.relative = 0.4vm_memory_high_watermark_paging_ratio = 0.7（设置小于1的值） 这个操作是为了在内存中有较多的数据超过设定阈值的时候，将部分数据存入磁盘 非持久化和持久化都会放入内存 但持久化消息本身就有一个磁盘副本，所以转移过程会现在内存中清除 为什么设置小于1，以为你如果你设置为1的阈值。内存都已经达到了极限了。你在去换页意义不是很大了 4）磁盘预警当磁盘的剩余空间低于确定的阈值时，RabbitMQ同样会阻塞生产者，这样可以避免因非持久化的消息持续换页而耗尽磁盘空间导致服务器崩溃。 默认情况下：磁盘预警为50MB的时候会进行预警。表示当前磁盘空间少于50MB的时候会阻塞生产者并且停止内存消息换页到磁盘的过程。这个阈值可以减小，但是不能完全的消除因磁盘耗尽而导致崩溃的可能性。比如在两次磁盘空间的检查空隙内，第一次检查是：60MB ，第二检查可能就是1MB,就会出现警告。 通过命令方式修改如下： 1234rabbitmqctl set_disk_free_limit &lt;disk_limit&gt;rabbitmqctl set_disk_free_limit memory_limit &lt;fraction&gt;disk_limit：固定单位 KB MB GBfraction ：是相对阈值，建议范围在:1.0~2.0之间。（相对于内存） 通过配置文件配置如下： 12disk_free_limit.relative = 3.0disk_free_limit.absolute = 50mb 这个操作是给磁盘空间设置了一个阈值，如果低于这个阈值，说明消息数据存太多到磁盘了，磁盘的容量已经不够了 6.生产者确认 origin：https://blog.csdn.net/qq_35387940/article/details/100514134 1）基本配置在rabbitmq-provider项目的application.yml文件上，加上消息确认的配置项后： 可以把publisher-confirms: true 替换为 publisher-confirm-type: correlated 12345678910111213141516171819server: port: 8021spring: #给项目来个名字 application: name: rabbitmq-provider #配置rabbitMq 服务器 rabbitmq: host: 127.0.0.1 port: 5672 username: root password: root #虚拟host 可以不设置,使用server默认host virtual-host: JCcccHost #消息确认配置项#确认消息已发送到交换机(Exchange)publisher-confirms: true#确认消息已发送到队列(Queue)publisher-returns: true 然后是配置相关的消息确认回调函数，RabbitConfig.java： 1234567891011121314151617181920212223242526272829303132333435363738@Configurationpublic class RabbitConfig &#123;@Beanpublic RabbitTemplate createRabbitTemplate(ConnectionFactory connectionFactory)&#123; RabbitTemplate rabbitTemplate = new RabbitTemplate(); rabbitTemplate.setConnectionFactory(connectionFactory); //设置开启Mandatory,才能触发回调函数,无论消息推送结果怎么样都强制调用回调函数 rabbitTemplate.setMandatory(true); //1.配置confirm回调函数 rabbitTemplate.setConfirmCallback(new RabbitTemplate.ConfirmCallback() &#123; @Override public void confirm(CorrelationData correlationData, boolean ack, String cause) &#123; System.out.println(&quot;ConfirmCallback: &quot;+&quot;相关数据：&quot;+correlationData); System.out.println(&quot;ConfirmCallback: &quot;+&quot;确认情况：&quot;+ack); System.out.println(&quot;ConfirmCallback: &quot;+&quot;原因：&quot;+cause); &#125;&#125;); //2.配置return回调函数 rabbitTemplate.setReturnCallback(new RabbitTemplate.ReturnCallback() &#123; @Override public void returnedMessage(Message message, int replyCode, String replyText, String exchange, String routingKey) &#123; System.out.println(&quot;ReturnCallback: &quot;+&quot;消息：&quot;+message); System.out.println(&quot;ReturnCallback: &quot;+&quot;回应码：&quot;+replyCode); System.out.println(&quot;ReturnCallback: &quot;+&quot;回应信息：&quot;+replyText); System.out.println(&quot;ReturnCallback: &quot;+&quot;交换机：&quot;+exchange); System.out.println(&quot;ReturnCallback: &quot;+&quot;路由键：&quot;+routingKey); &#125; &#125;); return rabbitTemplate; &#125;&#125; 配置了RabbitTemplate的一些属性 rabbitTemplate.setMandatory(true); 设置开启Mandatory,才能触发回调函数,无论消息推送结果怎么样都强制调用回调函数 setConfirmCallback配置了confirm回调函数，确认消息已经到了exchange中 setReturnCallback配置了return回调函数，确认消息已经到了queue中 2）触发情况到这里，生产者推送消息的消息确认调用回调函数已经完毕。可以看到上面写了两个回调函数，一个叫 ConfirmCallback ，一个叫 RetrunCallback；那么以上这两种回调函数都是在什么情况会触发呢？ 先从总体的情况分析，推送消息存在四种情况： ①消息推送到server，但是在server里找不到交换机②消息推送到server，找到交换机了，但是没找到队列③消息推送到server，交换机和队列啥都没找到④消息推送成功 那么我先写几个接口来分别测试和认证下以上4种情况，消息确认触发回调函数的情况： ①消息推送到server，但是在server里找不到交换机写个测试接口，把消息推送到名为‘non-existent-exchange’的交换机上（这个交换机是没有创建没有配置的）： 123456789101112@GetMapping(&quot;/TestMessageAck&quot;)public String TestMessageAck() &#123; String messageId = String.valueOf(UUID.randomUUID()); String messageData = &quot;message: non-existent-exchange test message &quot;; String createTime = LocalDateTime.now().format(DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss&quot;)); Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); map.put(&quot;messageId&quot;, messageId); map.put(&quot;messageData&quot;, messageData); map.put(&quot;createTime&quot;, createTime); rabbitTemplate.convertAndSend(&quot;non-existent-exchange&quot;, &quot;TestDirectRouting&quot;, map); return &quot;ok&quot;;&#125; 调用接口，查看rabbitmq-provuder项目的控制台输出情况（原因里面有说，没有找到交换机’non-existent-exchange’）： 1234562019-09-04 09:37:45.197 ERROR 8172 --- [ 127.0.0.1:5672] o.s.a.r.c.CachingConnectionFactory : Channel shutdown: channel error; protocol method: #method&lt;channel.close&gt;(reply-code=404, reply-text=NOT_FOUND - no exchange &#x27;non-existent-exchange&#x27; in vhost &#x27;JCcccHost&#x27;, class-id=60, method-id=40)ConfirmCallback: 相关数据：nullConfirmCallback: 确认情况：falseConfirmCallback: 原因：channel error; protocol method: #method&lt;channel.close&gt;(reply-code=404, reply-text=NOT_FOUND - no exchange &#x27;non-existent-exchange&#x27; in vhost &#x27;JCcccHost&#x27;, class-id=60, method-id=40) 结论： ①这种情况触发的是 ConfirmCallback 回调函数。 ②消息推送到server，找到交换机了，但是没找到队列这种情况就是需要新增一个交换机，但是不给这个交换机绑定队列，我来简单地在DirectRabitConfig里面新增一个直连交换机，名叫‘lonelyDirectExchange’，但没给它做任何绑定配置操作： 1234@BeanDirectExchange lonelyDirectExchange() &#123; return new DirectExchange(&quot;lonelyDirectExchange&quot;);&#125; 然后写个测试接口，把消息推送到名为‘lonelyDirectExchange’的交换机上（这个交换机是没有任何队列配置的）： 12345678910111213@GetMapping(&quot;/TestMessageAck2&quot;)public String TestMessageAck2() &#123; String messageId = String.valueOf(UUID.randomUUID()); String messageData = &quot;message: lonelyDirectExchange test message &quot;; String createTime = LocalDateTime.now().format(DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss&quot;)); Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); map.put(&quot;messageId&quot;, messageId); map.put(&quot;messageData&quot;, messageData); map.put(&quot;createTime&quot;, createTime); //TestDirectRouting没有配置 rabbitTemplate.convertAndSend(&quot;lonelyDirectExchange&quot;, &quot;TestDirectRouting&quot;, map); return &quot;ok&quot;;&#125; 调用接口，查看rabbitmq-provuder项目的控制台输出情况： 12345678ReturnCallback: 消息：(Body:&#x27;&#123;createTime=2019-09-04 09:48:01, messageId=563077d9-0a77-4c27-8794-ecfb183eac80, messageData=message: lonelyDirectExchange test message &#125;&#x27; MessageProperties [headers=&#123;&#125;, contentType=application/x-java-serialized-object, contentLength=0, receivedDeliveryMode=PERSISTENT, priority=0, deliveryTag=0])ReturnCallback: 回应码：312ReturnCallback: 回应信息：NO_ROUTEReturnCallback: 交换机：lonelyDirectExchangeReturnCallback: 路由键：TestDirectRoutingConfirmCallback: 相关数据：nullConfirmCallback: 确认情况：trueConfirmCallback: 原因：null 可以看到这种情况，两个函数都被调用了；这种情况下，消息是推送成功到服务器了的，所以ConfirmCallback对消息确认情况是true；而在RetrunCallback回调函数的打印参数里面可以看到，消息是推送到了交换机成功了，但是在路由分发给队列的时候，找不到队列，所以报了错误 NO_ROUTE 。 结论：②这种情况触发的是 ConfirmCallback和RetrunCallback两个回调函数。 ③消息推送到sever，交换机和队列啥都没找到这种情况其实一看就觉得跟①很像，没错 ，③和①情况回调是一致的，所以不做结果说明了。 结论： ③这种情况触发的是 ConfirmCallback 回调函数。 ④消息推送成功那么测试下，按照正常调用之前消息推送的接口就行，就调用下 &#x2F;sendFanoutMessage接口，可以看到控制台输出： 123ConfirmCallback: 相关数据：nullConfirmCallback: 确认情况：trueConfirmCallback: 原因：null 结论： ④这种情况触发的是 ConfirmCallback 回调函数。 7.消费者确认1）基本概念和生产者的消息确认机制不同，因为消息接收本来就是在监听消息，符合条件的消息就会消费下来。所以，消息接收的确认机制主要存在三种模式： 1）NONEAcknowledgeMode.NONE 自动确认， 这也是默认的消息确认情况。 RabbitMQ成功将消息发出（即将消息成功写入TCP Socket）中立即认为本次投递已经被正确处理，不管消费者端是否成功处理本次投递。 所以这种情况如果消费端消费逻辑抛出异常，也就是消费端没有处理成功这条消息，那么就相当于丢失了消息。 一般这种情况我们都是使用try catch捕捉异常后，打印日志用于追踪数据，这样找出对应数据再做后续处理。 2）AUTOAcknowledgeMode.AUTO 根据异常情况确认，这个实现非常麻烦，不做介绍 3）MANUAL①基本概念手动确认 ， 这个比较关键，也是我们配置接收消息确认机制时，多数选择的模式。消费者收到消息后，手动调用basic.ack&#x2F;basic.nack&#x2F;basic.reject后，RabbitMQ收到这些消息后，才认为本次投递成功。 basic.ack用于肯定确认 basic.nack用于否定确认（注意：这是AMQP 0-9-1的RabbitMQ扩展） basic.reject用于否定确认，但与basic.nack相比有一个限制：一次只能拒绝单条消息 消费者端以上的3个方法都表示消息已经被正确投递，但是basic.ack表示消息已经被正确处理，而basic.nack,basic.reject表示没有被正确处理： reject: 着重讲下reject，因为有时候一些场景是需要重新入列的。 channel.basicReject(deliveryTag, true); 第一个参数依然是当前消息到的数据的唯一id; 拒绝消费当前消息，如果第二参数传入true，就是将数据重新丢回队列里，那么下次还会消费这消息。设置false，就是告诉服务器，我已经知道这条消息数据了，因为一些原因拒绝它，而且服务器也把这个消息丢掉就行。 下次不想再消费这条消息了。 使用拒绝后重新入列这个确认模式要谨慎，因为一般都是出现异常的时候，catch异常再拒绝入列，选择是否重入列。 但是如果使用不当会导致一些每次都被你重入列的消息一直消费-入列-消费-入列这样循环，会导致消息积压。 Nack: 顺便也简单讲讲 nack，这个也是相当于设置不消费某条消息。 channel.basicNack(deliveryTag, false, true); 第一个参数依然是当前消息到的数据的唯一id; 第二个参数是指是否针对多条消息；如果是true，也就是说一次性针对当前通道的消息的tagID小于当前这条消息的，都拒绝确认。 第三个参数是指是否重新入列，也就是指不确认的消息是否重新丢回到队列里面去。 同样使用不确认后重新入列这个确认模式要谨慎，因为这里也可能因为考虑不周出现消息一直被重新丢回去的情况，导致积压。 ②单个队列MessageListenerConfig： 新建MessageListenerConfig.java上添加代码相关的配置代码： 1234567891011121314151617181920212223242526272829@Configurationpublic class MessageListenerConfig &#123; @Autowired private CachingConnectionFactory connectionFactory; @Autowired private MyAckReceiver myAckReceiver;//消息接收处理类 @Bean public SimpleMessageListenerContainer simpleMessageListenerContainer() &#123; SimpleMessageListenerContainer container = new SimpleMessageListenerContainer(connectionFactory); container.setConcurrentConsumers(1); container.setMaxConcurrentConsumers(1); // RabbitMQ默认是自动确认，这里改为手动确认消息 container.setAcknowledgeMode(AcknowledgeMode.MANUAL); //设置一个队列 container.setQueueNames(&quot;TestDirectQueue&quot;); //如果同时设置多个如下： 前提是队列都是必须已经创建存在的 // container.setQueueNames(&quot;TestDirectQueue&quot;,&quot;TestDirectQueue2&quot;,&quot;TestDirectQueue3&quot;); //另一种设置队列的方法,如果使用这种情况,那么要设置多个,就使用addQueues //container.setQueues(new Queue(&quot;TestDirectQueue&quot;,true)); //container.addQueues(new Queue(&quot;TestDirectQueue2&quot;,true)); //container.addQueues(new Queue(&quot;TestDirectQueue3&quot;,true)); container.setMessageListener(myAckReceiver); return container; &#125;&#125; 建立了一个监听器容器，然后手动配置监听器处理器 在容器中设置签收的模式：AcknowledgeMode.MANUAL等 对应的手动确认消息监听类，MyAckReceiver.java（手动确认模式需要实现 ChannelAwareMessageListener）： MyAckReceiver： 1234567891011121314151617181920212223242526272829303132333435363738394041@Componentpublic class MyAckReceiver implements ChannelAwareMessageListener &#123;@Overridepublic void onMessage(Message message, Channel channel) throws Exception &#123; //获取deliveryTag long deliveryTag = message.getMessageProperties().getDeliveryTag(); try &#123; //因为传递消息的时候用的map传递,所以将Map从Message内取出需要做些处理 String msg = message.toString(); String[] msgArray = msg.split(&quot;&#x27;&quot;);//可以点进Message里面看源码,单引号之间的数据就是我们的map消息数据 Map&lt;String, String&gt; msgMap = mapStringToMap(msgArray[1].trim(),3);//取出单引号之间的数据 String messageId=msgMap.get(&quot;messageId&quot;); String messageData=msgMap.get(&quot;messageData&quot;); String createTime=msgMap.get(&quot;createTime&quot;); System.out.println(&quot; MyAckReceiver messageId:&quot;+messageId+&quot; messageData:&quot;+messageData+&quot; createTime:&quot;+createTime); System.out.println(&quot;消费的主题消息来自：&quot;+message.getMessageProperties().getConsumerQueue()); //第二个参数，手动确认可以被批处理，当该参数为 true 时，则可以一次性确认 delivery_tag 小于等于传入值的所有消息 channel.basicAck(deliveryTag, true); //第二个参数，true会重新放回队列，所以需要自己根据业务逻辑判断什么时候使用拒绝 //channel.basicReject(deliveryTag, true); &#125; catch (Exception e) &#123; //第二个参数，true会重新放回队列，所以需要自己根据业务逻辑判断什么时候使用拒绝 channel.basicReject(deliveryTag, false); e.printStackTrace(); &#125; &#125; //&#123;key=value,key=value,key=value&#125; 格式转换成map private Map&lt;String, String&gt; mapStringToMap(String str,int entryNum ) &#123; str = str.substring(1, str.length() - 1); String[] strs = str.split(&quot;,&quot;,entryNum); Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); for (String string : strs) &#123; String key = string.split(&quot;=&quot;)[0].trim(); String value = string.split(&quot;=&quot;)[1]; map.put(key, value); &#125; return map; &#125;&#125; implement了ChannelAwareMessageListener，并重写了onMessage方法 没有出现异常channel.basicAck(deliveryTag, true);签收 catch异常channel.basicReject(deliveryTag, false);拒收并不放入队列中 如果错误是int i=1/0，并且在catch中channel.basicReject(deliveryTag, true);，那么该msg就会重复 放入队列—&gt;消费报错—&gt;签收失败—&gt;再次放入队列 这样就会导致导致消息积压 ③多个队列场景： 除了直连交换机的队列TestDirectQueue需要变成手动确认以外，我们还需要将一个其他的队列 或者多个队列也变成手动确认，而且不同队列实现不同的业务处理。 MessageListenerConfig： 1234567891011121314151617181920212223242526272829@Configurationpublic class MessageListenerConfig &#123; @Autowired private CachingConnectionFactory connectionFactory; @Autowired private MyAckReceiver myAckReceiver;//消息接收处理类 @Bean public SimpleMessageListenerContainer simpleMessageListenerContainer() &#123; SimpleMessageListenerContainer container = new SimpleMessageListenerContainer(connectionFactory); container.setConcurrentConsumers(1); container.setMaxConcurrentConsumers(1); // RabbitMQ默认是自动确认，这里改为手动确认消息 container.setAcknowledgeMode(AcknowledgeMode.MANUAL); //设置一个队列 container.setQueueNames(&quot;TestDirectQueue&quot;，&quot;fanout.A&quot;); //如果同时设置多个如下： 前提是队列都是必须已经创建存在的 // container.setQueueNames(&quot;TestDirectQueue&quot;,&quot;TestDirectQueue2&quot;,&quot;TestDirectQueue3&quot;); //另一种设置队列的方法,如果使用这种情况,那么要设置多个,就使用addQueues //container.setQueues(new Queue(&quot;TestDirectQueue&quot;,true)); //container.addQueues(new Queue(&quot;TestDirectQueue2&quot;,true)); //container.addQueues(new Queue(&quot;TestDirectQueue3&quot;,true)); container.setMessageListener(myAckReceiver); return container; &#125;&#125; MyAckReceiver： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051@Componentpublic class MyAckReceiver implements ChannelAwareMessageListener &#123; @Override public void onMessage(Message message, Channel channel) throws Exception &#123; long deliveryTag = message.getMessageProperties().getDeliveryTag(); try &#123; //因为传递消息的时候用的map传递,所以将Map从Message内取出需要做些处理 String msg = message.toString(); String[] msgArray = msg.split(&quot;&#x27;&quot;);//可以点进Message里面看源码,单引号直接的数据就是我们的map消息数据 Map&lt;String, String&gt; msgMap = mapStringToMap(msgArray[1].trim(),3); String messageId=msgMap.get(&quot;messageId&quot;); String messageData=msgMap.get(&quot;messageData&quot;); String createTime=msgMap.get(&quot;createTime&quot;); //1.根据队列的名字来做不同点业务处理 if (&quot;TestDirectQueue&quot;.equals(message.getMessageProperties().getConsumerQueue()))&#123; System.out.println(&quot;消费的消息来自的队列名为：&quot;+message.getMessageProperties().getConsumerQueue()); System.out.println(&quot;消息成功消费到 messageId:&quot;+messageId+&quot; messageData:&quot;+messageData+&quot; createTime:&quot;+createTime); System.out.println(&quot;执行TestDirectQueue中的消息的业务处理流程......&quot;); &#125; //2.根据队列的名字来做不同点业务处理 if (&quot;fanout.A&quot;.equals(message.getMessageProperties().getConsumerQueue()))&#123; System.out.println(&quot;消费的消息来自的队列名为：&quot;+message.getMessageProperties().getConsumerQueue()); System.out.println(&quot;消息成功消费到 messageId:&quot;+messageId+&quot; messageData:&quot;+messageData+&quot; createTime:&quot;+createTime); System.out.println(&quot;执行fanout.A中的消息的业务处理流程......&quot;); &#125; channel.basicAck(deliveryTag, true); // channel.basicReject(deliveryTag, true);//为true会重新放回队列 &#125; catch (Exception e) &#123; channel.basicReject(deliveryTag, false); e.printStackTrace(); &#125; &#125; //&#123;key=value,key=value,key=value&#125; 格式转换成map private Map&lt;String, String&gt; mapStringToMap(String str,int enNum) &#123; str = str.substring(1, str.length() - 1); String[] strs = str.split(&quot;,&quot;,enNum); Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); for (String string : strs) &#123; String key = string.split(&quot;=&quot;)[0].trim(); String value = string.split(&quot;=&quot;)[1]; map.put(key, value); &#125; return map; &#125;&#125; 9.消息可靠性 持久化 exchange要持人化 queue要持久化 message要持久化 生产方确认Confirm 消费方确认Ack Broker高可用 https://www.cnblogs.com/linjiqin/p/12683076.html 10.限流与并发1）情景 A系统每秒最多能接受1000个请求，这时候来了5000个请求，可以先存入MQ，然后每秒从MQ拉取1000个请求 2）yml配置RabbitMQ消费端配置： 12345678910111213spring: rabbitmq: host: localhost port: 5672 username: guest password: guest listener: simple: acknowledge-mode: manual # 手动确定（默认自动确认） concurrency: 1 # 消费端的监听个数(即@RabbitListener开启几个线程去处理数据。) max-concurrency: 10 # 消费端的监听最大个数 prefetch: 10 #最多处理的消息数 connection-timeout: 15000 # 超时时间 编写消息监听器MyListener： 12345678910111213141516@Componentpublic class MyListener &#123; @RabbitListener(queues = &quot;item_queue&quot;) public void printMsg(String msg, Channel channel, Message message) throws IOException &#123; //设置5秒的睡眠好看变化情况 try &#123; Thread.sleep(5000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;消息处理成功：&quot; + msg); // 手工ack channel.basicAck(message.getMessageProperties().getDeliveryTag(),false); &#125;&#125; yml中配置后，可以直接通过@RabbitListener方式来使用listener 3）配置listener容器1234567891011121314151617181920212223242526public class MessageListenerConfig &#123; @Resource private CachingConnectionFactory cachingConnectionFactory; @Resource private MyAckReceiver myAckReceiver; @Bean public SimpleMessageListenerContainer simpleMessageListenerContainer() &#123; SimpleMessageListenerContainer container = new SimpleMessageListenerContainer(cachingConnectionFactory); //设置最小并发的消费者数量 container.setConcurrentConsumers(10); //设置最大并发的消费者数量 container.setMaxConcurrentConsumers(20); //限流，单位时间内消费多少条记录 container.setPrefetchCount(10); //设置rabbit 确认消息的模式，默认是自动确认 container.setAcknowledgeMode(AcknowledgeMode.MANUAL); //设置一个队列 //container.setQueueNames(&quot;testDirectQueue&quot;); //还可以设置多个队列 container.setQueueNames(&quot;testDirectQueue&quot;, &quot;fanout.A&quot;); //设置监听器 container.setMessageListener(myAckReceiver); return container; &#125; &#125; 通过设置simpleMessageListenerContainer来配置了限流与并发，同时set了myAckReceiver（自己实现） 设置最小并发的消费者数量 container.setConcurrentConsumers(10); 设置最大并发的消费者数量 container.setMaxConcurrentConsumers(20); 限流，单位时间内消费多少条记录 container.setPrefetchCount(10); MyAckReceiver： 1234567891011121314151617181920212223242526272829303132333435363738394041@Componentpublic class MyAckReceiver implements ChannelAwareMessageListener &#123;@Overridepublic void onMessage(Message message, Channel channel) throws Exception &#123; //获取deliveryTag long deliveryTag = message.getMessageProperties().getDeliveryTag(); try &#123; //因为传递消息的时候用的map传递,所以将Map从Message内取出需要做些处理 String msg = message.toString(); String[] msgArray = msg.split(&quot;&#x27;&quot;);//可以点进Message里面看源码,单引号之间的数据就是我们的map消息数据 Map&lt;String, String&gt; msgMap = mapStringToMap(msgArray[1].trim(),3);//取出单引号之间的数据 String messageId=msgMap.get(&quot;messageId&quot;); String messageData=msgMap.get(&quot;messageData&quot;); String createTime=msgMap.get(&quot;createTime&quot;); System.out.println(&quot; MyAckReceiver messageId:&quot;+messageId+&quot; messageData:&quot;+messageData+&quot; createTime:&quot;+createTime); System.out.println(&quot;消费的主题消息来自：&quot;+message.getMessageProperties().getConsumerQueue()); //第二个参数，手动确认可以被批处理，当该参数为 true 时，则可以一次性确认 delivery_tag 小于等于传入值的所有消息 channel.basicAck(deliveryTag, true); //第二个参数，true会重新放回队列，所以需要自己根据业务逻辑判断什么时候使用拒绝 //channel.basicReject(deliveryTag, true); &#125; catch (Exception e) &#123; //第二个参数，true会重新放回队列，所以需要自己根据业务逻辑判断什么时候使用拒绝 channel.basicReject(deliveryTag, false); e.printStackTrace(); &#125; &#125; //&#123;key=value,key=value,key=value&#125; 格式转换成map private Map&lt;String, String&gt; mapStringToMap(String str,int entryNum ) &#123; str = str.substring(1, str.length() - 1); String[] strs = str.split(&quot;,&quot;,entryNum); Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); for (String string : strs) &#123; String key = string.split(&quot;=&quot;)[0].trim(); String value = string.split(&quot;=&quot;)[1]; map.put(key, value); &#125; return map; &#125;&#125; implement了ChannelAwareMessageListener，并重写了onMessage方法 4）concurrency和prefetch origin：https://cloud.tencent.com/developer/article/1956259 prefetch：消费端一次消费的最大数量 消费端必须是手动确认 比如prefetch&#x3D;10，那么消费端如果不确认，就无法消费接下来的11~20条 上面配置中，concurrency =1，即每个Listener容器将开启一个线程去处理消息 如果在Listener配置了exclusive参数，即确定此容器中的单个customer是否具有对队列的独占访问权限。如果为true，则容器的并发性必须为1。 由于是线程独占，所以并发数不能&gt;1 11.延迟队列 https://juejin.cn/post/6844903512657100807#heading-18 通过ttl+死信队列的方式实现延迟队列 如ttl队列30分钟过期，所有消息就会进入死信队列 消费者监听死信队列，30分钟后就能消费对应的数据 12.日志与监控1）日志 RabbitMQ默认日志存放路径： &#x2F;var&#x2F;log&#x2F;rabbitmq&#x2F;&#114;&#x61;&#98;&#98;&#105;&#116;&#64;&#120;&#120;&#x78;&#46;&#108;&#x6f;&#103; xxx代表 日志包含了RabbitMQ的版本号、Erlang的版本号、RabbitMQ服务节点名称、cookie的hash值、RabbitMQ配置文件地址、内存限制、磁盘限制、默认账户guest的创建以及权限配置等等。 2）web管控台监控web consle中可以实时监控MQ的信息 3）rabbitmqctl管理和监控123456789查看队列 # rabbitmqctl list_queues查看exchanges # rabbitmqctl list_exchanges查看用户 # rabbitmqctl list_users查看连接 # rabbitmqctl list_connections查看消费者信息 # rabbitmqctl list_consumers查看环境变量 # rabbitmqctl environment查看未被确认的队列 # rabbitmqctl list_queues name messages_unacknowledged查看单个队列的内存使用 # rabbitmqctl list_queues name memory查看准备就绪的队列 # rabbitmqctl list_queues name messages_ready rabbitmqctl help 可以看到相关信息 实际上没有web consle直观全面 13.消息追踪1）基本概念在使用任何消息中间件的过程中，难免会出现某条消息异常丢失的情况。 对于RabbitMQ而言： 可能是因为生产者或消费者与RabbitMQ断开了连接，而它们与RabbitMQ又采用了不同的确认机制； 可能是因为交换器与队列之间不同的转发策略； 甚至是交换器并没有与任何队列进行绑定，生产者又不感知或者没有采取相应的措施； 另外RabbitMQ本身的集群策略也可能导致消息的丢失。 这个时候就需要有一个较好的机制跟踪记录消息的投递过程，以此协助开发和运维人员进行问题的定位。在RabbitMQ中可以使用Firehose和rabbitmq_tracing插件功能来实现消息追踪。 2）Firehose​ firehose的机制是将生产者投递给rabbitmq的消息，rabbitmq投递给消费者的消息按照指定的格式发送到默认的exchange上。这个默认的exchange的名称为amq.rabbitmq.trace，它是一个topic类 型的exchange。发送到这个exchange上的消息的routing key为 publish.exchangename 和 deliver.queuename。其中exchangename和queuename为实际exchange和queue的名称，分别 对应生产者投递到exchange的消息，和消费者从queue上获取的消息。注意：打开 trace 会影响消息写入功能，适当打开后请关闭。 rabbitmqctl trace_on：开启Firehose命令 rabbitmqctl trace_off：关闭Firehose命令 3）rabbitmq_tracing​ rabbitmq_tracing和Firehose在实现上如出一辙，只不过rabbitmq_tracing的方式比Firehose多了一 层GUI的包装，更容易使用和管理。需要启用插件：rabbitmq-plugins enable rabbitmq_tracing ​ 消息中心的消息追踪需要使用Trace实现，Trace是Rabbitmq用于记录每一次发送的消息，方便使用Rabbitmq的开发者调试、排错。可通过插件形式提供可视化界面（也是在web consle）。Trace启动后会自动创建系统Exchange：amq.rabbitmq.trace ,每个队列会自动绑定该Exchange，绑定后发送到队列的消息都会记录到Trace日志。 以下是trace的相关命令和使用（要使用需要先rabbitmq启用插件，再打开开关才能使用）： 命令集 描述 rabbitmq-plugins list 查看插件列表 rabbitmq-plugins enable rabbitmq_tracing rabbitmq启用trace插件 rabbitmqctl trace_on 打开trace的开关 rabbitmqctl trace_on -p itcast 打开trace的开关(itcast为需要日志追踪的vhost) rabbitmqctl trace_off 关闭trace的开关 rabbitmq-plugins disable rabbitmq_tracing rabbitmq关闭Trace插件 rabbitmqctl set_user_tags heima administrator 只有administrator的角色才能查看日志界面 安装插件并开启 trace_on 之后，会发现多个 exchange：amq.rabbitmq.trace ，类型为：topic。 14.消息补偿 以订单服务和配送服务为例： 下单成功，并且将订单相关信息入库DB，用户下单操作就结束了 下单服务发送订单ID和商品等相关信息到Q1 同时还需要再延迟发送一条相同的消息到Q3 consumer配送服务监听Q1，接收到该消息后进行下单操作并入库DB 配送服务成功消费以后发送确认消息，其实就是再发一条相同的消息到Q2 回调检查服务监听Q2的消息 回调检查服务将从Q2获得的消息写到MDB中等待比对 回调检查服务还会监听Q3中的延迟消息，并将该消息和MDB中Q2传来的消息进行比对 MDB中没有，那么说明配送服务肯定消费失败了，并没有正确的进行配送 回调检查服务就会调用订单服务重新在发送该消息，再经历一次上述过程 MDB中有，那么说明配送服务成功消费了 还有一种极端情况2和3中正常消息和延迟消息都发送失败了 这种情况会单开一个服务，定时比对DB和MDB中的数据 DB&#x3D;&#x3D;MDB，说明消息都被正确消费了 DB&gt;MDB，说明有消息没有正确消费，再次调用订单服务发送消息 15.消息幂等性保障1）基本概念 幂等性指一次和多次请求某一个资源，对于资源本身应该具有同样的结果。也就是说，其任意多次执行对资源本身所产生的影响均与一次执行的影响相同。 在MQ中指，消费多条相同的消息，得到与消费该消息一次相同的结果 例子： 如扣款操作，因为某种因数导致发送了两条扣款消息（balance-500），尽管消息发送了两条，但是最终结果仍然需要保证只会扣一次款-500 2）解决方案①cas解决 发送的消息中带了version&#x3D;1，在第一条消息被消费检查数据库，数据库中的version&#x3D;1，修改成功，修改完以后则则让version+1&#x3D;2，第二条消息要修改的时候version&#x3D;2已经不是1了，那么就会修改失败。 数据库中的UPDATE 表名 SET 字段名1 = 值1, 字段名2 = 值2, ... where id=xxx and version=1这条指令数据库是可以保证原子性的 16.集群搭建1）基本原理RabbitMQ这款消息队列中间件产品本身是基于Erlang编写，Erlang语言天生具备分布式特性（通过同步Erlang集群各节点的magic cookie来实现）。因此，RabbitMQ天然支持Clustering。这使得RabbitMQ本身不需要像ActiveMQ、Kafka那样通过ZooKeeper分别来实现HA方案和保存集群的元数据。集群是保证可靠性的一种方式，同时可以通过水平扩展以达到增加消息吞吐量能力的目的。 2）单机多实例①首先确保RabbitMQ运行没有问题rabbitmqctl status 12[root@super ~]# rabbitmqctl statusStatus of node rabbit@super ... ②停止rabbitmq服务 123[root@super sbin]# service rabbitmq-server stopStopping rabbitmq-server: rabbitmq-server. ③启动第一个节点 12345678910[root@super sbin]# RABBITMQ_NODE_PORT=5673 RABBITMQ_NODENAME=rabbit1 rabbitmq-server start RabbitMQ 3.6.5. Copyright (C) 2007-2016 Pivotal Software, Inc. ## ## Licensed under the MPL. See http://www.rabbitmq.com/ ## ## ########## Logs: /var/log/rabbitmq/rabbit1.log ###### ## /var/log/rabbitmq/rabbit1-sasl.log ########## Starting broker... completed with 6 plugins. port&#x3D;5673，web port&#x3D;15672 ④启动第二个节点 web管理插件端口占用,所以还要指定其web插件占用的端口号。 1234567891011[root@super ~]# RABBITMQ_NODE_PORT=5674 RABBITMQ_SERVER_START_ARGS=&quot;-rabbitmq_management listener [&#123;port,15674&#125;]&quot; RABBITMQ_NODENAME=rabbit2 rabbitmq-server start RabbitMQ 3.6.5. Copyright (C) 2007-2016 Pivotal Software, Inc. ## ## Licensed under the MPL. See http://www.rabbitmq.com/ ## ## ########## Logs: /var/log/rabbitmq/rabbit2.log ###### ## /var/log/rabbitmq/rabbit2-sasl.log ########## Starting broker... completed with 6 plugins. port&#x3D;5674，web port&#x3D;15674 结束命令： 12rabbitmqctl -n rabbit1 stoprabbitmqctl -n rabbit2 stop ⑤rabbit1操作作为主节点： 1234567[root@super ~]# rabbitmqctl -n rabbit1 stop_app Stopping node rabbit1@super ...[root@super ~]# rabbitmqctl -n rabbit1 reset Resetting node rabbit1@super ...[root@super ~]# rabbitmqctl -n rabbit1 start_appStarting node rabbit1@super ...[root@super ~]# 在启动集群前需要先停掉服务，reset以下数据，然后再重启 ⑥rabbit2操作为从节点： 123456789[root@super ~]# rabbitmqctl -n rabbit2 stop_appStopping node rabbit2@super ...[root@super ~]# rabbitmqctl -n rabbit2 resetResetting node rabbit2@super ...[root@super ~]# rabbitmqctl -n rabbit2 join_cluster rabbit1@&#x27;xxx&#x27; &#x27;xxx&#x27;内是主机名换成自己的Clustering node rabbit2@super with rabbit1@super ...[root@super ~]# rabbitmqctl -n rabbit2 start_appStarting node rabbit2@super ... 作为从节点加入了某主节点 ⑦查看集群状态： 1234567[root@super ~]# rabbitmqctl cluster_status -n rabbit1Cluster status of node rabbit1@super ...[&#123;nodes,[&#123;disc,[rabbit1@super,rabbit2@super]&#125;]&#125;, &#123;running_nodes,[rabbit2@super,rabbit1@super]&#125;, &#123;cluster_name,&lt;&lt;&quot;rabbit1@super&quot;&gt;&gt;&#125;, &#123;partitions,[]&#125;, &#123;alarms,[&#123;rabbit2@super,[]&#125;,&#123;rabbit1@super,[]&#125;]&#125;] ⑧可以通过web consle监控集群 3）集群相关指令rabbitmqctl join_cluster {cluster_node} [–ram]将节点加入指定集群中。在这个命令执行前需要停止RabbitMQ应用并重置节点。 rabbitmqctl cluster_status显示集群的状态。 rabbitmqctl change_cluster_node_type {disc|ram}修改集群节点的类型。在这个命令执行前需要停止RabbitMQ应用。 rabbitmqctl forget_cluster_node [–offline]将节点从集群中删除，允许离线执行。 rabbitmqctl update_cluster_nodes {clusternode} 在集群中的节点应用启动前咨询clusternode节点的最新信息，并更新相应的集群信息。这个和join_cluster不同，它不加入集群。考虑这样一种情况，节点A和节点B都在集群中，当节点A离线了，节点C又和节点B组成了一个集群，然后节点B又离开了集群，当A醒来的时候，它会尝试联系节点B，但是这样会失败，因为节点B已经不在集群中了。 rabbitmqctl cancel_sync_queue [-p vhost] {queue}取消队列queue同步镜像的操作。 rabbitmqctl set_cluster_name {name}设置集群名称。集群名称在客户端连接时会通报给客户端。Federation和Shovel插件也会有用到集群名称的地方。集群名称默认是集群中第一个节点的名称，通过这个命令可以重新设置。 4）镜像集群①基本概念 上面已经完成RabbitMQ默认集群模式，但并不保证队列的高可用性，尽管交换机、绑定这些可以复制到集群里的任何一个节点，但是队列内容不会复制。虽然该模式解决一项目组节点压力，但队列节点宕机直接导致该队列无法应用，只能等待重启，所以要想在队列节点宕机或故障也能正常应用，就要复制队列内容到集群里的每个节点，必须要创建镜像队列。 镜像队列是基于普通的集群模式的，然后再添加一些策略，所以你还是得先配置普通集群，然后才能设置镜像队列，我们就以上面的集群接着做。 设置的镜像队列可以通过开启的网页的管理端Admin-&gt;Policies，也可以通过命令。 配置完以后，如果rabbit1宕机，rabbit2中仍然还有数据 ②web consle配置 https://www.bilibili.com/video/BV15k4y1k7Ep?p=36&amp;vd_source=65b93dcb67042b5352bb42ae6bcb0ac1 Name:策略名称 Pattern：匹配的规则，如果是匹配所有的队列或交换机，是^ 若匹配a开头的队列或交换机就是，a^ Definition:使用ha-mode模式中的all，也就是同步所有匹配的队列。问号链接帮助文档。 ③命令配置rabbitmqctl set_policy my_ha &quot;^&quot; &#39;&#123;&quot;ha-mode&quot;:&quot;all&quot;&#125;&#39; 命令也包含了Name，Pattern，Definition，其实和web consle的设置是一样的 5）负载均衡-HAProxyHAProxy提供高可用性、负载均衡以及基于TCP和HTTP应用的代理，支持虚拟主机，它是免费、快速并且可靠的一种解决方案,包括Twitter，Reddit，StackOverflow，GitHub在内的多家知名互联网公司在使用。HAProxy实现了一种事件驱动、单一进程模型，此模型支持非常大的并发连接数。 同时，考虑以下一个问题，我们创建了这么多节点，那么我们在java代码中应该写哪一个呢？如果写的节点宕机了，那我们又该怎么办呢？为此，我们可以使用HAProxy进行统一代理，将所有node都通过一个ip进行映射 ①安装HAProxy123456789101112131415//下载依赖包yum install gcc vim wget//上传haproxy源码包//解压tar -zxvf haproxy-1.6.5.tar.gz -C /usr/local//进入目录、进行编译、安装cd /usr/local/haproxy-1.6.5make TARGET=linux31 PREFIX=/usr/local/haproxymake install PREFIX=/usr/local/haproxymkdir /etc/haproxy//赋权groupadd -r -g 149 haproxyuseradd -g haproxy -r -s /sbin/nologin -u 149 haproxy//创建haproxy配置文件vim /etc/haproxy/haproxy.cfg ②配置HAProxy配置文件路径：&#x2F;etc&#x2F;haproxy&#x2F;haproxy.cfg 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#logging optionsglobal log 127.0.0.1 local0 info maxconn 5120 chroot /usr/local/haproxy uid 99 gid 99 daemon quiet nbproc 20 pidfile /var/run/haproxy.piddefaults log global mode tcp option tcplog option dontlognull retries 3 option redispatch maxconn 2000 contimeout 5s clitimeout 60s srvtimeout 15s #front-end IP for consumers and producterslisten rabbitmq_cluster bind 0.0.0.0:5672 #HAProxy提供的统一的访问端口 mode tcp #balance url_param userid #balance url_param session_id check_post 64 #balance hdr(User-Agent) #balance hdr(host) #balance hdr(Host) use_domain_only #balance rdp-cookie #balance leastconn #balance source //ip balance roundrobin # 配置rabbitMQ节点，node1可以自己取名字 server node1 127.0.0.1:5673 check inter 5000 rise 2 fall 2 server node2 127.0.0.1:5674 check inter 5000 rise 2 fall 2listen stats bind 172.16.98.133:8100 #此处+下方的/rabbitmq-stats，可以在web端查看代理情况 mode http option httplog stats enable stats uri /rabbitmq-stats stats refresh 5s ③启动HAproxy负载123456/usr/local/haproxy/sbin/haproxy -f /etc/haproxy/haproxy.cfg//查看haproxy进程状态ps -ef | grep haproxy访问如下地址对mq节点进行监控http://172.16.98.133:8100/rabbitmq-stats 代码中访问mq集群地址，则变为访问haproxy地址:5672 我们可以通过统一的端口5672来访问rabbitMQ集群 集群中某主机宕机了，也不影响消息的传递","categories":[{"name":"MQ","slug":"MQ","permalink":"http://example.com/categories/MQ/"}],"tags":[{"name":"rabbitMQ","slug":"rabbitMQ","permalink":"http://example.com/tags/rabbitMQ/"}]},{"title":"mysql基本语法","slug":"mysql基本语法","date":"2022-04-04T07:29:48.000Z","updated":"2022-08-18T04:34:55.238Z","comments":true,"path":"2022/04/04/mysql基本语法/","link":"","permalink":"http://example.com/2022/04/04/mysql%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/","excerpt":"DDL、DML、DQL、DCL、函数、约束、多表查询、数据类型、权限表","text":"DDL、DML、DQL、DCL、函数、约束、多表查询、数据类型、权限表 一、通用语法及分类 DDL: 数据定义语言，用来定义数据库对象（数据库、表、字段） DML: 数据操作语言，用来对数据库表中的数据进行增删改 DQL: 数据查询语言，用来查询数据库中表的记录 DCL: 数据控制语言，用来创建数据库用户、控制数据库的控制权限 1.DDL（数据定义语言）数据定义语言 1）数据库操作查询所有数据库：SHOW DATABASES;查询当前数据库：SELECT DATABASE();创建数据库：CREATE DATABASE [ IF NOT EXISTS ] 数据库名 [ DEFAULT CHARSET 字符集] [COLLATE 排序规则 ];删除数据库：DROP DATABASE [ IF EXISTS ] 数据库名;使用数据库：USE 数据库名; 注意事项 UTF8字符集长度为3字节，有些符号占4字节，所以推荐用utf8mb4字符集 2）表操作查询当前数据库所有表：SHOW TABLES;查询表结构：DESC 表名;查询指定表的建表语句：SHOW CREATE TABLE 表名; 创建表： 1234567CREATE TABLE 表名( 字段1 字段1类型 [COMMENT 字段1注释], 字段2 字段2类型 [COMMENT 字段2注释], 字段3 字段3类型 [COMMENT 字段3注释], ... 字段n 字段n类型 [COMMENT 字段n注释])[ COMMENT 表注释 ]; 最后一个字段后面没有逗号 添加字段：ALTER TABLE 表名 ADD 字段名 类型(长度) [COMMENT 注释] [约束];例：ALTER TABLE emp ADD nickname varchar(20) COMMENT &#39;昵称&#39;; 修改数据类型：ALTER TABLE 表名 MODIFY 字段名 新数据类型(长度);修改字段名和字段类型：ALTER TABLE 表名 CHANGE 旧字段名 新字段名 类型(长度) [COMMENT 注释] [约束];例：将emp表的nickname字段修改为username，类型为varchar(30)ALTER TABLE emp CHANGE nickname username varchar(30) COMMENT &#39;昵称&#39;; 删除字段：ALTER TABLE 表名 DROP 字段名; 修改表名：ALTER TABLE 表名 RENAME TO 新表名 删除表：DROP TABLE [IF EXISTS] 表名;删除表，并重新创建该表：TRUNCATE TABLE 表名; 2.DML（数据操作语言）1）添加数据指定字段：INSERT INTO 表名 (字段名1, 字段名2, ...) VALUES (值1, 值2, ...);全部字段：INSERT INTO 表名 VALUES (值1, 值2, ...); 批量添加数据：INSERT INTO 表名 (字段名1, 字段名2, ...) VALUES (值1, 值2, ...), (值1, 值2, ...), (值1, 值2, ...);INSERT INTO 表名 VALUES (值1, 值2, ...), (值1, 值2, ...), (值1, 值2, ...); 注意事项 字符串和日期类型数据应该包含在引号中 插入的数据大小应该在字段的规定范围内 2）更新和删除数据修改数据：UPDATE 表名 SET 字段名1 = 值1, 字段名2 = 值2, ... [ WHERE 条件 ];例：UPDATE emp SET name = &#39;Jack&#39; WHERE id = 1; 删除数据：DELETE FROM 表名 [ WHERE 条件 ]; 3.DQL（数据查询语言）语法： 1234567891011121314SELECT 字段列表FROM 表名字段WHERE 条件列表GROUP BY 分组字段列表HAVING 分组后的条件列表ORDER BY 排序字段列表LIMIT 分页参数 1）基础查询查询多个字段：SELECT 字段1, 字段2, 字段3, ... FROM 表名;SELECT * FROM 表名; 设置别名：SELECT 字段1 [ AS 别名1 ], 字段2 [ AS 别名2 ], 字段3 [ AS 别名3 ], ... FROM 表名;SELECT 字段1 [ 别名1 ], 字段2 [ 别名2 ], 字段3 [ 别名3 ], ... FROM 表名; 去除重复记录：SELECT DISTINCT 字段列表 FROM 表名; 转义：SELECT * FROM 表名 WHERE name LIKE &#39;/_张三&#39; ESCAPE &#39;/&#39;&#x2F; 之后的_不作为通配符 2）条件查询语法：SELECT 字段列表 FROM 表名 WHERE 条件列表; 条件： 比较运算符 功能 &gt; 大于 &gt;&#x3D; 大于等于 &lt; 小于 &lt;&#x3D; 小于等于 &#x3D; 等于 &lt;&gt; 或 !&#x3D; 不等于 BETWEEN … AND … 在某个范围内（含最小、最大值） IN(…) 在in之后的列表中的值，多选一 LIKE 占位符 模糊匹配（_匹配单个字符，%匹配任意个字符） IS NULL 是NULL 逻辑运算符 功能 AND 或 &amp;&amp; 并且（多个条件同时成立） OR 或 &amp;#124;&amp;#124; 或者（多个条件任意一个成立） NOT 或 ! 非，不是 例子： 123456789101112131415161718192021222324252627-- 年龄等于30select * from employee where age = 30;-- 年龄小于30select * from employee where age &lt; 30;-- 小于等于select * from employee where age &lt;= 30;-- 没有身份证select * from employee where idcard is null or idcard = &#x27;&#x27;;-- 有身份证select * from employee where idcard;select * from employee where idcard is not null;-- 不等于select * from employee where age != 30;-- 年龄在20到30之间select * from employee where age between 20 and 30;select * from employee where age &gt;= 20 and age &lt;= 30;-- 下面语句不报错，但查不到任何信息select * from employee where age between 30 and 20;-- 性别为女且年龄小于30select * from employee where age &lt; 30 and gender = &#x27;女&#x27;;-- 年龄等于25或30或35select * from employee where age = 25 or age = 30 or age = 35;select * from employee where age in (25, 30, 35);-- 姓名为两个字select * from employee where name like &#x27;__&#x27;;-- 身份证最后为Xselect * from employee where idcard like &#x27;%X&#x27;; 3）聚合查询（聚合函数）常见聚合函数： 函数 功能 count 统计数量 max 最大值 min 最小值 avg 平均值 sum 求和 语法：SELECT 聚合函数(字段列表) FROM 表名;例：SELECT count(id) from employee where workaddress = &quot;广东省&quot;; 4）分组查询语法：SELECT 字段列表 FROM 表名 [ WHERE 条件 ] GROUP BY 分组字段名 [ HAVING 分组后的过滤条件 ]; where 和 having 的区别： 执行时机不同：where是分组之前进行过滤，不满足where条件不参与分组；having是分组后对结果进行过滤。 判断条件不同：where不能对聚合函数进行判断，而having可以。 例子： 12345678910-- 根据性别分组，统计男性和女性数量（只显示分组数量，不显示哪个是男哪个是女）select count(*) from employee group by gender;-- 根据性别分组，统计男性和女性数量select gender, count(*) from employee group by gender;-- 根据性别分组，统计男性和女性的平均年龄select gender, avg(age) from employee group by gender;-- 年龄小于45，并根据工作地址分组select workaddress, count(*) from employee where age &lt; 45 group by workaddress;-- 年龄小于45，并根据工作地址分组，获取员工数量大于等于3的工作地址select workaddress, count(*) address_count from employee where age &lt; 45 group by workaddress having address_count &gt;= 3; 注意事项 执行顺序：where &gt; 聚合函数 &gt; having 分组之后，查询的字段一般为聚合函数和分组字段，查询其他字段无任何意义 5）排序查询语法：SELECT 字段列表 FROM 表名 ORDER BY 字段1 排序方式1, 字段2 排序方式2; 排序方式： ASC: 升序（默认） DESC: 降序 例子： 12345-- 根据年龄升序排序SELECT * FROM employee ORDER BY age ASC;SELECT * FROM employee ORDER BY age;-- 两字段排序，根据年龄升序排序，入职时间降序排序SELECT * FROM employee ORDER BY age ASC, entrydate DESC; 注意事项 如果是多字段排序，当第一个字段值相同时，才会根据第二个字段进行排序 6）分页查询语法：SELECT 字段列表 FROM 表名 LIMIT 起始索引, 查询记录数; 例子： 1234-- 查询第一页数据，展示10条SELECT * FROM employee LIMIT 0, 10;-- 查询第二页SELECT * FROM employee LIMIT 10, 10; 注意事项 起始索引从0开始，起始索引 &#x3D; （查询页码 - 1） * 每页显示记录数 分页查询是数据库的方言，不同数据库有不同实现，MySQL是LIMIT 如果查询的是第一页数据，起始索引可以省略，直接简写 LIMIT 10 7）DQL执行顺序 FROM -&gt; WHERE -&gt; GROUP BY -&gt; SELECT -&gt; ORDER BY -&gt; LIMIT ①先确定查询的是那些表 from ②根据查询条件来进行查询 where ③根据条件查询完的结果进行分组查询，分组查询完以后再通过having条件筛选 group by having ④将结果最终需要展示的列进行筛选 select ⑤将结果排序 order by ⑥最后确认显示数据的条数 limit 4.DCL1）管理用户查询用户： 12USER mysql;SELECT * FROM user; 创建用户:CREATE USER &#39;用户名&#39;@&#39;主机名&#39; IDENTIFIED BY &#39;密码&#39;; 修改用户密码：ALTER USER &#39;用户名&#39;@&#39;主机名&#39; IDENTIFIED WITH mysql_native_password BY &#39;新密码&#39;; 删除用户：DROP USER &#39;用户名&#39;@&#39;主机名&#39;; 例子： 123456789-- 创建用户test，只能在当前主机localhost访问create user &#x27;test&#x27;@&#x27;localhost&#x27; identified by &#x27;123456&#x27;;-- 创建用户test，能在任意主机访问create user &#x27;test&#x27;@&#x27;%&#x27; identified by &#x27;123456&#x27;;create user &#x27;test&#x27; identified by &#x27;123456&#x27;;-- 修改密码alter user &#x27;test&#x27;@&#x27;localhost&#x27; identified with mysql_native_password by &#x27;1234&#x27;;-- 删除用户drop user &#x27;test&#x27;@&#x27;localhost&#x27;; 注意事项 主机名可以使用 % 通配 2）权限控制常用权限： 权限 说明 ALL, ALL PRIVILEGES 所有权限 SELECT 查询数据 INSERT 插入数据 UPDATE 修改数据 DELETE 删除数据 ALTER 修改表 DROP 删除数据库&#x2F;表&#x2F;视图 CREATE 创建数据库&#x2F;表 更多权限请看权限一览表 查询权限：SHOW GRANTS FOR &#39;用户名&#39;@&#39;主机名&#39;; 授予权限：GRANT 权限列表 ON 数据库名.表名 TO &#39;用户名&#39;@&#39;主机名&#39;; 撤销权限：REVOKE 权限列表 ON 数据库名.表名 FROM &#39;用户名&#39;@&#39;主机名&#39;; 注意事项 多个权限用逗号分隔 授权时，数据库名和表名可以用 * 进行通配，代表所有 5.函数 字符串函数 数值函数 日期函数 流程函数 1）字符串函数常用函数： 函数 功能 CONCAT(s1, s2, …, sn) 字符串拼接，将s1, s2, …, sn拼接成一个字符串 LOWER(str) 将字符串全部转为小写 UPPER(str) 将字符串全部转为大写 LPAD(str, n, pad) 左填充，用字符串pad对str的左边进行填充，达到n个字符串长度 RPAD(str, n, pad) 右填充，用字符串pad对str的右边进行填充，达到n个字符串长度 TRIM(str) 去掉字符串头部和尾部的空格 SUBSTRING(str, start, len) 返回从字符串str从start位置起的len个长度的字符串 使用示例： 1234567891011121314-- 拼接SELECT CONCAT(&#x27;Hello&#x27;, &#x27;World&#x27;);-- 小写SELECT LOWER(&#x27;Hello&#x27;);-- 大写SELECT UPPER(&#x27;Hello&#x27;);-- 左填充SELECT LPAD(&#x27;01&#x27;, 5, &#x27;-&#x27;);-- 右填充SELECT RPAD(&#x27;01&#x27;, 5, &#x27;-&#x27;);-- 去除空格SELECT TRIM(&#x27; Hello World &#x27;);-- 切片（起始索引为1）SELECT SUBSTRING(&#x27;Hello World&#x27;, 1, 5); 2）数值函数常见函数： 函数 功能 CEIL(x) 向上取整 FLOOR(x) 向下取整 MOD(x, y) 返回x&#x2F;y的模 RAND() 返回0~1内的随机数 ROUND(x, y) 求参数x的四舍五入值，保留y位小数 3）日期函数常用函数： 函数 功能 CURDATE() 返回当前日期 CURTIME() 返回当前时间 NOW() 返回当前日期和时间 YEAR(date) 获取指定date的年份 MONTH(date) 获取指定date的月份 DAY(date) 获取指定date的日期 DATE_ADD(date, INTERVAL expr type) 返回一个日期&#x2F;时间值加上一个时间间隔expr后的时间值 DATEDIFF(date1, date2) 返回起始时间date1和结束时间date2之间的天数 例子： 123-- DATE_ADDSELECT DATE_ADD(NOW(), INTERVAL 70 YEAR);-- 这样求出来就是now的日期+时间，往后推70年的结果，往前推就是-70 4）流程函数常用函数： 函数 功能 IF(value, t, f) 如果value为true，则返回t，否则返回f IFNULL(value1, value2) 如果value1不为空，返回value1，否则返回value2 CASE WHEN [ val1 ] THEN [ res1 ] … ELSE [ default ] END 如果val1为true，返回res1，… 否则返回default默认值 CASE [ expr ] WHEN [ val1 ] THEN [ res1 ] … ELSE [ default ] END 如果expr的值等于val1，返回res1，… 否则返回default默认值 例子： 12345678select name, (case when age &gt; 30 then &#x27;中年&#x27; else &#x27;青年&#x27; end)from employee;select name, (case workaddress when &#x27;北京市&#x27; then &#x27;一线城市&#x27; when &#x27;上海市&#x27; then &#x27;一线城市&#x27; else &#x27;二线城市&#x27; end) as &#x27;工作地址&#x27;from employee; 6.约束分类： 约束 描述 关键字 非空约束 限制该字段的数据不能为null NOT NULL 唯一约束 保证该字段的所有数据都是唯一、不重复的 UNIQUE 主键约束 主键是一行数据的唯一标识，要求非空且唯一 PRIMARY KEY 默认约束 保存数据时，如果未指定该字段的值，则采用默认值 DEFAULT 检查约束（8.0.1版本后） 保证字段值满足某一个条件 CHECK 外键约束 用来让两张图的数据之间建立连接，保证数据的一致性和完整性 FOREIGN KEY 约束是作用于表中字段上的，可以再创建表&#x2F;修改表的时候添加约束。 1）常用约束 约束条件 关键字 主键 PRIMARY KEY 自动增长 AUTO_INCREMENT 不为空 NOT NULL 唯一 UNIQUE 逻辑条件 CHECK 默认值 DEFAULT 例子： 1234567create table user( id int primary key auto_increment, name varchar(10) not null unique, age int check(age &gt; 0 and age &lt; 120), status char(1) default &#x27;1&#x27;, gender char(1)); 2）外键约束添加外键： 123456789CREATE TABLE 表名( 字段名 字段类型, ... [CONSTRAINT] [外键名称] FOREIGN KEY(外键字段名) REFERENCES 主表(主表列名));ALTER TABLE 表名 ADD CONSTRAINT 外键名称 FOREIGN KEY (外键字段名) REFERENCES 主表(主表列名);-- 例子alter table emp add constraint fk_emp_dept_id foreign key(dept_id) references dept(id); 删除外键：ALTER TABLE 表名 DROP FOREIGN KEY 外键名; 删除&#x2F;更新行为 行为 说明 NO ACTION 当在父表中删除&#x2F;更新对应记录时，首先检查该记录是否有对应外键，如果有则不允许删除&#x2F;更新（与RESTRICT一致） RESTRICT 当在父表中删除&#x2F;更新对应记录时，首先检查该记录是否有对应外键，如果有则不允许删除&#x2F;更新（与NO ACTION一致） CASCADE 当在父表中删除&#x2F;更新对应记录时，首先检查该记录是否有对应外键，如果有则也删除&#x2F;更新外键在子表中的记录 SET NULL 当在父表中删除&#x2F;更新对应记录时，首先检查该记录是否有对应外键，如果有则设置子表中该外键值为null（要求该外键允许为null） SET DEFAULT 父表有变更时，子表将外键设为一个默认值（Innodb不支持） 更改删除&#x2F;更新行为：ALTER TABLE 表名 ADD CONSTRAINT 外键名称 FOREIGN KEY (外键字段) REFERENCES 主表名(主表字段名) ON UPDATE 行为 ON DELETE 行为; 7.多表查询1）多表关系 一对多（多对一） 多对多 一对一 一对多 案例：部门与员工关系：一个部门对应多个员工，一个员工对应一个部门实现：在多的一方建立外键，指向一的一方的主键 多对多 案例：学生与课程关系：一个学生可以选多门课程，一门课程也可以供多个学生选修实现：建立第三张中间表，中间表至少包含两个外键，分别关联两方主键 一对一 案例：用户与用户详情关系：一对一关系，多用于单表拆分，将一张表的基础字段放在一张表中，其他详情字段放在另一张表中，以提升操作效率实现：在任意一方加入外键，关联另外一方的主键，并且设置外键为唯一的（UNIQUE） 2）查询合并查询（笛卡尔积，会展示所有组合结果）：select * from employee, dept; 笛卡尔积：两个集合A集合和B集合的所有组合情况（在多表查询时，需要消除无效的笛卡尔积） 消除无效笛卡尔积：select * from employee, dept where employee.dept = dept.id; 3）内连接查询内连接查询的是两张表交集的部分 隐式内连接：SELECT 字段列表 FROM 表1, 表2 WHERE 条件 ...; 显式内连接：SELECT 字段列表 FROM 表1 [ INNER ] JOIN 表2 ON 连接条件 ...; 显式性能比隐式高 例子： 12345-- 查询员工姓名，及关联的部门的名称-- 隐式select e.name, d.name from employee as e, dept as d where e.dept = d.id;-- 显式select e.name, d.name from employee as e inner join dept as d on e.dept = d.id; 4）外连接查询左外连接：查询左表所有数据，以及两张表交集部分数据SELECT 字段列表 FROM 表1 LEFT [ OUTER ] JOIN 表2 ON 条件 ...;相当于查询表1的所有数据，包含表1和表2交集部分数据 右外连接：查询右表所有数据，以及两张表交集部分数据SELECT 字段列表 FROM 表1 RIGHT [ OUTER ] JOIN 表2 ON 条件 ...; 例子： 12345-- 左select e.*, d.name from employee as e left outer join dept as d on e.dept = d.id;select d.name, e.* from dept d left outer join emp e on e.dept = d.id; -- 这条语句与下面的语句效果一样-- 右select d.name, e.* from employee as e right outer join dept as d on e.dept = d.id; 左连接可以查询到没有dept的employee，右连接可以查询到没有employee的dept 5）自连接查询当前表与自身的连接查询，自连接必须使用表别名 语法：SELECT 字段列表 FROM 表A 别名A JOIN 表A 别名B ON 条件 ...; 自连接查询，可以是内连接查询，也可以是外连接查询 例子： 1234-- 查询员工及其所属领导的名字select a.name, b.name from employee a, employee b where a.manager = b.id;-- 没有领导的也查询出来select a.name, b.name from employee a left join employee b on a.manager = b.id; 6）联合查询 union, union all把多次查询的结果合并，形成一个新的查询集 语法： 123SELECT 字段列表 FROM 表A ...UNION [ALL]SELECT 字段列表 FROM 表B ... 注意事项 UNION ALL 会有重复结果，UNION 不会 联合查询比使用or效率高，不会使索引失效 7）子查询SQL语句中嵌套SELECT语句，称谓嵌套查询，又称子查询。SELECT * FROM t1 WHERE column1 = ( SELECT column1 FROM t2);子查询外部的语句可以是 INSERT &#x2F; UPDATE &#x2F; DELETE &#x2F; SELECT 的任何一个 根据子查询结果可以分为： 标量子查询（子查询结果为单个值） 列子查询（子查询结果为一列） 行子查询（子查询结果为一行） 表子查询（子查询结果为多行多列） 根据子查询位置可分为： WHERE 之后 FROM 之后 SELECT 之后 ①标量子查询子查询返回的结果是单个值（数字、字符串、日期等）。常用操作符：- &lt; &gt; &gt; &gt;&#x3D; &lt; &lt;&#x3D; 例子： 123456789-- 查询销售部所有员工select id from dept where name = &#x27;销售部&#x27;;-- 根据销售部部门ID，查询员工信息select * from employee where dept = 4;-- 合并（子查询）select * from employee where dept = (select id from dept where name = &#x27;销售部&#x27;);-- 查询xxx入职之后的员工信息select * from employee where entrydate &gt; (select entrydate from employee where name = &#x27;xxx&#x27;); ②列子查询返回的结果是一列（可以是多行）。 常用操作符： 操作符 描述 IN 在指定的集合范围内，多选一 NOT IN 不在指定的集合范围内 ANY 子查询返回列表中，有任意一个满足即可 SOME 与ANY等同，使用SOME的地方都可以使用ANY ALL 子查询返回列表的所有值都必须满足 例子： 123456-- 查询销售部和市场部的所有员工信息select * from employee where dept in (select id from dept where name = &#x27;销售部&#x27; or name = &#x27;市场部&#x27;);-- 查询比财务部所有人工资都高的员工信息select * from employee where salary &gt; all(select salary from employee where dept = (select id from dept where name = &#x27;财务部&#x27;));-- 查询比研发部任意一人工资高的员工信息select * from employee where salary &gt; any (select salary from employee where dept = (select id from dept where name = &#x27;研发部&#x27;)); ③行子查询返回的结果是一行（可以是多列）。常用操作符：&#x3D;, &lt;, &gt;, IN, NOT IN 例子： 123-- 查询与xxx的薪资及直属领导相同的员工信息select * from employee where (salary, manager) = (12500, 1);select * from employee where (salary, manager) = (select salary, manager from employee where name = &#x27;xxx&#x27;); ④表子查询返回的结果是多行多列常用操作符：IN 例子： 1234-- 查询与xxx1，xxx2的职位和薪资相同的员工select * from employee where (job, salary) in (select job, salary from employee where name = &#x27;xxx1&#x27; or name = &#x27;xxx2&#x27;);-- 查询入职日期是2006-01-01之后的员工，及其部门信息select e.*, d.* from (select * from employee where entrydate &gt; &#x27;2006-01-01&#x27;) as e left join dept as d on e.dept = d.id; 二、数据类型1）整型 类型名称 取值范围 大小 TINYINT -128〜127 1个字节 SMALLINT -32768〜32767 2个宇节 MEDIUMINT -8388608〜8388607 3个字节 INT (INTEGHR) -2147483648〜2147483647 4个字节 BIGINT -9223372036854775808〜9223372036854775807 8个字节 无符号在数据类型后加 unsigned 关键字。 2）浮点型 类型名称 说明 存储需求 FLOAT 单精度浮点数 4 个字节 DOUBLE 双精度浮点数 8 个字节 DECIMAL (M, D)，DEC 压缩的“严格”定点数 M+2 个字节 3）日期和时间 类型名称 日期格式 日期范围 存储需求 YEAR YYYY 1901 ~ 2155 1 个字节 TIME HH:MM:SS -838:59:59 ~ 838:59:59 3 个字节 DATE YYYY-MM-DD 1000-01-01 ~ 9999-12-3 3 个字节 DATETIME YYYY-MM-DD HH:MM:SS 1000-01-01 00:00:00 ~ 9999-12-31 23:59:59 8 个字节 TIMESTAMP YYYY-MM-DD HH:MM:SS 1980-01-01 00:00:01 UTC ~ 2040-01-19 03:14:07 UTC 4 个字节 4）字符串 类型名称 说明 存储需求 CHAR(M) 固定长度非二进制字符串 M 字节，1&lt;&#x3D;M&lt;&#x3D;255 VARCHAR(M) 变长非二进制字符串 L+1字节，在此，L&lt; &#x3D; M和 1&lt;&#x3D;M&lt;&#x3D;255 TINYTEXT 非常小的非二进制字符串 L+1字节，在此，L&lt;2^8 TEXT 小的非二进制字符串 L+2字节，在此，L&lt;2^16 MEDIUMTEXT 中等大小的非二进制字符串 L+3字节，在此，L&lt;2^24 LONGTEXT 大的非二进制字符串 L+4字节，在此，L&lt;2^32 ENUM 枚举类型，只能有一个枚举字符串值 1或2个字节，取决于枚举值的数目 (最大值为65535) SET 一个设置，字符串对象可以有零个或 多个SET成员 1、2、3、4或8个字节，取决于集合 成员的数量（最多64个成员） 5）二进制类型 类型名称 说明 存储需求 BIT(M) 位字段类型 大约 (M+7)&#x2F;8 字节 BINARY(M) 固定长度二进制字符串 M 字节 VARBINARY (M) 可变长度二进制字符串 M+1 字节 TINYBLOB (M) 非常小的BLOB L+1 字节，在此，L&lt;2^8 BLOB (M) 小 BLOB L+2 字节，在此，L&lt;2^16 MEDIUMBLOB (M) 中等大小的BLOB L+3 字节，在此，L&lt;2^24 LONGBLOB (M) 非常大的BLOB L+4 字节，在此，L&lt;2^32 三、权限一览表 具体权限的作用详见官方文档 GRANT 和 REVOKE 允许的静态权限 Privilege Grant Table Column Context ALL [PRIVILEGES] Synonym for “all privileges” Server administration ALTER Alter_priv Tables ALTER ROUTINE Alter_routine_priv Stored routines CREATE Create_priv Databases, tables, or indexes CREATE ROLE Create_role_priv Server administration CREATE ROUTINE Create_routine_priv Stored routines CREATE TABLESPACE Create_tablespace_priv Server administration CREATE TEMPORARY TABLES Create_tmp_table_priv Tables CREATE USER Create_user_priv Server administration CREATE VIEW Create_view_priv Views DELETE Delete_priv Tables DROP Drop_priv Databases, tables, or views DROP ROLE Drop_role_priv Server administration EVENT Event_priv Databases EXECUTE Execute_priv Stored routines FILE File_priv File access on server host GRANT OPTION Grant_priv Databases, tables, or stored routines INDEX Index_priv Tables INSERT Insert_priv Tables or columns LOCK TABLES Lock_tables_priv Databases PROCESS Process_priv Server administration PROXY See proxies_priv table Server administration REFERENCES References_priv Databases or tables RELOAD Reload_priv Server administration REPLICATION CLIENT Repl_client_priv Server administration REPLICATION SLAVE Repl_slave_priv Server administration SELECT Select_priv Tables or columns SHOW DATABASES Show_db_priv Server administration SHOW VIEW Show_view_priv Views SHUTDOWN Shutdown_priv Server administration SUPER Super_priv Server administration TRIGGER Trigger_priv Tables UPDATE Update_priv Tables or columns USAGE Synonym for “no privileges” Server administration GRANT 和 REVOKE 允许的动态权限 Privilege Context APPLICATION_PASSWORD_ADMIN Dual password administration AUDIT_ABORT_EXEMPT Allow queries blocked by audit log filter AUDIT_ADMIN Audit log administration AUTHENTICATION_POLICY_ADMIN Authentication administration BACKUP_ADMIN Backup administration BINLOG_ADMIN Backup and Replication administration BINLOG_ENCRYPTION_ADMIN Backup and Replication administration CLONE_ADMIN Clone administration CONNECTION_ADMIN Server administration ENCRYPTION_KEY_ADMIN Server administration FIREWALL_ADMIN Firewall administration FIREWALL_EXEMPT Firewall administration FIREWALL_USER Firewall administration FLUSH_OPTIMIZER_COSTS Server administration FLUSH_STATUS Server administration FLUSH_TABLES Server administration FLUSH_USER_RESOURCES Server administration GROUP_REPLICATION_ADMIN Replication administration GROUP_REPLICATION_STREAM Replication administration INNODB_REDO_LOG_ARCHIVE Redo log archiving administration NDB_STORED_USER NDB Cluster PASSWORDLESS_USER_ADMIN Authentication administration PERSIST_RO_VARIABLES_ADMIN Server administration REPLICATION_APPLIER PRIVILEGE_CHECKS_USER for a replication channel REPLICATION_SLAVE_ADMIN Replication administration RESOURCE_GROUP_ADMIN Resource group administration RESOURCE_GROUP_USER Resource group administration ROLE_ADMIN Server administration SESSION_VARIABLES_ADMIN Server administration SET_USER_ID Server administration SHOW_ROUTINE Server administration SYSTEM_USER Server administration SYSTEM_VARIABLES_ADMIN Server administration TABLE_ENCRYPTION_ADMIN Server administration VERSION_TOKEN_ADMIN Server administration XA_RECOVER_ADMIN Server administration 四、test","categories":[{"name":"mysql","slug":"mysql","permalink":"http://example.com/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://example.com/tags/mysql/"}]},{"title":"springMVC的使用","slug":"springMVC","date":"2022-04-04T07:29:48.000Z","updated":"2022-08-18T04:44:13.706Z","comments":true,"path":"2022/04/04/springMVC/","link":"","permalink":"http://example.com/2022/04/04/springMVC/","excerpt":"springMVC的基本过程、bean的加载控制、url参数传递、数据响应、Rest风格、静态资源的放行、异常处理、拦截器","text":"springMVC的基本过程、bean的加载控制、url参数传递、数据响应、Rest风格、静态资源的放行、异常处理、拦截器 1.springMVC执行过程 1）启动服务器的初始化​ ①服务器启动会找到ServletContainersInitConfig类（继承了MVC所规定的AbstractDispatcherServletInitializer） 该类的作用其实是和web.xml中的dispatcherServlet作用是一样的： 记载了springMVC的配置类 定义了拦截的请求 ②执行createServletApplicationContext方法 ​ 创建了WebApplicationContext对象，并使用regist加载SpringMVC的配置类SpringMvcConfig来初始化SpringMVC的容器 ③加载springMvcConfig ④执行@ComponentScan加载对应的bean 这里仅仅扫描了controller，目的是让springMvc的容器来管理web相关的controller， 至于service和dao中的bean则交给spring来进行管理 ⑤加载UserController，每个@RequestMapping的名称对应一个具体的方法 此时就建立起了 /save 和 save 方法的对应关系 ⑥执行getServletMappings方法，设定SpringMVC拦截请求的路径规则 拦截规则补充： https://www.cnblogs.com/chenmz1995/p/10367199.html https://liuxingchang.blog.csdn.net/article/details/109016430?spm=1001.2101.3001.6661.1&amp;utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-109016430-blog-105250658.pc_relevant_paycolumn_v3&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-109016430-blog-105250658.pc_relevant_paycolumn_v3&amp;utm_relevant_index=1 2）单次请求过程 3）基本标签①@Controller ②@RequestMapping ③@ResposeBody 因为在返回的时候直接返回String的话是根据该值，通过视图解析器找到对应的页面，如果我们想想页面直接返回该结果，就可以添加@ResponseBody注解 2.bean的加载控制 1）精确扫描 这样springConfig就不会扫描到controller了 注意： ​ 这里可以不用扫描dao包，因为dao的bean都是通过动态代理的方式生成bean对象，而不是让spring容器生成的 如果这里不扫描也可以，但是通用性会差一些，如果不再使用mybatis技术，而是使用其它技术，dao层的bean就会没有 2）排除 3.url参数传递1）基本参数传递①普通参数(名称匹配) ②普通参数(名称不匹配) ③@RequestParam注解 注： @RequestParam参数和@RequestBody的对比：https://blog.csdn.net/weixin_38004638/article/details/99655322 Content-type的几种常见类型：https://blog.csdn.net/duanHaoYu97/article/details/123408852 ④pojo参数 ⑤pojo嵌套参数 ⑥数组参数 ⑦集合 ​ 集合和数组最大的不同是需要添加@RequestParam参数 2）json参数传递 ①准备pom依赖： 开启自动转换JSON数据的支持（告诉MVC接收数据后进行JSON转换）： ②json普通数组 ③json对象普通pojo： 嵌套pojo： ④json对象数组 ⑤@EnableWebMvc ⑥@RequestBody @RequestParam和@RequestBody对比： 注： @RequestParam参数和@RequestBody的对比：https://blog.csdn.net/weixin_38004638/article/details/99655322 Content-type的几种常见类型：https://blog.csdn.net/duanHaoYu97/article/details/123408852 3）日期参数传递①@DateTimeFormat ②converter接口 ②eg 4.响应1）json数据响应①准备pom依赖： 开启自动转换JSON数据的支持（告诉MVC接收数据后进行JSON转换）： ②pojo ③pojo集合 2）json的转换原理①@ResponseBody ②转换过程 ①接口下有很多实现类，其中②这个实现类就是Jackson2HttpMessageConverter，这样就会把我们返回的数据转换成json数据 5.Rest风格1）基本概念 相较于传统方式，rest风格的书写隐藏了访问资源的行为，我们无法通过地址的值对资源进行了什么操作： 我们通过地址来指明要访问的模块http://localhost/users，这就代表我们访问了users模块 具体的操作由方法来决定 注意：模块名称用复数，这是一个约定俗成的规范 2）基本实现①新增12345678910@Controllerpublic class UserController &#123; //设置当前请求方法为POST，表示REST风格中的添加操作 @RequestMapping(value = &quot;/users&quot;,method = RequestMethod.POST) @ResponseBody public String save() &#123; System.out.println(&quot;user save...&quot;); return &quot;&#123;&#x27;module&#x27;:&#x27;user save&#x27;&#125;&quot;; &#125;&#125; 将请求路径更改为/users 访问该方法使用 POST: http://localhost/users 使用method属性限定该方法的访问方式为POST 如果发送的不是POST请求，比如发送GET请求，则会报错 ②删除12345678910@Controllerpublic class UserController &#123; //设置当前请求方法为DELETE，表示REST风格中的删除操作 @RequestMapping(value = &quot;/users&quot;,method = RequestMethod.DELETE) @ResponseBody public String delete(Integer id) &#123; System.out.println(&quot;user delete...&quot; + id); return &quot;&#123;&#x27;module&#x27;:&#x27;user delete&#x27;&#125;&quot;; &#125;&#125; 将请求路径更改为/users 访问该方法使用 DELETE: http://localhost/users 访问成功，但是删除方法没有携带所要删除数据的id,所以针对RESTful的开发，如何携带数据参数? 传递路径参数 前端发送请求的时候使用:http://localhost/users/1,路径中的1就是我们想要传递的参数。 后端获取参数，需要做如下修改: 修改@RequestMapping的value属性，将其中修改为/users/&#123;id&#125;，目的是和路径匹配 在方法的形参前添加**@PathVariable**注解 12345678910@Controllerpublic class UserController &#123; //设置当前请求方法为DELETE，表示REST风格中的删除操作 @RequestMapping(value = &quot;/users/&#123;id&#125;&quot;,method = RequestMethod.DELETE) @ResponseBody public String delete(@PathVariable Integer id) &#123; System.out.println(&quot;user delete...&quot; + id); return &quot;&#123;&#x27;module&#x27;:&#x27;user delete&#x27;&#125;&quot;; &#125;&#125; 思考如下两个问题: 如果方法形参的名称和路径&#123;&#125;中的值不一致，该怎么办? 如果有多个参数需要传递该如何编写? 前端发送请求的时候使用:http://localhost/users/1/tom,路径中的1和tom就是我们想要传递的两个参数。 后端获取参数，需要做如下修改: 12345678910@Controllerpublic class UserController &#123; //设置当前请求方法为DELETE，表示REST风格中的删除操作 @RequestMapping(value = &quot;/users/&#123;id&#125;/&#123;name&#125;&quot;,method = RequestMethod.DELETE) @ResponseBody public String delete(@PathVariable(&quot;id&quot;) Integer id,@PathVariable(&quot;name&quot;) String name) &#123; System.out.println(&quot;user delete...&quot; + id+&quot;,&quot;+name); return &quot;&#123;&#x27;module&#x27;:&#x27;user delete&#x27;&#125;&quot;; &#125;&#125; ③修改12345678910@Controllerpublic class UserController &#123; //设置当前请求方法为PUT，表示REST风格中的修改操作 @RequestMapping(value = &quot;/users&quot;,method = RequestMethod.PUT) @ResponseBody public String update(@RequestBody User user) &#123; System.out.println(&quot;user update...&quot; + user); return &quot;&#123;&#x27;module&#x27;:&#x27;user update&#x27;&#125;&quot;; &#125;&#125; 将请求路径更改为/users 访问该方法使用 PUT: http://localhost/users 访问并携带参数: ④查询单个根据ID查询 12345678910@Controllerpublic class UserController &#123; //设置当前请求方法为GET，表示REST风格中的查询操作 @RequestMapping(value = &quot;/users/&#123;id&#125;&quot; ,method = RequestMethod.GET) @ResponseBody public String getById(@PathVariable Integer id)&#123; System.out.println(&quot;user getById...&quot;+id); return &quot;&#123;&#x27;module&#x27;:&#x27;user getById&#x27;&#125;&quot;; &#125;&#125; 将请求路径更改为/users 访问该方法使用 GET: http://localhost/users/666 ⑤查询所有查询所有12345678910@Controllerpublic class UserController &#123; //设置当前请求方法为GET，表示REST风格中的查询操作 @RequestMapping(value = &quot;/users&quot; ,method = RequestMethod.GET) @ResponseBody public String getAll() &#123; System.out.println(&quot;user getAll...&quot;); return &quot;&#123;&#x27;module&#x27;:&#x27;user getAll&#x27;&#125;&quot;; &#125;&#125; 将请求路径更改为/users 访问该方法使用 GET: http://localhost/users 3）RestFul快速开发①@RestController 名称 @RestController 类型 &#x3D;&#x3D;类注解&#x3D;&#x3D; 位置 基于SpringMVC的RESTful开发控制器类定义上方 作用 设置当前控制器类为RESTful风格，等同于@Controller与@ResponseBody两个注解组合功能 ②@xxxMapping 名称 @GetMapping @PostMapping @PutMapping @DeleteMapping 类型 &#x3D;&#x3D;方法注解&#x3D;&#x3D; 位置 基于SpringMVC的RESTful开发控制器方法定义上方 作用 设置当前控制器方法请求访问路径与请求动作，每种对应一个请求动作，例如@GetMapping对应GET请求 相关属性 value（默认）：请求访问路径 ③@PathVariable 名称 @PathVariable 类型 &#x3D;&#x3D;形参注解&#x3D;&#x3D; 位置 SpringMVC控制器方法形参定义前面 作用 绑定路径参数与处理器方法形参间的关系，要求路径参数名与形参名一一对应 ④三种接收参数的注解对比关于接收参数，我们学过三个注解@RequestBody、@RequestParam、@PathVariable,这三个注解之间的区别和应用分别是什么? 区别 @RequestParam用于接收url地址传参或表单传参，也就是/test/demo_form.php?name1=value1&amp;name2=value2 请求头：application&#x2F;x-www-form-urlencoded @RequestBody用于接收json数据，json数据是在请求体中的 请求头：application&#x2F;json @PathVariable用于接收路径参数，使用{参数名称}描述路径参数：/user/&#123;userId&#125; 应用 后期开发中，发送请求参数超过1个时，以json格式为主，@RequestBody应用较广 如果发送非json格式数据，选用@RequestParam接收请求参数 采用RESTful进行开发，当参数数量较少时，例如1个，可以采用@PathVariable接收请求路径变量，通常用于传递id值 6.静态资源的放行 在进行这样的配置后，mvc就会拦截掉页面请求从而把他当成一个mapping，所以我们需要放行该请求让tomcat做这件事 解决方法： 1）编写SpringMvcSupport继承WebMvcConfigurationSupport 123456789101112@Configurationpublic class SpringMvcSupport extends WebMvcConfigurationSupport &#123; //设置静态资源访问过滤，当前类需要设置为配置类，并被扫描加载 @Override protected void addResourceHandlers(ResourceHandlerRegistry registry) &#123; //当访问/pages/????时候，从/pages目录下查找内容 registry.addResourceHandler(&quot;/pages/**&quot;).addResourceLocations(&quot;/pages/&quot;); registry.addResourceHandler(&quot;/js/**&quot;).addResourceLocations(&quot;/js/&quot;); registry.addResourceHandler(&quot;/css/**&quot;).addResourceLocations(&quot;/css/&quot;); registry.addResourceHandler(&quot;/plugins/**&quot;).addResourceLocations(&quot;/plugins/&quot;); &#125;&#125; ①放行路径 和 ②访问目录 之间的对应关系：如果拦截到了①这样的请求，那么就访问②这个目录下的资源 2） 保证扫描到该配置类 该配置类是在config目录下，SpringMVC扫描的是controller包，所以该配置类还未生效，要想生效需要将SpringMvcConfig配置类进行修改 12345678910111213@Configuration@ComponentScan(&#123;&quot;com.itheima.controller&quot;,&quot;com.itheima.config&quot;&#125;)@EnableWebMvcpublic class SpringMvcConfig &#123;&#125;或者@Configuration@ComponentScan(&quot;com.itheima&quot;)@EnableWebMvcpublic class SpringMvcConfig &#123;&#125; 7.基本配置1）servletConfig类似web.xml ①基本②过滤器 2）web.xml①监听器读取类路径下的配置applicationContext.xml配置文件 ②过滤器解决中文乱码 ③配置前端控制器 ④欢迎页面 ⑤错误页面 3）springMvcConfig 8.SSM整合1）依赖pom.xml添加SSM所需要的依赖jar包 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.fla&lt;/groupId&gt; &lt;artifactId&gt;springmvc_08_ssm&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;5.2.10.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;5.2.10.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;5.2.10.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.5.6&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.16&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt; &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.1&lt;/version&gt; &lt;configuration&gt; &lt;port&gt;80&lt;/port&gt; &lt;path&gt;/&lt;/path&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 2）SpringConfig配置类1234567@Configuration@ComponentScan(&#123;&quot;com.fla.service&quot;&#125;)@PropertySource(&quot;classpath:jdbc.properties&quot;)@Import(&#123;JdbcConfig.class,MyBatisConfig.class&#125;)@EnableTransactionManagementpublic class SpringConfig &#123;&#125; 3）JdbcConfig配置类123456789101112131415161718192021222324252627public class JdbcConfig &#123; @Value(&quot;$&#123;jdbc.driver&#125;&quot;) private String driver; @Value(&quot;$&#123;jdbc.url&#125;&quot;) private String url; @Value(&quot;$&#123;jdbc.username&#125;&quot;) private String username; @Value(&quot;$&#123;jdbc.password&#125;&quot;) private String password; @Bean public DataSource dataSource()&#123; DruidDataSource dataSource = new DruidDataSource(); dataSource.setDriverClassName(driver); dataSource.setUrl(url); dataSource.setUsername(username); dataSource.setPassword(password); return dataSource; &#125; @Bean public PlatformTransactionManager transactionManager(DataSource dataSource)&#123; DataSourceTransactionManager ds = new DataSourceTransactionManager(); ds.setDataSource(dataSource); return ds; &#125;&#125; 4）MybatisConfig配置类12345678910111213141516public class MyBatisConfig &#123; @Bean public SqlSessionFactoryBean sqlSessionFactory(DataSource dataSource)&#123; SqlSessionFactoryBean factoryBean = new SqlSessionFactoryBean(); factoryBean.setDataSource(dataSource); factoryBean.setTypeAliasesPackage(&quot;com.fla.domain&quot;); return factoryBean; &#125; @Bean public MapperScannerConfigurer mapperScannerConfigurer()&#123; MapperScannerConfigurer msc = new MapperScannerConfigurer(); msc.setBasePackage(&quot;com.fla.dao&quot;); return msc; &#125;&#125; 5）SpringMVC配置类12345@Configuration@ComponentScan(&quot;com.fla.controller&quot;)@EnableWebMvcpublic class SpringMvcConfig &#123;&#125; 6）Web项目入口配置类12345678910111213141516171819202122public class ServletConfig extends AbstractAnnotationConfigDispatcherServletInitializer &#123; //加载Spring配置类 protected Class&lt;?&gt;[] getRootConfigClasses() &#123; return new Class[]&#123;SpringConfig.class&#125;; &#125; //加载SpringMVC配置类 protected Class&lt;?&gt;[] getServletConfigClasses() &#123; return new Class[]&#123;SpringMvcConfig.class&#125;; &#125; //设置SpringMVC请求地址拦截规则 protected String[] getServletMappings() &#123; return new String[]&#123;&quot;/&quot;&#125;; &#125; //设置post请求中文乱码过滤器 @Override protected Filter[] getServletFilters() &#123; CharacterEncodingFilter filter = new CharacterEncodingFilter(); filter.setEncoding(&quot;utf-8&quot;); return new Filter[]&#123;filter&#125;; &#125;&#125; 至此SSM整合的环境就已经搭建好了。在这个环境上，我们如何进行功能模块的开发呢? 9.异常处理1）异常处理器①@RestControllerAdvice ②@ExceptionHandler ③eg 2）异常划分及处理①异常分类 ②自定义异常系统级： 业务级： ③自定义异常code ④eg模拟异常： 异常处理器： 10.拦截器1）基本概念①基本流程 (1)浏览器发送一个请求会先到Tomcat的web服务器 (2)Tomcat服务器接收到请求以后，会去判断请求的是静态资源还是动态资源 (3)如果是静态资源，会直接到Tomcat的项目部署目录下去直接访问（静态资源的放行） (4)如果是动态资源，就需要交给项目的后台代码进行处理 (5)在找到具体的方法之前，我们可以去配置过滤器(可以配置多个)，按照顺序进行执行过 滤器是servlet技术，对所有请求有效 (6)然后进入到中央处理器(SpringMVC中的内容)，MVC会根据配置找到对应操作 (7)我们需要在每个Controller方法执行的前后添加业务，可以用拦截器实现 拦截器是mvc技术，仅对mvc处理内容有效 ②作用 拦截器（Interceptor）是一种动态拦截方法调用的机制，在SpringMVC中动态拦截控制器方法的执行 作用: 在指定的方法调用前后执行预先设定的代码 阻止原始方法的执行：如权限不够不能执行 拦截器就是用来做增强 ③过滤器和拦截器对比 归属不同：Filter属于Servlet技术，Interceptor属于SpringMVC技术 拦截内容不同：Filter对所有访问进行增强，Interceptor仅针对SpringMVC的访问进行增强 2）eg ①创建拦截器类让类实现HandlerInterceptor接口，重写接口中的三个方法。 1234567891011121314151617181920212223@Component//定义拦截器类，实现HandlerInterceptor接口//注意当前类必须受Spring容器控制public class ProjectInterceptor implements HandlerInterceptor &#123; @Override //原始方法调用前执行的内容 public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; System.out.println(&quot;preHandle...&quot;); return true; &#125; @Override //原始方法调用后执行的内容 public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception &#123; System.out.println(&quot;postHandle...&quot;); &#125; @Override //原始方法调用完成后执行的内容 public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception &#123; System.out.println(&quot;afterCompletion...&quot;); &#125;&#125; 注意: ​ 拦截器类要被SpringMVC容器扫描到。 ②创建拦截器配置类1234567891011121314151617@Configurationpublic class SpringMvcSupport extends WebMvcConfigurationSupport &#123; @Autowired private ProjectInterceptor projectInterceptor; //静态资源的放行 @Override protected void addResourceHandlers(ResourceHandlerRegistry registry) &#123; registry.addResourceHandler(&quot;/pages/**&quot;).addResourceLocations(&quot;/pages/&quot;); &#125; //配置拦截器 @Override protected void addInterceptors(InterceptorRegistry registry) &#123; registry.addInterceptor(projectInterceptor).addPathPatterns(&quot;/books&quot; ); &#125;&#125; 注意： ​ 这种方式是将拦截器的配置单独写出来的，这个类继承了WebMvcConfigurationSupport，且springConfig中并没有写什么内容（如西面的③中所写的） ③SpringMVC添加SpringMvcSupport包扫描123456@Configuration@ComponentScan(&#123;&quot;com.fla.controller&quot;,&quot;com.fla.config&quot;&#125;)@EnableWebMvcpublic class SpringMvcConfig&#123; &#125; ④运行程序测试使用PostMan发送http://localhost/books 如果发送http://localhost/books/100会发现拦截器没有被执行，原因是拦截器的addPathPatterns方法配置的拦截路径是/books,我们现在发送的是/books/100，所以没有匹配上，因此没有拦截，拦截器就不会执行。 ⑤修改拦截器拦截规则12345678910111213141516@Configurationpublic class SpringMvcSupport extends WebMvcConfigurationSupport &#123; @Autowired private ProjectInterceptor projectInterceptor; @Override protected void addResourceHandlers(ResourceHandlerRegistry registry) &#123; registry.addResourceHandler(&quot;/pages/**&quot;).addResourceLocations(&quot;/pages/&quot;); &#125; @Override protected void addInterceptors(InterceptorRegistry registry) &#123; //配置拦截器 registry.addInterceptor(projectInterceptor).addPathPatterns(&quot;/books&quot;,&quot;/books/*&quot; ); &#125;&#125; 这个时候，如果再次访问http://localhost/books/100，拦截器就会被执行。 最后说一件事，就是拦截器中的preHandler方法，如果返回true,则代表放行，会执行原始Controller类中要请求的方法，如果返回false，则代表拦截，后面的就不会再执行了。 ⑥简化SpringMvc的编写1234567891011121314@Configuration@ComponentScan(&#123;&quot;com.fla.controller&quot;&#125;)@EnableWebMvc//实现WebMvcConfigurer接口可以简化开发，但具有一定的侵入性public class SpringMvcConfig implements WebMvcConfigurer &#123; @Autowired private ProjectInterceptor projectInterceptor; @Override public void addInterceptors(InterceptorRegistry registry) &#123; //配置多拦截器 registry.addInterceptor(projectInterceptor).addPathPatterns(&quot;/books&quot;,&quot;/books/*&quot;); &#125;&#125; 这样有一个缺点（侵入式变强），该类已经和spring的api产生了绑定关系： 这里是直接将拦截器的配置写在了springConfig类中，实现了WebMvcConfigurer并开启了@EnableWebMvc 3）拦截器方法及参数①preHandle原始方法之前运行preHandle 12345public boolean preHandle(HttpServletRequest request,HttpServletResponse response, Object handler) throws Exception &#123; System.out.println(&quot;preHandle&quot;); return true;&#125; 这个方法中可以通过返回值boolean来决定是否要进行放行，我们可以把业务逻辑放在该方法 如果满足业务则返回true放行，不满足则返回false拦截 handler:被调用的处理器对象，本质上是一个方法对象，对反射中的Method对象进行了再包装，可以使用Method eg：request对象获取请求头的Content-Type 123456public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; String contentType = request.getHeader(&quot;Content-Type&quot;); System.out.println(&quot;preHandle...&quot;+contentType); return true;&#125; eg：handler参数获取方法的相关信息，handler的getClass得到的就是HandlerMethod 1234567public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; HandlerMethod hm = (HandlerMethod)handler; String methodName = hm.getMethod().getName();//可以获取方法的名称 System.out.println(&quot;preHandle...&quot;+methodName); return true;&#125; ②postHandle原始方法运行后运行，如果原始方法被拦截，则不执行 123456public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception &#123; System.out.println(&quot;postHandle&quot;);&#125; ModelAndView ③afterCompletion拦截器最后执行的方法，无论原始方法是否执行 123456public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception &#123; System.out.println(&quot;afterCompletion&quot;);&#125; 前三个参数与上面的是一致的。 ex:如果处理器执行过程中出现异常对象，可以针对异常情况进行单独处理 但由于有全局异常处理器类，所以该参数的使用率也不高。 4）拦截器链 多个拦截器的顺序就是配置的顺序： eg： ​ 当图中pre2返回false后，会进入after1的方法，并不会进入after2！","categories":[{"name":"Spring MVC","slug":"Spring-MVC","permalink":"http://example.com/categories/Spring-MVC/"}],"tags":[{"name":"Spring MVC","slug":"Spring-MVC","permalink":"http://example.com/tags/Spring-MVC/"}]},{"title":"计算机网络","slug":"计算机网络","date":"2022-04-04T03:36:48.000Z","updated":"2022-09-19T16:20:48.826Z","comments":true,"path":"2022/04/04/计算机网络/","link":"","permalink":"http://example.com/2022/04/04/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/","excerpt":"网络层：IPV4（分类、划分子网、无分类）、IP首部、ICMP 运输层：UDP、TCP、三次握手、四次挥手、TCP流量控制、拥塞控制、ARQ、保活计数器、TCP首部格式 应用层：DHCP、DNS、FTP、电子邮件、HTTP、HTTPS","text":"网络层：IPV4（分类、划分子网、无分类）、IP首部、ICMP 运输层：UDP、TCP、三次握手、四次挥手、TCP流量控制、拥塞控制、ARQ、保活计数器、TCP首部格式 应用层：DHCP、DNS、FTP、电子邮件、HTTP、HTTPS 三、网络层1.IPv4地址1）分类编址①A类地址 A类地址网络号占8比特，主机号占24比特 网络号最高位固定为0 注意： 最小网络号是全0，保留不指派 最小可分配网络号：00000001 ——&gt; 1 最大可分配网络号：01111110 ——&gt; 126 最小本地环回测试地址主机号为1，因为全0就是127.0.0.0代表网络号 最大本地环回测试地址主机号为255-1（最后一位为0），因为全1就是127.255.255.255代表广播地址了 ②B类地址 B类地址网络号和主机号分别占16比特 B类地址网络号前两位固定为10 注意： 最小可分配网络号：10000000 . 00000000 ——&gt; 127.0 最大网可分配络号：10111111 . 11111111 ——&gt; 191(2^7+2^6-1).255 ③C类地址 C类地址网络号占24位，主机号占8位 C类地址网络号前三位固定为110 注意： 最小可分配网络号：11000000.00000000.00000000 ——&gt; 192.0.0 最大可分配网络号：11011111 . 11111111.11111111 ——&gt; 223(2^7+2^6+2^5-1).255.255 2）划分子网编址 ①子网掩码 ②eg1 218开头是110，所以这是一个C类地址，其子网掩码255.255.255.128（前三部分为C类网络号，最后从主机号借用的地址） 可以看出从主机号中借用了1位，1位可以划分出两个子网（0,1），每个子网有7位可以来编码 ②eg2 ③eg3 3）无分类编址 ①eg1②eg2 ③路由聚合（构建超网） 对于图中R1连接了5个网络，若R1要将这五个网络信息发送给R2，就需要发送5个路由信息给R2（也就是需要5个网络编号）： 很显然，这五个网络网络号中由很多部分是共同前缀的，那么这部分能不能采用CIDR路由聚合成一个网络再发送给R2来降低路由表大小呢，显然是可以的： 因此其聚合地址块为： 这样就把5条记录缩小到一条记录，当R2的目的网络是该聚合地址块就会跳R1 在路由过程中，我们可以得出两个结论： 网络前缀越长，地址块越小（主机号），路由约具体。 若地址块转发分组时发现由多条路由可选，则选择网络前缀最长的那条，这样子的路由更具体（网络号越长，说明再继续聚合的可能性越低，也就越具体） ④eg3 ⑤eg4 2.IP首部格式 1）版本 2）首部长度 3）可选字段 4）填充字段 5）区分服务字段 6）总长度字段 eg： 7）标识、标志、片偏移字段 ①数据报分片 ②eg 8）生存时间 9）协议字段 10）首部检验和 11）源IP地址、目的IP地址字段 3.ICMP1）差错报文 ①终点不可达 ②源点抑制 ③时间超过 ④参数问题 ⑤改变路由 ⑥不发送给ICMP差错报告报文情况 2）ICMP询问报文回送回答和回答报文： ICMP回送回答报文是由主机和路由向一个特定的目的主机发出的询问 收到此报文的主机必须给源主机或路由器发送ICMP回送回答报文 主要用来测试目的站是否可达以及了解其有关状态 时间戳请求和回答： ICMP时间戳请求报文时请某个主机或路由器回答当前的日期和时间 ICMP时间戳回答报文中有一个32位的字段，写入的整数代表从1900年1月1日到当前时刻一共有多少秒 主要用来进行时钟同步和测量时间 3）应用①PING 用来测试主机或路由器之间的连通性 应用层直接使用网际层的ICMP(没有经过运输层的TCP或UDP) 使用ICMP回送请求和回答报文 ②traceroute 跟踪路由traceroute实现原理： ​ 简单来说，就是先发送一个TTL &#x3D; 1的报文，当TTL到达第一个主机后TTL变为0，此时路由器会回送时间超过差错报文，此时就能知道第一个路由位置。此时源主机再发送TTL &#x3D; 2 的报文，当TTL到达第二个主机后TTL变为0，此时路由器会回送时间超过差错报文，此时就能够知道第二个路由的地址。如此反复，直到到达最后一个路由。 四、运输层1.一次请求 浏览器输入域名www.baidu.com，用户PC中的DNS客户端进程就会发送一个DNS请求报文到DNS服务器 DNS是通过UDP封装的，DNS客户端会随机挑选一个短暂端口号作为源端口号49152，目标端口号是53 DNS服务器接收到了这个DNS请求报文，就会解析数据然后返回结果 源端口:53 目标端口:49152 客户端接收到了DNS响应报文，发现目的端口是49152，知道了是DNS的回复，解析该报文得到域名对应的IP地址xxx 知道了ip地址xxx，就可以开始构建HTTP请求，xxx:80，请求对应的数据 ​ 源端口:挑选到 49153 目标端口:80 web服务器收到HTTP请求，解析出目标端口，HTTP服务端进程解析到了请求内容，然后返回回去 客户端接收到HTTP响应报文，49153HTTP客户端进程解析内容 源端口:80 目标端口:49153 2.UDP和TCP的对比 TCP：运输层采用面向连接的 TCP 协议时，尽管下面的网络是不可靠的（只提供尽最大努力服务），但TCP协议就相当于在逻辑上建立了一条通信信道，该信道是全双工的可靠信道。 UDP：当运输层采用无连接的 UDP 协议时，这种逻辑通信信道是一条不可靠信道。 接下来，我们从以下几个方面对比UDP和TCP: 以下的连接是逻辑上连接不是物理上的连接。 在连接方式上： UDP是无连接的通信方式 TCP是通过著名的三次握手建立连接，四次挥手释放连接。 在传播方式上： UDP由于不建立连接，支持多播和广播 TCP由于每次通信需要建立基于TCP连接的可靠信道，且每次只能建立一条连接，因此只支持单播 在报文传输处理上： UDP协议中，对于应用层传输下来的报文不进行处理，保留报文的边界。在给报文加上UDP首部，进行发送。UDP接收方首部接收到UDP数据后，去除其首部，交付给应用层。 可以看出，UDP是针对报文为单位进行处理的，也就是UDP是面向应用报文的。 在TCP发送方： TCP协议会把应用进程交付下来的数据块（报文）看作是一连串无结构的字节流（TCP并不知道这些子节含义），将他们编号，并存储在自己的发送缓存中，TCP再根据发送策略，提取一定量的字节，加上TCP首部，构建成TCP报文进行发送。 对于接收方,同时进行两件事： 从所接受到的TCP报文段中，取出数据载荷部分并存储在接收缓存中，同时将接收缓存中的一些字节交付给应用进程。 有两个点值得注意： TCP协议保证接收方收到的字节流和发送方应用进程发出的字节流完全一样 TCP不保证接收方应用进程所收到的数据块与发送方发送的数据块，具有对应大小的关系,例如，发送方应用进程交给发送方的TCP共10个数据块，但接收方的TCP可能只用了4个数据块，就把收到的字节流交付给了上层的应用进程。就是不会全部将数据交付给上层。 因此接收方的TCP应用进程必须有能力识别收到的字节流，把它还原成有意义的应用层数据 可以看出，TCP对报文的处理是以字节为单位的，也就是TCP是面向字节流的，这正是TCP实现可靠传输、流量控制、以及拥塞控制的基础 在给上层提供的服务上： UDP提供的是不可靠服务：对于发送的UDP数据报，接收方在检测到其误码后直接丢弃，不做其他操作。对于发送方发送过程中出现分组丢失，也不做处理。因此其传输数据是不一定能使接收方全部收到数据，因此是不可靠服务。 TCP提供的是可靠服务：由于TCP字传输过程中需要建立连接，通过建立的可靠信道进行传输，因此不会出现传输差错，也就是误码、丢失、乱序、重复。因此可以保证发送端发送什么，接收端接收到什么，是可靠传输。 协议首部的对比： 由于UDP不提供可靠传输的服务，因此其首部只需要在网际层的基础上添加区分端口的子节，其首部比较简单。 在TCP中，需要提供可靠传输、流量控制、拥塞控制等服务，首部比较复杂，字段比较多。 3.三次握手 准备阶段：服务器创建 传输控制块：TCP连接表 发送和接收缓存的指针 重传队列指针 当前接收 发送序列号然后客户端进入监听状态： ①客户端创建传输控制块：发送和接收缓存的指针 重传队列的指正 当前发送和传输的序列号然后客户端向服务器发送一个TCP连接请求并进入 同步已发送状态TCP连接请求 SYN置1 并且seq&#x3D;x ②服务器接收到该请求后发送一个TCP连接回应并进入 同步已接收状态并发送一个 TCP连接请求确认报文段TCP连接请求确认报文段：SYN&#x3D;1，ACK&#x3D;1，ack&#x3D;x+1，seq&#x3D;y ③客户端接收到TCP连接请求确认报文段后进入连接已经建立状态并再发送一个普通TCP确认报文段普通TCP确认报文段：ACK&#x3D;1,ack&#x3D;y+1,sex&#x3D;x+1服务器接收到普通TCP确认报文段后也进入了连接已建立状态注：TCP规定SYN设置为1的报文段不能携带数据但是要消耗掉一个序号 Problem：为什么不能两次握手？ 主要是因为防止已失效的TCP请求发送到服务器发生错误 第一个TCP连接请求发送后并没有丢失而是因为某些原因迟到了，这时候客户端再次发送给了一个TCP连接请求， 服务器收到第二个TCP连接请求后发送一个TCP连接请求确认报文段给客户端，这时候两方都通过两次握手进入了同步已连接状态并建立了连接，在传输完毕后两边关闭了该连接。 这时第一个TCP连接请求迟到到了服务器，服务器这时候又进入了连接已建立状态并发送TCP连接请求确认报文段给客户端，客户端不理睬仍为关闭状态，服务器会一直接收不到数据处于等待状态，这样就会白白浪费网络资源 3.四次挥手 ①首先TCP客户端上层应用通知下层主动关闭连接，这时候客户端会向服务器放松一个TCP连接释放报文段并进入终止等待1状态， 其中FIN&#x3D;1，ACK&#x3D;1，seq&#x3D;u（客户端上一个发送的seq+1)，ack&#x3D;v（客户端上一个接收的seq+1）， 注：FIN&#x3D;1的报文段不携带任何数据但是要消耗一个seq ②服务器接收该TCP连接释放报文段并进入关闭等待状态然后通知上层应用要关闭连接（被动关闭），然后服务器向客户端发送一个 普通TCP确认报文段并进入关闭等待状态，ACK&#x3D;1,seq&#x3D;v,ack&#x3D;u+1 注：这时候从客户端到服务器这一方向的连接就已经关闭了，这时候TCP处于半关闭状态，这段时间内客户端不再向服务器发送数据， 服务器若有数据可以继续向客户端发送，客户端会接收这些数据 ③服务器在发送了这些数据后会向客户端发送一个TCP连接释放报文段并且进入最后确认状态， 其中FIN&#x3D;1，ACK&#x3D;1，seq&#x3D;w（服务器上一个发送的seq+1），ack&#x3D;u+1，因为上一个客户端发送的seq&#x3D;u并且客户端没有再发送数据 ④客户端在接收了该TCP连接释放报文段后会进入时间等待状态，并且向服务器发送一个普通TCP确认报文段， ACK&#x3D;1，seq&#x3D;u+1，ack&#x3D;w+1，而服务器在接收到该普通TCP确认报文报文段后会关闭连接 ⑤最后客户端会进入时间等待阶段，并等待2个MSL时间后关闭连接：MSL是最长报文段寿命 Problem：户端在最终关闭前需要等待2个MSL，有必要吗？ 有：因为最后客户端向服务器发送普通TCP确认报文段，发送完后客户端直接进入关闭状态，这必然会引起服务器的超时重传， 服务器会再次发送TCP链接释放报文段，但是这时候客户端已经关闭不会理财，客户端就会持续超时重传从而无法进入关闭状态， 这样会造成网络资源的浪费 所以等待2个MSL（Maximum Segment Lifetime，RFC793建议2分钟）时间有两个作用： ①保证服务器接收到了第四次挥手，正常进入关闭状态 ②同时在2个MSL时间内，TCP连接中所有的数据会被清空，保证下一次TCP连接不会出现旧的报文段 4.实现可靠传输1）TCP流量控制 ①首先建立连接的双发按约定的窗口大小来发送数据，目前是400：应发送前三条数据，当A-&gt;B发送了三条数据后，第三条丢失 ②这时候接收方会发送一个TCP确认报文段，按最小seq来返回ack，并调整窗口大小，则现在窗口的覆盖区域应从201开始后三条：如图 ③发送窗口内还未发送的数据，当重传计数器到时候发送201报文段 ④接收方发送TCP确认报文段，这时调整窗口大小为100，来控制A的流量，此时窗口内数据仅由501-600 注：在确认数据后，接收方和发送方都会在缓存中删除已确认的数据 持续计时器（发送零窗口探测报文） 当接收方缓冲有空间可以接收数据后会发送一个新的报文段调整接收窗口，但是这条报文丢失了，为避免死锁情况： 发送方在发送窗口值被调整为0后会启动持续计数器，该计数器会在到时后发送零窗口探测报文，接收方会根据自己情况来返回接收窗口大小 2）拥塞控制慢开始、拥塞避免、快重传、快恢复 ①首先TCP传输从0开始传输报文段（慢开始算法），慢开始时候1+1,2+2,4+4，这样的方式增加 ②达到慢开始门限阈值ssthresh后开始使用拥塞避免算法，这时候拥塞窗口是以+1的方式慢慢增加 ​ 当达到某一值后发生了超时重传，说明网络可能发生了拥塞，这时候就会把慢开始门限值ssthresh&#x3D;现在的值的一半， ​ 然后令拥塞窗口值&#x3D;1开始使用慢开始算法 ③当达到新的慢开始门限阈值后开始使用慢开始算法，然后使用拥塞避免算法，若这后面的报文段出现了失序到达情况（某个报文段丢失了），服务器会发送重复确认，当客户端收到三个重复确认报文段后就会马上重传响应的报文段而不是等待重传计数器到时后重传 ④当客户端收到了三个重复确认并进行快重传后，服务器会马上执行快恢复：将慢开始ssthresh门限&#x3D;现在值的一半并开始拥塞避免 3）ARQ协议https://zhuanlan.zhihu.com/p/261152357 ①停等ARQ协议（stop-and-wait）停等ARQ协议相当于发送窗口和接收窗口大小均为1的滑动窗口协议。即发送方发送一个帧后，必须接收到一个确认帧（ACK）才能发送下一个。 原理： 发送方发送数据后会进入等待状态直到获得接收方发送的确认信息 当发送方没有接收到确认信息，那么可能有两种情况： ​ ①发送方发送的数据丢失，发送方重传计时器到时间后，发送方会重新发送该数据 ​ ②接收方发送的确认信息（ACK）丢失，发送方重传计时器到时后会重发该数据，此时接收方就会得到重复数据， ​ 接收方会直接丢弃重复数据并返回确认 优点： 原理简单，广泛运用于分组交换网络。 缺点： 较长的等待时间，使得数据传输速度低。低速传输时频道利用率较高，高速传输时频道利用率较低。 ② 回退N帧ARQ协议（Go-Back-N）连续ARQ协议（Continuous ARQ），为了克服停等ARQ协议长时间等待ACK的缺点，这个协议会连续发送一组数据包，然后再等待这些数据包的ACK。 具体实现： ①接收方对发送方的数据进行累计确认，如发送方发送了0,1,2,3,4,5条数据，可能接收方第一次确认就是确认3（说明0,1,2,3都正确到达，这里和TCP的ack有些不同，ack&#x3D;3是指已经到了的最后一个分组，不是期待的下一个分组） ②接收方的窗口值一直都是1，若接收的序列是3,1,2,0,5，那么滑动的顺序仍然是0,1,2,3，并发送ack&#x3D;3（5到了，4没到，仍然确认3） ③这时4号分组出现了数据丢失（5号没丢），那么接收方会反复发送ack&#x3D;3告知接收方，虽然接收方接收到了5号，但发送方仍然要回退到4号开始重新发送（因为3号及之前的已经被确认了） 总结： ③选择重传协议连续ARQ协议（Continuous ARQ），为了克服停等ARQ协议长时间等待ACK的缺点，这个协议会连续发送一组数据包，然后再等待这些数据包的ACK。 具体实现： ①发送方发送窗口为4，接收方接收窗口也4，发送方首先发送了0,1,2,3共4个分组，其中0,1,4三个分组都被正确接收且发送了ack，因为0,1号分组是正确顺序接收，所以发送和接收方都可以向前滑两个。 ②接收方会对每一个到达的分组都发送ack（不再是累计确认），发送窗口此时可以发送4,5号分组，并且接收窗口也正确接收了并发送了ack。发送方3,4,5号都已经被发送且接收到了ack，此时要做一个标记避免超时重传。 ③因为2号分组丢失，当重传计时器超时就会重传2号分组，接收方接收到了2号后接收方接收窗口才能向后滑，同时发送ack&#x3D;2，发送方接收到了ack&#x3D;2后发送窗口才能继续想后滑。 总结： ④回退N帧和选择重传的对比 回退N帧的接收窗口为1；选择重传接收窗口大小一般&#x3D;发送窗口大小 回退N帧累计确认，ack&#x3D;3是确认3号分组及以前的；选择重传对对每一个分组都要确认，ack&#x3D;3只代表3号分组已接收 4）保活计数器 当主机发生故障时就不能向服务器发送数据，TCP如何发现并关闭连接以免浪费网络资源： ①服务器每接收到一次来自客户端的数据就会更新启动保活计数器 ②当保活计时器到时就会发送TCP探测报文，发送10个后还未响应就会断开连接 5）总结 5.TCP首部格式 ①数据偏移：占4字节（0-15），并以4字节为单位，数据偏移*4&#x3D;首部长度 ②保留字段，保留为以后所使用，现在为0 ③SYN：三挥手阶段建立时用来同步序号 ④ACK：TCP连接建立后所有报文段都需置1 ⑤RST：置1时代表TCP连接出现异常，需要释放该次连接并重新连接 ​ RST也可用来拒绝一个非法报文段或者打开一个TCP连接（服务器） ⑥URG：置1时代表紧急指针字段有效 ⑦紧急指针：报文段格式：首部+紧急数据（有的话）+普通数据 ​ 数 据 载 荷 紧急指针会指出本报文段数据载荷部分包含了多长的紧急数据 注：发送方有紧急数据时可将紧急数据插队到发送缓存最前面并立刻封装到一个TCP报文段中发送 ⑦PSH：置1后接收方收到该报文段会尽快交付给上层应用而不是等接受缓存都填满后再上传 ⑧选项（长度可变） 最大报文段长度MSS选项：TCP报文段数据载荷部分最大长度 窗口扩大选项：扩大窗口（提高吞吐率） 时间戳选项：计算往返RTT ​ 用于处理序号超范围情况 选择确认选项： ⑨填充：保证首部字段是4倍数 五、应用层1.DHCP ①DHCP主要作用就是给网络中的主机分配配置信息，因为静态单独配置配置信息会较为麻烦，所以可以在主机的时候启动DHCP程序来 获得一个暂时的配置信息 ②DHCP使用的C&#x2F;S方式，网络层使用的是UDP协议 ​ 客户端使用的是68号，服务器使用的是67号 ③租借过程 a.寻找DHCP服务器过程：源地址是0.0.0.0（未分配IP），地址是广播地址（255.255.255.255），源端口号是68，目的端口是67 ​ 所有主机只监听了68号端口，所以不会理睬该数据，所有DHCP监听67号端口会接收该数据 b.DHCP服务器在接收了数据后会选择一个IP地址并通过ARP来检查该IP地址是否已被占用，然后将配置信息广播发送，主机会根据事务ID 来判断是否是自己需要的数据， c.主机在接收该IP地址租约信息后会再次向服务器进行确认 d.服务器会再次确认该IP地址然后广播发送，主机会通过ARP再次确认该IP未被占用，直到这时候主机才有了IP地址 ④续约过程 a.主机向DHCP发送续约，DHCP同意并返回数据扩大租期 b.主机向DHCP发送续约，DHCP不同意并返回数据，这时候主机应立即停用该IP信息并重新租借 ⑤主机不再使用IP租约，主机地址是0.0.0.0（不用租约），广播发送通知DHCP服务器 2.DNS1）层次树状的域名结构①基本结构 ②域名分类 ③树形结构 2）域名服务器和查询方式 ①递归查询（不采用） 递归查询：主机首先会查询本地域名服务器（默认配置域名服务器），然后首先查询最上层的根域名服务器，根域名服务器不会返回结果，它会根据域名递归查询其所属的顶级域名服务器，这时候可能会返回结果，也可能再次查询相应的权限域名服务器，最后再将结果层层返回。 这种方式未被采用因为会对域名服务器造成太大负担，所以不被采用 ②迭代查询（采用） 主机到本地域名服务器采用的是递归查询，其它都是迭代查询，本地域名服务器首先询问根域名服务器，根域名服务器将下一个需要查询的顶级域名服务器告诉本地DNS，本地DNS会根据信息查询对应的顶级DNS，返回的可能是结果，也有可能是下一个需要查询的权限DNS，最后本地DNS将最终结果传回给本地主机。 这样其实将查询的负担交给了本地DNS，这是现在采用的方式 ③高速缓存查询 主机和本地域名服务器都有高速缓存， 主机会首先查询高速缓存表然后再进行DNS的查询 3.应用协议1）文件传送协议FTP①ftp主要功能是文件传输：客户能将各类型文件传送到FTP服务器，客户也能从FTP服务器下载各种类型文件 ②FTP的工作原理 a.主动连接（FTP服务器主动与客户端建立数据连接） 客户端选择一个临时端口号1向FTP服务器21号端口发起连接，这次连接是用于传送控制命令的控制连接，然后客户端选择一个临时端口号2并通过控制连接来告知FTP服务器该临时端口号2，FTP服务器20号端口来与客户端的临时端口2建立数据连接，该连接用于传输数据 b.被动连接（FTP服务器被动接受客户端的数据连接） 客户端首先通过临时端口号1来主动与FTP服务器21号端口建立控制连接，服务器选择一个临时端口a并告知给客户端，客户端选择一个临时端口号2来主动与FTP服务器进行连接，FTP服务器被动接受连接 2）电子邮件 ①邮件发送协议SMTP ①用户代理TCP连接发送方SMTP服务器1，SMTP服务器TCP连接接收方SMTP服务器2，文件的传输是通过SMTP协议传输的 ②SMTP使用命令和应答的方式来传输文件 ③SMTP文件只能传输ASKII码文本数据，为实现中文、法语等传输，需要用到MIME转换： 发送方通过MIME将数据转换成ASKII文本，接收方再通过MIME将ASKII转换成原来的数据 ②邮件接收协议POP3&#x2F;IMAP都是基于TCP连接的接收协议，IMAP功能较POP3更强大 ③基于万维网的电子邮件HTTP 用户通过浏览器使用HTTP协议将邮件传输到邮箱所属的邮件服务器，邮件服务器通过SMTP协议来将文件传输到对应的邮件服务器，接收方用户通过浏览器使用HTTP协议从邮件服务器来接收邮件 3）HTTPTCP客户进程会使用80端口来与服务器建立TCP连接，并且通过该链接来发送请求报文并接收响应报文 万维网通过URL来指明英特网上任何种类的资源 ①HTTP版本​ ①HTTP&#x2F;1.0采用非持续连接方式：每次请求都要和服务器建立TCP连接，收到连接后马上关闭该连接 该种方式会在三握手的第3次握手携带HTTP请求报文并进行发送： 每次请求一个资源都需要2倍RTT，若一个网页有很多引用对象，那么就会浪费大量时间 为提高效率，浏览器会建立多个并行的TCP来与服务器建立连接并请求对象，但这样会大量占用万维网资源并增加服务器负担 ②HTTP&#x2F;1.1采用持续连接方式：万维网在发送响应报文后仍保持该条连接，这样后续客户端还可以继续发送HTTP请求 为提高效率，HTTP&#x2F;1.1持续连接还可以采用流水线方式，即在收到HTTP响应前发送多条HTTP请求，然后服务器再发回一个个HTTP响应 ②HTTP报文格式HTTP报文是面向文本的，包含了多个ASKII码串且长度不确定 ①请求报文 其中Connection:close说明该连接是收到回应后就关闭，是非持续连接，keep-alive是持续连接 Request格式 ​ 1. 请求行 ​ 请求方式&#x2F;请求url 请求协议&#x2F;版本 ​ GET&#x2F;login.html HTTP&#x2F;1.1 ​ ①请求方式： ​ HTTP协议有7中请求方式，常用的有2种 ​ GET： ​ 1. 请求参数在请求行中，在url后。 ​ 2. 请求的url长度有限制的 ​ 3. 不太安全 ​ POST： ​ 1. 请求参数在请求体中 ​ 2. 请求的url长度没有限制的 ​ 3. 相对安全 ​ ​ 2. 请求头（首部行）：客户端浏览器告诉服务器一些信息 ​ 请求头名称: 请求头值 ​ 常见的请求头： ​ ①User-Agent ​ 浏览器告诉服务器，我访问你使用的浏览器版本信息，可以在服务器端获取该头的信息，解决浏览器的兼容性问题 ​ ​ ②Referer：http://localhost/login.html ​ 告诉服务器，我(当前请求)从哪里来？ ​ 作用： ​ 1. 防盗链：防止第三方网站盗链 ​ 2. 统计工作：通过判断来查看从各处来的流量 ​ ​ 3. 请求空行 ​ 空行（CRLF），就是用于分割POST请求的请求头，和请求体的。 ​ ​ 4. 请求体(正文)： ​ 封装POST请求消息的请求参数的 ​ ​ 5.例子 ​ POST &#x2F;login.html HTTP&#x2F;1.1 （请求行） ​ Host: localhost （请求头、首部行） ​ User-Agent: Mozilla&#x2F;5.0 (Windows NT 6.1; Win64; x64; rv:60.0) Gecko&#x2F;20100101 Firefox&#x2F;60.0 ​ Accept: text&#x2F;html,application&#x2F;xhtml+xml,application&#x2F;xml;q&#x3D;0.9,&#x2F;;q&#x3D;0.8 ​ Accept-Language: zh-CN,zh;q&#x3D;0.8,zh-TW;q&#x3D;0.7,zh-HK;q&#x3D;0.5,en-US;q&#x3D;0.3,en;q&#x3D;0.2 ​ Accept-Encoding: gzip, deflate ​ Referer: http://localhost/login.html ​ Connection: keep-alive ​ Upgrade-Insecure-Requests: 1 ​ （请求空行） ​ username&#x3D;zhangsan （请求正文） ②响应报文 Response格式 ​ 1. 状态行 ​ 协议&#x2F;版本 响应状态码 状态码描述 ​ 响应状态码：服务器告诉客户端浏览器本次请求和响应的一个状态， 状态码都是3位数字 ​ 分类： ​ 1xx：服务器已收客户端消息，但没有接受完成，等待一段时间后，发送1xx多状态码 ​ 2xx：成功。代表：200 ​ 3xx：重定向。代表：302(重定向)，304(访问缓存) ​ 4xx：客户端错误。 ​ 代表： ​ 404（请求路径没有对应的资源） ​ 405：请求方式没有对应的doXxx方法 ​ 5xx：服务器端错误。代表：500(服务器内部出现异常) ​ ​ ​ 2. 响应头（首部行）： ​ 头名称： 值 ​ 常见的响应头： ​ ①Content-Type：服务器告诉客户端本次响应体数据格式以及编码格式 ​ 浏览器是通过MIME类型来判断数据类型的 ​ ②Content-disposition：服务器告诉客户端以什么格式打开响应体数据 ​ 值： ​ in-line:默认值,在当前页面内打开 ​ attachment;filename&#x3D;xxx：以附件形式打开响应体。文件下载 ​ ​ 3. 响应空行：空行CRLF ​ ​ 4. 响应体:传输的数据 ​ 5. 响应字符串格式 ​ HTTP&#x2F;1.1 200 OK （响应头） ​ Content-Type: text&#x2F;html;charset&#x3D;UTF-8（响应行） ​ Content-Length: 101 ​ Date: Wed, 06 Jun 2018 07:08:42 GMT ​ （响应空行） ​ （响应正文） ​ ​ $Title$ ​ ​ ​ hello , response ​ ​ ③cookie ①原理 ​ cookie的作用是完成一次会话中多次请求的数据共享 ​ 服务器首先通过在响应头中通过set-cookie:username&#x3D;zs来添加信息，浏览器获取到这个cookie后会将其存入浏览器内存，然后浏览器在后续向服务器发送数据的时候，会在请求头中添加cookie：username&#x3D;zs，最后服务端可以获取所有cookie值的数组 （1）编写一个新Servlet类，名称为BServlet 123456789101112@WebServlet(&quot;/bServlet&quot;)public class BServlet extends HttpServlet &#123; @Override protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; &#125; @Override protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; this.doGet(request, response); &#125;&#125; （2）在BServlet中使用request对象获取Cookie数组，遍历数组，从数据中获取指定名称对应的值 12345678910111213141516171819202122232425@WebServlet(&quot;/bServlet&quot;)public class BServlet extends HttpServlet &#123; @Override protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; //获取Cookie //1. 获取Cookie数组 Cookie[] cookies = request.getCookies(); //2. 遍历数组 for (Cookie cookie : cookies) &#123; //3. 获取数据 String name = cookie.getName(); if(&quot;username&quot;.equals(name))&#123; String value = cookie.getValue(); System.out.println(name+&quot;:&quot;+value); break; &#125; &#125; &#125; @Override protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; this.doGet(request, response); &#125;&#125; ②cookie相关 cookie存储中文 这个时候，我们可以使用之前学过的一个知识点叫URL编码，所以如果需要存储中文，就需要进行转码，具体的实现思路为: 首先将cookie进行URL编码，最后取出的cookie再通过URL解码来获取 cookie的存活时间：setMaxAge(int seconds) 为了再关闭浏览器后不消除cookie，我们可以设置一个cookie的存活时间 ④session①原理：session是基于cookie实现的 session是存储于服务器端的，也是为了实现一次会话中多次请求间的数据共享 SessionDemo1 1234567891011121314151617@WebServlet(&quot;/demo1&quot;)public class SessionDemo1 extends HttpServlet &#123; @Override protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; //存储到Session中 //1. 获取Session对象 HttpSession session = request.getSession(); System.out.println(session); //2. 存储数据 session.setAttribute(&quot;username&quot;,&quot;zs&quot;); &#125; @Override protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; this.doGet(request, response); &#125;&#125; SessionDemo2 123456789101112131415161718@WebServlet(&quot;/demo2&quot;)public class SessionDemo2 extends HttpServlet &#123; @Override protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; //获取数据，从session中 //1. 获取Session对象 HttpSession session = request.getSession(); System.out.println(session); //2. 获取数据 Object username = session.getAttribute(&quot;username&quot;); System.out.println(username); &#125; @Override protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; this.doGet(request, response); &#125;&#125; （1）demo1在第一次获取session的时候，session会有一个唯一标识id=10 （2）demo1在处理完业务并将数据存入session中后，需要通过tomcat将响应结果返回给浏览器 （3）tomcat发现在demo1使用了session对象，就会把session的唯一标识id=10当做一个cookie存，添加set-cookie:JESSIONID=10到响应头中并响应给浏览器 （4）浏览器收到响应后会把cookie:JESSIONID=10存入浏览器内存中 （5）浏览器在同一会话中访问demo2的时候，请求头中会含有cookie:JESSIONID=10并将请求发送到服务器 （6）demo2在收到该请求后，就会读取到sessionid=10，这样demo2在获取session的时候就会从服务器内存中寻找id=10的session对象，如果找到了则返回该对象，如果没找到则返回一个新的session对象 （7）当浏览器关闭后，记录了sessionid=10的cookie已经被销毁，这时候再获取session，就是一个全新的session 了 ②session相关 钝化和活化 若浏览器关闭后，内存被释放掉，session也就没有了，tomcat会通过session的钝化和活化来持久化保存session数据 钝化：在服务器正常关闭后，Tomcat会自动将Session数据写入硬盘的文件中 钝化的数据路径为:项目目录\\target\\tomcat\\work\\Tomcat\\localhost\\项目名称\\SESSIONS.ser 活化：再次启动服务器后，从文件中加载数据到Session中 数据加载到Session中后，路径中的SESSIONS.ser文件会被删除掉 销毁session （1）web.xml中配置无操作自动销毁时间，默认30分钟 1234567891011&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;web-app xmlns=&quot;http://xmlns.jcp.org/xml/ns/javaee&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_3_1.xsd&quot; version=&quot;3.1&quot;&gt; &lt;session-config&gt; &lt;session-timeout&gt;100&lt;/session-timeout&gt; &lt;/session-config&gt;&lt;/web-app&gt; ​ （2）调用Session对象的invalidate()进行销毁 1234//1. 获取Session对象 HttpSession session = request.getSession();//2.销毁 session.invalidate(); ⑤万维网中的缓存（代理服务器） ①过程 当使用校园网来访问internet上某一个资源的时候，会先请求代理服务器，若代理服务器有所请求的对象会直接直接返回给用户 若代理服务器没有，则代理服务器会向原始服务器发送请求，在得到响应后会先Web缓存一份再将相应发送给用户 ②修改时间和有效日期 如果原始服务器有所修改，那么代理服务器返回给用户主机的就是旧的响应，为避免这种情况，我们会用到修改时间和有效日期 （1）若修改时间未到期，那么代理服务器会将web缓存中的响应返回给用户主机（可能也是旧的） （2）若修改时间到期，那么代理服务器会向原始服务器发送请求看是否有改变，如果没改变则更新修改时间并直接返回缓存的响应 （3）若修改时间到期，且原始服务器有修改 原始服务器会在响应中返回最新的数据，代理服务器会更新修改时间并将最新数据web缓存然后发给用户主机 ⑥http1.1 和 http2.0https://www.bilibili.com/video/BV1p64y1r7XF?spm_id_from=333.337.search-card.all.click&amp;vd_source=65b93dcb67042b5352bb42ae6bcb0ac1 最早http建立的连接无法复用，就是一次数据传输之后立即断开，后来使用keep-alive保活。但是请求只有在服务应答之后才能发送后续请求在客户端形成阻塞。后来使用管道化http pipeling技术，使得客户端可以一次发送多个请求，但是对于服务端的第一个资源无法返回的时候（客户端没确认），也会形成服务端阻塞。虽然可以服务端和客户端建立多个连接，但是也需要多次3次握手。http1.x的问题在于：1、队头阻塞问题：无论是服务端还有客户端都会有请求资源的阻塞问题。2、冗余头部字段：http1.x的头部都采用文本格式，未压缩，并且每次都会发送重复的字段。3、只有客户端发送请求，服务方才会应答。 http2.x改进：1、采用二进制协议编码，传输的是二进制格式传输数据。响应分为header和data两个帧。 每个frame中都有一个streamId，frame是乱序到达的 1235frame到了，4frame丢失了，如果1235能组成一个消息，可以先处理1235 2、头部字段会保存在客户端和服务端，动态添加，避免重复发送。 发送的时候只发送关键信息的key，value在客户端和服务端都存有索引表（由key找到对应的value） 3、多路复用：支持多个请求在一个连接中完成，也不会由因为某个包的延迟而导致整体链路的阻塞。 将3个请求分成3个流，再将消息封装成帧，然后利用同一个tcp连接发送 4、服务端会主动根据请求分析，客户端是否需要其他资源，主动发送。 5、由于使用了单个TCP连接，如果该TCP出问题，可用性较低。1.x是有多个TCP连接的，如果1个出问题还可以使用其它的 ⑦http3.0 尽管2.0以拆帧的方式来试图解决阻塞问题，但是还是基于TCP的可靠连接，TCP是要求有序的，所以仍然会有阻塞问题 3.0弃用了TCP协议，而是采用了quic协议来解决阻塞问题 ⑧常见状态码 https://xiaolincoding.com/network/2_http/http_interview.html#http-%E5%B8%B8%E8%A7%81%E7%9A%84%E7%8A%B6%E6%80%81%E7%A0%81%E6%9C%89%E5%93%AA%E4%BA%9B 原文作者，此处仅方便回顾 1xx 类状态码属于提示信息，是协议处理中的一种中间状态，实际用到的比较少。 2xx 类状态码表示服务器成功处理了客户端的请求，也是我们最愿意看到的状态。 「200 OK」是最常见的成功状态码，表示一切正常。如果是非 HEAD 请求，服务器返回的响应头都会有 body 数据。 「204 No Content」也是常见的成功状态码，与 200 OK 基本相同，但响应头没有 body 数据。 「206 Partial Content」是应用于 HTTP 分块下载或断点续传，表示响应返回的 body 数据并不是资源的全部，而是其中的一部分，也是服务器处理成功的状态。 3xx 类状态码表示客户端请求的资源发生了变动，需要客户端用新的 URL 重新发送请求获取资源，也就是重定向。 「301 Moved Permanently」表示永久重定向，说明请求的资源已经不存在了，需改用新的 URL 再次访问。 「302 Found」表示临时重定向，说明请求的资源还在，但暂时需要用另一个 URL 来访问。 301 和 302 都会在响应头里使用字段 Location，指明后续要跳转的 URL，浏览器会自动重定向新的 URL。 「304 Not Modified」不具有跳转的含义，表示资源未修改，重定向已存在的缓冲文件，也称缓存重定向，也就是告诉客户端可以继续使用缓存资源，用于缓存控制。 4xx 类状态码表示客户端发送的报文有误，服务器无法处理，也就是错误码的含义。 「400 Bad Request」表示客户端请求的报文有错误，但只是个笼统的错误。 「403 Forbidden」表示服务器禁止访问资源，并不是客户端的请求出错。 「404 Not Found」表示请求的资源在服务器上不存在或未找到，所以无法提供给客户端。 5xx 类状态码表示客户端请求报文正确，但是服务器处理时内部发生了错误，属于服务器端的错误码。 「500 Internal Server Error」与 400 类型，是个笼统通用的错误码，服务器发生了什么错误，我们并不知道。 「501 Not Implemented」表示客户端请求的功能还不支持，类似“即将开业，敬请期待”的意思。 「502 Bad Gateway」通常是服务器作为网关或代理时返回的错误码，表示服务器自身工作正常，访问后端服务器发生了错误。 「503 Service Unavailable」表示服务器当前很忙，暂时无法响应客户端，类似“网络服务正忙，请稍后重试”的意思。 4）HTTPS","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://example.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}]},{"title":"git","slug":"git - 副本","date":"2022-04-03T11:17:48.000Z","updated":"2022-07-30T03:56:37.006Z","comments":true,"path":"2022/04/03/git - 副本/","link":"","permalink":"http://example.com/2022/04/03/git%20-%20%E5%89%AF%E6%9C%AC/","excerpt":"git的相关配置及操作","text":"git的相关配置及操作 1. 为常用指令配置别名有些常用的指令参数非常多，每次都要输入好多参数，我们可以使用别名。 打开用户目录，创建 .bashrc 文件 部分windows系统不允许用户创建点号开头的文件，可以打开gitBash,执行 touch ~&#x2F;.bashrc 在 .bashrc 文件中输入如下内容： 12345#用于输出git提交日志 alias git-log=&#x27;git log --pretty=oneline --all --graph --abbrev-commit&#x27; #用于输出当前目录所有文件及基本信息 alias ll=&#x27;ls -al&#x27;1234 打开gitBash，执行 source ~&#x2F;.bashrc 2. 解决GitBash乱码问题 打开GitBash执行下面命令 12git config --global core.quotepath false1 ${git_home}&#x2F;etc&#x2F;bash.bashrc 文件最后加入下面两行 123export LANG=&quot;zh_CN.UTF-8&quot; export LC_ALL=&quot;zh_CN.UTF-8&quot;12 3. 获取本地仓库要使用Git对我们的代码进行版本控制，首先需要获得本地仓库 1）在电脑的任意位置创建一个空目录（例如test）作为我们的本地Git仓库 2）进入这个目录中，点击右键打开Git bash窗口 3）执行命令git init 4）如果创建成功后可在文件夹下看到隐藏的.git目录。 4. 基础操作指令Git工作目录下对于文件的修改(增加、删除、更新)会存在几个状态，这些修改的状态会随着我们执行Git 的命令而发生变化。 工作区，就是平时存放项目代码的地方。 git add (工作区 –&gt; 暂存区) git add .添加所有文件、文件夹和子文件夹，包括.gitignore和以点开头的任何其他内容； git commit (暂存区 –&gt; 本地仓库) 1）git status 作用：查看的修改的状态（暂存区、工作区） 命令形式：git status 新创建的文件是未跟踪状态 即将被提交的意思 提交完后显示缓冲区没有东西可以提交了 新建文件是显示new file 修改文件就是实现modified: 2）git add 作用：添加工作区一个或多个文件的修改到暂存区 命令形式：git add 单个文件名|通配符 将所有修改加入暂存区：git add . 3）git commit 作用：提交暂存区汇总所有内容到本地仓库的当前分支 命令形式：git commit -m ‘注释内容’ 提交时候添加的备注会被放到日志中 提交完后显示缓冲区没有东西可以提交了 4）git log在3.1.3中配置的别名 git-log 就包含了这些参数，所以后续可以直接使用指令 git-log 作用:查看提交记录 命令形式：git log [option] options all 显示所有分支 pretty&#x3D;oneline 将提交信息显示为一行 abbrev-commit 使得输出的commitId更简短 graph 以图的形式显示 查看log我们一般都是会加上上面全部的参数的，这样显示更美观有序，我们在上面给这个指令设置了别名 123#用于输出git提交日志 alias git-log=&#x27;git log --pretty=oneline --all --graph --abbrev-commit&#x27; 12 我们只要使用git -log命令就好了 ，简单命令为git log 提交时候添加的备注会被放到日志中 5. 版本回退撤回到之前的某个操作，他回去删除我们撤回到位置之后的版本 作用：版本切换 命令形式：git reset –hard commitID commitID 可以使用 git-log 或 git log 指令查看 如何查看已经删除的记录？ git reflog 这个指令可以看到已经删除的提交记录 我们可以在reflog里面知道删除文件的id，我们可以直接使用命令git reset –hard commitID 还原 所以git reset –hard commitID既可以做版本回退，也可以做版本还原 6. 添加文件至忽略列表一般我们总会有些文件无需纳入Git 的管理，也不希望它们总出现在未跟踪文件列表。 通常都是些自动 生成的文件，比如日志文件，或者编译过程中创建的临时文件等。 在这种情况下，我们可以在工作目录 中创建一个名为 .gitignore 的文件（文件名称固定），列出要忽略的文件模式。下面是一个示例： 这样后缀为.a的文件就不会被加到缓冲区中，这样git就不会去处理这些文件了，一般.gitignore文件公司会给 7. 分支1）基本指令查看本地分支 命令：git branch *号表示所在的分支 ②创建本地分支 命令：git branch 分支名 创建的新分支会建立在当前分支的版本之上，所以新建的分支会有当前分支的内容 ③切换分支(checkout) 命令：git checkout 分支名 我们还可以直接切换到一个不存在的分支（创建并切换） 命令：git checkout -b 分支名 ④合并分支(merge)注意：分支上的内容必须先提交,才能切换分支 一个分支上的提交可以合并到另一个分支 命令：git merge 分支名称 在每个人都开发完后就将所有的代码汇总到一起，此时就要执行分支的合并操作 master使我们的主线，我们一般将分支合并到主线上面去 步骤：切换到master分支，然后执行合并命令，执行完后，分支上的资源、文件就会被合并到主线上面去 第三行是master分支创建了ignore文件，然后第二行执行dev01分支合并到master分支的操作，最后head指向了主分支 例如将test01分支合并到master分支上，master分支上就会加上test01分支上的内容 分支岔开表示有多个分支在修改同一个文件 ⑤删除分支不能删除当前分支，只能删除其他分支 git branch -d b1 删除分支时，需要做各种检查 git branch -D b1 不做任何检查，强制删除 小d删除了就使用D，一般使用d就够了 我们去删除没有合并的分支的时候就会出现删除不了的情况，此时就可以使用D 2）解决冲突当我们合并分支后，两个或者多个分支对同一个文件的同一个地方进行修改的时候（不是同一个地方是不会出现冲突的 ），此时git就不知道要取哪个分支修改的值，是取a分支修改的值，还是取b分支修改的值呢，此时就产生了冲突 冲突的报错 此时的文件样子 第一个count值表示的是当前分支master修改的值 第二个count值是在dev分支修改的值 当两个分支上对文件的修改可能会存在冲突，例如同时修改了同一个文件的同一行，这时就需要手动解 决冲突，解决冲突步骤如下： 其实我们就是直接手动去删除文件中的一个分支的代码，留下一个分支，这样就不会冲突了 处理文件中冲突的地方 将解决完冲突的文件加入暂存区(add) 提交到仓库(commit) 冲突部分的内容处理如下所示： 3）开发中分支使用原则与流程几乎所有的版本控制系统都以某种形式支持分支。 使用分支意味着你可以把你的工作从开发主线上分离 开来进行重大的Bug修改、开发新的功能，以免影响开发主线。 在开发中，一般有如下分支使用原则与流程： master （生产） 分支 线上分支，主分支，中小规模项目作为线上运行的应用对应的分支； develop（开发）分支 是从master创建的分支，一般作为开发部门的主要开发分支，如果没有其他并行开发不同期上线 要求，都可以在此版本进行开发，阶段开发完成后，需要是合并到master分支,准备上线。 例如我们要开发新功能，我们要可以在develop分支上在建一个分支，新功能一般叫做feature分支，开发完以后在合并到 develop分支上面去，而不是直接提交到master分支，最后项目做完了develop在合并到master分支上 develop和master分支是不可删除的 feature&#x2F;xxxx分支（用完可删） 从develop创建的分支，一般是同期并行开发，但不同期上线时创建的分支，分支上的研发任务完 成后合并到develop分支，用完后可删除。 hotfifix&#x2F;xxxx分支， 从master派生的分支，一般作为线上bug修复使用，修复测试完成后需要合并到master、test、develop分支。 还有一些其他分支，在此不再详述，例如test分支（用于代码测试）、pre分支（预上线分支）等等。 8.操作远程仓库1）添加远程仓库此操作是先初始化本地库，然后与已创建的远程库进行对接。 我们要将本地仓库和远程仓库连接起来，一般一个本地仓库对应一个远程仓库远侧，远程仓库默认名为origin 命令： git remote add &lt;远端名称&gt; &lt;仓库路径SSH&gt; 远端名称，默认是origin，取决于远端服务器设置 因为一个本地仓库可能会关联多个远程仓库，所以需要取名字 仓库路径，从远端服务器获取此SSH， 例如: git remote add origin &#103;&#x69;&#x74;&#x40;&#x67;&#105;&#x74;&#x65;&#x65;&#x2e;&#99;&#111;&#x6d;:czbk_zhang_meng&#x2F;git_test.git 2）查看远程仓库 命令：git remote 3）推送到远程仓库 命令：git push [-f] [–set-upstream] [远端名称] [本地分支名]:[远端分支名] ] 如果远程分支名和本地分支名称相同，则可以只写本地分支 本来是：git push origin master ：master 表示将本地仓库的master分支提交到远程仓库的master分支 git push origin master 这里表示将本地仓库当前master分支的内容推到远程仓库上面去 -f 表示强制覆盖，远端可能有修改，但是以此次push为准 –set-upstream 推送到远端的同时并且建立起和远端分支的关联关系。 第一次的时候需要建立这种关系，然后就可以直接git push了 git push –set-upstream origin master 如果当前分支已经和远端分支关联，则可以省略分支名和远端名。 git push 将master分支推送到已关联的远端分支。 [-f] 表示强制覆盖远程仓库的内容 4） 本地分支与远程分支的关联关系 查看关联关系我们可以使用 git branch -vv 命令 5）从远程仓库克隆 如果已经有一个远端仓库，我们可以直接clone到本地。 命令: git clone &lt;仓库路径&gt; [本地目录] 本地目录可以省略，会自动生成一个目录，就是SSH后面那部分 ​ 左边是我们上传的仓库，右边是克隆下来的仓库 ​ 不同的主机都把修改完了版本资源放在远程仓库上，然后其他人都是克隆，这样就可以实现不同主机之间的数据同步了，数据都是一样的 ​ 克隆一般只会执行一次，就是在你进去公司的时候，别人提交了以后，我们不需要去克隆整个仓库，仓库是很大的，克隆也需要花很多时间，所以要去远程仓库中抓取我们要的版本信息，就是那些别人新增加的提交 6）从远程仓库中抓取和拉取 远程分支和本地的分支一样，我们可以进行merge操作，只是需要先把远端仓库里的更新都下载到本地，再进行操作。 抓取 命令：git fetch [remote name] [branch name] 抓取指令就是将仓库里的更新都抓取到本地，不会进行合并，不合并本地仓库就是没有更新，此时还没有拿到远程仓库的内容，合并后才会拿到更新的内容 如果不指定远端名称和分支名，则抓取所有分支。 拉取 命令：git pull [remote name] [branch name] 拉取指令就是将远端仓库的修改拉到本地并自动进行合并，等同于fetch+merge 如果不指定远端名称和分支名，则抓取所有并更新当前分支。 只去执行抓取命令 git fetch，本地仓库是没有远程仓库来的内容的 此时的test1中没有test06，将origin&#x2F;master合并到本地仓库的master 此时test1中就有test06文件了 所以我们一般执行拉取命令 直接执行git pull 此时test1中有直接拿到文件test07 为什么将远程仓库更新的资源抓取到本地要合并呢？ 总结：push的时候要使远程仓库的更新是最新的，就是最后一个修改的版本要使远程仓库的，pull要使本地仓库的更新是最新的，就是最后一个修改的版本要使本地仓库的 因为我们此时我们获取到的是远程仓库的版本更新，但是我们本地的版本不是最新的，也就是说此时我们本地和远程仓库不是同步的，所以我们要将远端拿到的修改合并到本地仓库的master上，使得本地的版本修改变为最新的 发现此时本地的修改和远程仓库的修改同步了 在test01这个本地仓库进行一次提交并推送到远程仓库 在另一个仓库将远程提交的代码拉取到本地仓库 7）解决合并冲突 我们要更新远程仓库的资源时，先要获取此时远程仓库的资源后，在合并到自己的master分支中，然后再上传到远程仓库上 在一段时间，A、B用户修改了同一个文件，且修改了同一行位置的代码，此时会发生合并冲突。 A用户在本地修改代码后优先推送到远程仓库，此时B用户在本地修订代码，提交到本地仓库后，也需要 推送到远程仓库，此时B用户晚于A用户，故需要先拉取远程仓库的提交，经过合并后才能推送到远端分 支,如下图所示。 在B用户拉取代码时，因为A、B用户同一段时间修改了同一个文件的相同位置代码，故会发生合并冲 突。 就是b在更新一个资源之前，有一个a在b之前率先改掉了这个资源，此时就会出现分支冲突的问题，git不知道是要取a修改的值，还是b修改的值，此时就要我们手动去对应文件里去修改，到底要保留哪一个 远程分支也是分支，所以合并时冲突的解决方式也和解决本地分支冲突相同相同 origin：https://blog.csdn.net/a18307096730/article/details/124586216?spm=1001.2014.3001.5502","categories":[{"name":"git","slug":"git","permalink":"http://example.com/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"http://example.com/tags/git/"}]},{"title":"Spring Boot基本使用","slug":"spring Boot基本使用","date":"2022-04-03T04:06:12.000Z","updated":"2022-08-18T04:32:43.745Z","comments":true,"path":"2022/04/03/spring Boot基本使用/","link":"","permalink":"http://example.com/2022/04/03/spring%20Boot%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","excerpt":"parent、starter、引导类、内嵌tomcat、yml基础配置、第三方技术整合","text":"parent、starter、引导类、内嵌tomcat、yml基础配置、第三方技术整合 1.简化操作1）parent①spring initializr ​ ①boot继承了②starter-parent，而②starter-parent中又继承了③boot-dependencies，在③中定义了一系列**properties用于版本管理，又定义了dependencyManagement**用于坐标管理，所以我们可以很方便的定义使用依赖而不用在意版本。 ②阿里云 阿里云则是直接import过来了，效果是一样的（继承只能是一次） 2）starter①使用当我们使用某一种技术的时候，导入的往往是一系列组合，所以我们可以将这一类组合进行一个整合做成一个starter，如starter-web web-starter的version则是在parent中定义的 注意： ​ SpringBoot官方给出了好多个starter的定义，方便我们使用，而且名称都是如下格式 1命名规则：spring-boot-starter-技术名称 所以我们在使用某种技术的时候，首先去找该技术的starter，如果没有则自己写 如druid： ②starter与parent的区别 starter：是一个坐标中定了若干个坐标，以前写多个的，现在写一个，是用来减少依赖配置的书写量的 parent：是定义了几百个依赖版本号，以前写依赖需要自己手工控制版本，现在由SpringBoot统一管理，这样就不存在版本冲突了，是用来减少依赖冲突的 3）引导类当前这个类运行后就会产生一个Spring容器对象，并且可以将这个对象保存起来，通过容器对象直接操作Bean。 123456789@SpringBootApplicationpublic class Springboot0101QuickstartApplication &#123; public static void main(String[] args) &#123; ConfigurableApplicationContext ctx = SpringApplication.run(Springboot0101QuickstartApplication.class, args); BookController bean = ctx.getBean(BookController.class); System.out.println(&quot;bean======&gt;&quot; + bean); &#125;&#125; ​ 通过上述操作不难看出，其实SpringBoot程序启动还是创建了一个Spring容器对象。这个类在SpringBoot程序中是所有功能的入口，称这个类为引导类。 ​ 作为一个引导类最典型的特征就是当前类上方声明了一个注解**@SpringBootApplication**，注意默认扫描的路径是当前package及子包 4）内嵌tomcat①基本​ tomcat运行起来也是对象，如果是对象，那Spring容器是用来管理对象的，tomcat服务器运行其实就是以对象的形式在Spring容器中运行的。 ②内置服务器SpringBoot提供了3款内置的服务器： tomcat(默认)：apache出品，粉丝多，应用面广，负载了若干较重的组件 jetty：更轻量级，负载性能远不及tomcat undertow：负载性能勉强跑赢tomcat 2.基础配置1）三种格式 application.properties（properties格式） 12server.port=80spring.main.banner-mode=off application.yml（yml格式，主流） 12345server: port: 81logging: level: root: debug application.yaml（yaml格式） 12server: port: 82 ​ 最终端口为80，同时其他两条配置也生效了。即每个配置文件中的项都会生效，只不过如果多个配置文件中有相同类型的配置会优先级高的文件覆盖优先级的文件中的配置。如果配置项不同的话，那所有的配置项都会生效。 总结 配置文件间的加载优先级 properties（最高）&gt; yml &gt; yaml（最低） 不同配置文件中相同配置按照加载优先级相互覆盖，不同配置文件中不同配置全部保留 ：12,13,14，最终1234都会加载 2）yml的书写①基本语法12345678boolean: TRUE #TRUE,true,True,FALSE,false，False均可float: 3.14 #6.8523015e+5 #支持科学计数法int: 123 #0b1010_0111_0100_1010_1110 #支持二进制、八进制、十六进制null: ~ #使用~表示nullstring: HelloWorld #字符串可以直接书写string2: &quot;Hello World&quot; #可以使用双引号包裹特殊字符date: 2018-02-17 #日期必须使用yyyy-MM-dd格式datetime: 2018-02-17T15:02:31+08:00 #时间和日期之间使用T连接，最后使用+代表时区 12345678910111213141516171819202122232425262728enterprise: name: itcast age: 16 subject: - Java - 前端 - 大数据subject: - Java - 前端 - 大数据 #数组书写缩略格式1likes: [Java,前端,大数据] #数组书写缩略格式2users: #对象数组格式1 - name: Tom age: 4 - name: Jerry age: 5users: #对象数组格式2 - name: Tom age: 4 - name: Jerry age: 5 users2: [ &#123; name:Tom , age:4 &#125; , &#123; name:Jerry , age:5 &#125; ] #对象数组缩略格式 总结： yaml语法规则 大小写敏感 属性层级关系使用多行描述，每行结尾使用冒号结束 使用缩进表示层级关系，同层级左侧对齐，只允许使用空格（不允许使用Tab键） 属性值前面添加空格（属性名与属性值之间使用冒号+空格作为分隔） #号 表示注释 注意属性名冒号后面与数据之间有一个空格 字面值、对象数据格式、数组数据格式 ②yml内部的数据引用在书写yaml数据时，经常出现如下现象，比如很多个文件都具有相同的目录前缀 12345center: dataDir: /usr/local/fire/data tmpDir: /usr/local/fire/tmp logDir: /usr/local/fire/log msgDir: /usr/local/fire/msgDir 或者 12345center: dataDir: D:/usr/local/fire/data tmpDir: D:/usr/local/fire/tmp logDir: D:/usr/local/fire/log msgDir: D:/usr/local/fire/msgDir ​ 这个时候可以使用引用格式来定义数据，其实就是搞了个变量名，然后引用变量了，格式如下： 1234567baseDir: /usr/local/firecenter: dataDir: $&#123;baseDir&#125;/data tmpDir: $&#123;baseDir&#125;/tmp logDir: $&#123;baseDir&#125;/log msgDir: $&#123;baseDir&#125;/msgDir ​ ③转义问题还有一个注意事项，在书写字符串时，如果需要使用转义字符，需要将数据字符串使用双引号包裹起来 lession: Spring**\\tboot\\n**lesson 这是不会解析的，会将整体看成一个字符串 lesson: “Spring**\\tboot\\n**lesson” 当加上””，其中的的转义内容就会被解析 3）yml数据的读取①读取单一数据 ②读取全部数据​ SpringBoot提供了一个对象，能够把所有的数据都封装到这一个对象中，这个对象叫做Environment，使用自动装配注解可以将所有的yaml数据封装到这个对象中 ​ 数据封装到了Environment对象中，获取属性时，通过Environment的接口操作进行，具体方法时getProperties（String），参数填写属性名即可： 使用Environment对象封装全部配置信息 使用@Autowired自动装配数据到Environment对象中 ③读取对象数据​ @ConfigurationProperties可以将一组yaml对象数据封装一个Java对象中。首先定义一个对象，并将该对象纳入Spring管控的范围，也就是定义成一个bean，然后使用注解 @ConfigurationProperties 指定该对象加载哪一组yaml中配置的信息。 ​ 这个@ConfigurationProperties必须告诉他加载的数据前缀是什么，这样指定前缀下的所有属性就封装到这个对象中。数据属性名要与对象的变量名一一对应，不然没法封装。其实以后如果你要定义一组数据自己使用，就可以先写一个对象，然后定义好属性，并在yml中配置即可。 ④yaml文件中的数据引用​ 如果你在书写yaml数据时，经常出现如下现象，比如很多个文件都具有相同的目录前缀 12345center: dataDir: /usr/local/fire/data tmpDir: /usr/local/fire/tmp logDir: /usr/local/fire/log msgDir: /usr/local/fire/msgDir ​ 或者 12345center: dataDir: D:/usr/local/fire/data tmpDir: D:/usr/local/fire/tmp logDir: D:/usr/local/fire/log msgDir: D:/usr/local/fire/msgDir ​ 这个时候你可以使用引用格式来定义数据，其实就是搞了个变量名，然后引用变量了，格式如下： 123456baseDir: /usr/local/firecenter: dataDir: $&#123;baseDir&#125;/data tmpDir: $&#123;baseDir&#125;/tmp logDir: $&#123;baseDir&#125;/log msgDir: $&#123;baseDir&#125;/msgDir ​ 还有一个注意事项，在书写字符串时，如果需要使用转义字符，需要将数据字符串使用双引号包裹起来 1lesson: &quot;Spring\\tboot\\nlesson&quot; 3.第三方技术整合整合过程很简单： ①导入对应starter ②修改相应配置 ③直接使用 1）Junit①spring整合123456789101112131415//1.加载spring整合junit专用的类运行器@RunWith(SpringJUnit4ClassRunner.class)//2.指定对应的配置信息@ContextConfiguration(classes = SpringConfig.class)public class AccountServiceTestCase &#123; //3.注入你要测试的对象 @Autowired private AccountService accountService; @Test public void testGetById()&#123; //执行要测试的对象对应的方法 System.out.println(accountService.findById(2)); &#125;&#125; ②boot整合12345678910111213//1.表示该类为test类@SpringBootTestclass Springboot04JunitApplicationTests &#123; //2.注入你要测试的对象 @Autowired private BookDao bookDao; @Test void contextLoads() &#123; //执行要测试的对象对应的方法 bookDao.save(); System.out.println(&quot;two...&quot;); &#125;&#125; 注意： ​ 如果test类的包名和引导类的包名不同（不在一个包下），那么引导类就无法扫描到该test类，有两种配置可以解决 在@SpringBootTest指明 12345//指明引导类@SpringBootTest(classes = Springboot04JunitApplication.class)class Springboot04JunitApplicationTests &#123;&#125; 沿用spring的老方法 12345@SpringBootTest@ContextConfiguration(classes = Springboot04JunitApplication.class)class Springboot04JunitApplicationTests &#123;&#125; ③原理​ 运行test类的时候，test类会在自己的包下找相应的带了@SpringBootConfiguration的配置类，而@SpringBootApplication就含有该注解： 2）整合MyBatis①spring整合a）springConfig Spring核心配置： 123456@Configuration@ComponentScan(&quot;com.itheima&quot;)@PropertySource(&quot;jdbc.properties&quot;)@Import(&#123;dbcconfig.class, Mybatisconfig.class&#125;)public class SpringConfig &#123;&#125; b）jdbcConfig（druid） 123456789101112131415161718192021@Configurationpublic class JdbcConfig &#123; @Value(&quot;$&#123;jdbc.driver&#125;&quot;) private String driver; @Value(&quot;$&#123;jdbc.url&#125;&quot;) private String url; @Value(&quot;$&#123;jdbc.username&#125;&quot;) private String userName; @Value(&quot;$&#123;jdbc.password&#125;&quot;) private String password; @Bean(&quot;dataSource&quot;) public DataSource dataSource()&#123; DruidDataSource ds = new DruidDataSource(); ds.setDriverClassName(driver); ds.setUrl(url); ds.setUsername(userName); ds.setPassword(password); return ds; &#125;&#125; 数据库连接信息（properties格式） 1234jdbc.driver=com.mysql.jdbc.Driverjdbc.url=jdbc:mysql://localhost:3306/spring_db?useSSL=falsejdbc.username=rootjdbc.password=root c）MybatisConfig MyBatis要交给Spring接管的bean： 123456789101112131415161718192021222324//定义mybatis专用的配置类@Configurationpublic class MyBatisConfig &#123; //定义创建SqlSessionFactory对应的bean @Bean public SqlSessionFactoryBean sqlSessionFactory(DataSource dataSource)&#123;//自动注入dataSource //SqlSessionFactoryBean是由mybatis-spring包提供的，专用于整合用的对象 SqlSessionFactoryBean sfb = new SqlSessionFactoryBean(); //设置数据源替代原始配置中的environments的配置 sfb.setDataSource(dataSource); //设置类型别名替代原始配置中的typeAliases的配置 sfb.setTypeAliasesPackage(&quot;com.itheima.domain&quot;); return sfb; &#125; //定义加载所有的映射配置：告诉mapper类在哪儿 @Bean public MapperScannerConfigurer mapperScannerConfigurer()&#123; MapperScannerConfigurer msc = new MapperScannerConfigurer(); msc.setBasePackage(&quot;com.itheima.dao&quot;); return msc; &#125;&#125; ②boot整合步骤①：创建模块时勾选要使用的技术，MyBatis，由于要操作数据库，还要勾选对应数据库 步骤②：配置数据源相关信息（这个格式是boot定义的） 1234567#配置dataSource相关信息spring: datasource: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:3306/ssm_db username: root password: root 步骤③：配置mapper 实体类 123456public class Book &#123; private Integer id; private String type; private String name; private String description;&#125; 映射接口（Dao） 12345@Mapperpublic interface BookDao &#123; @Select(&quot;select * from tbl_book where id = #&#123;id&#125;&quot;) public Book getById(Integer id);&#125; 注意dao接口需要加@Mapper： https://blog.csdn.net/z828849eser/article/details/87561407 ③两个小问题问题1：mysql8.0的时区问题 ​ 当前使用的SpringBoot版本是2.5.4，对应的坐标设置中Mysql驱动使用的是8x版本。当SpringBoot2.4.3（不含）版本之前会出现一个小BUG，就是MySQL驱动升级到8以后要求强制配置时区，如果不设置会出问题。解决方案很简单，驱动url上面添加上对应设置就行了 1234567#2.配置相关信息spring: datasource: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:3306/ssm_db?serverTimezone=UTC #在这里设置时区的信息 username: root password: root ​ 这里设置的UTC是全球标准时间，是英国时间，中国处在东八区，需要在这个基础上加上8小时，这样才能和中国地区的时间对应的，也可以修改配置不写UTC，写Asia&#x2F;Shanghai也可以解决这个问题。 1234567#2.配置相关信息spring: datasource: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:3306/ssm_db?serverTimezone=Asia/Shanghai #在这里设置时区的信息 username: root password: root ​ 永久解决的办法是可以去修改mysql中的配置文件mysql.ini，在mysqld项中添加default-time-zone&#x3D;+8:00也可以解决这个问题 问题2：com.mysql.jdbc.Driver过时问题 数据库驱动过时的警告，根据提示修改配置即可，弃用**com.mysql.jdbc.Driver，换用com.mysql.cj.jdbc.Driver** 12Loading class `com.mysql.jdbc.Driver&#x27;. This is deprecated. The new driver class is`com.mysql.cj.jdbc.Driver&#x27;. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary. 3）整合MyBatis-plus①boot整合步骤①：导入对应的starter 12345&lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.4.3&lt;/version&gt;&lt;/dependency&gt; starter命名规则： starter所属 命名规则 示例 官方提供 spring-boot-starter-技术名称 spring-boot-starter-web spring-boot-starter-test 第三方提供 第三方技术名称-spring-boot-starter druid-spring-boot-starter 第三方提供 第三方技术名称-boot-starter（第三方技术名称过长，简化命名） mybatis-plus-boot-starter 注意： ​ 目前spring boot并未收录该技术，所以需要我们手动去maven.repository中找 步骤②：配置数据源相关信息 123456spring: datasource: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:3306/ssm_db username: root password: root 步骤③：配置mapper 映射接口（Dao） 123@Mapperpublic interface BookDao extends BaseMapper&lt;Book&gt; &#123;&#125; ②表前缀名问题目前数据库的表名定义规则是tbl_模块名称，为了能和实体类相对应，需要配置application.yml文件 添加如下配置即可，设置所有表名的通用前缀名： 1234mybatis-plus: global-config: db-config: table-prefix: tbl_ #设置所有表的通用前缀名称为tbl_ ③主键自增问题​ MP技术默认的主键生成策略为雪花算法，生成的主键ID长度较大，和mysql的自增设定不相符，需要配置一下使MP使用数据库的主键生成策略， 在application.yml中添加对应配置即可，具体如下： 12345678910111213141516server: port: 80spring: datasource: druid: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:3306/ssm_db?serverTimezone=UTC username: root password: rootmybatis-plus: global-config: db-config: table-prefix: tbl_ #设置表名通用前缀 id-type: auto #设置主键id字段的生成策略为参照数据库设定的策略，当前数据库设置id生成策略为自增 ④log配置查看MP运行日志 ​ 在进行数据层测试的时候，因为基础的CRUD操作均由MP给我们提供了，所以就出现了一个局面，开发者不需要书写SQL语句了，一切的一切都是黑盒的，我们需要一种方式查看执行过程。 ​ SpringBoot整合MP的时候充分考虑到了这点，通过配置的形式就可以查阅执行期SQL语句，配置如下 1234567mybatis-plus: global-config: db-config: table-prefix: tbl_ id-type: auto #主键自增方式 configuration: log-impl: org.apache.ibatis.logging.stdout.StdOutImpl #打印输出到控制台 ​ 再来看运行结果，此时就显示了运行期执行SQL的情况。 12345678910111213141516171819202122Creating a new SqlSessionSqlSession [org.apache.ibatis.session.defaults.DefaultSqlSession@2c9a6717] was not registered for synchronization because synchronization is not activeJDBC Connection [com.mysql.cj.jdbc.ConnectionImpl@6ca30b8a] will not be managed by Spring==&gt; Preparing: SELECT id,type,name,description FROM tbl_book==&gt; Parameters: &lt;== Columns: id, type, name, description&lt;== Row: 1, 计算机理论, Spring实战 第5版, Spring入门经典教程，深入理解Spring原理技术内幕&lt;== Row: 2, 计算机理论, Spring 5核心原理与30个类手写实战, 十年沉淀之作，手写Spring精华思想&lt;== Row: 3, 计算机理论, Spring 5 设计模式, 深入Spring源码剖析Spring源码中蕴含的10大设计模式&lt;== Row: 4, 计算机理论, Spring MVC+MyBatis开发从入门到项目实战, 全方位解析面向Web应用的轻量级框架，带你成为Spring MVC开发高手&lt;== Row: 5, 计算机理论, 轻量级Java Web企业应用实战, 源码级剖析Spring框架，适合已掌握Java基础的读者&lt;== Row: 6, 计算机理论, Java核心技术 卷I 基础知识（原书第11版）, Core Java 第11版，Jolt大奖获奖作品，针对Java SE9、10、11全面更新&lt;== Row: 7, 计算机理论, 深入理解Java虚拟机, 5个维度全面剖析JVM，大厂面试知识点全覆盖&lt;== Row: 8, 计算机理论, Java编程思想（第4版）, Java学习必读经典,殿堂级著作！赢得了全球程序员的广泛赞誉&lt;== Row: 9, 计算机理论, 零基础学Java（全彩版）, 零基础自学编程的入门图书，由浅入深，详解Java语言的编程思想和核心技术&lt;== Row: 10, 市场营销, 直播就该这么做：主播高效沟通实战指南, 李子柒、李佳琦、薇娅成长为网红的秘密都在书中&lt;== Row: 11, 市场营销, 直播销讲实战一本通, 和秋叶一起学系列网络营销书籍&lt;== Row: 12, 市场营销, 直播带货：淘宝、天猫直播从新手到高手, 一本教你如何玩转直播的书，10堂课轻松实现带货月入3W+&lt;== Row: 13, 测试类型, 测试数据, 测试描述数据&lt;== Row: 14, 测试数据update, 测试数据update, 测试数据update&lt;== Row: 15, -----------------, 测试数据123, 测试数据123&lt;== Total: 15 ​ 其中清晰的标注了当前执行的SQL语句是什么，携带了什么参数，对应的执行结果是什么，所有信息应有尽有。 ​ 此处设置的是日志的显示形式，当前配置的是控制台输出，当然还可以由更多的选择，根据需求切换即可 4）整合druid①默认的dataPool​ 在没有指定数据源时，我们的配置如下： 123456spring: datasource: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:3306/ssm_db?serverTimezone=Asia/Shanghai username: root password: root ​ 此时虽然没有指定数据源，但是SpringBoot选了一个它认为最好的数据源对象，这就是HiKari，通过启动日志可以查看到对应的身影。 1232021-11-29 09:39:15.202 INFO 12260 --- [ main] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Starting...2021-11-29 09:39:15.208 WARN 12260 --- [ main] com.zaxxer.hikari.util.DriverDataSource : Registered driver with driverClassName=com.mysql.jdbc.Driver was not found, trying direct instantiation.2021-11-29 09:39:15.551 INFO 12260 --- [ main] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Start completed. 上述信息中每一行都有HiKari的身影，如果需要更换数据源，其实只需要两步即可。 注： Hikari的故事：https://cloud.tencent.com/developer/news/561800 druid和Hikari的对比：https://juejin.cn/post/6885974851949953031 ②boot整合通用步骤①：导入对应的坐标（注意，是坐标，此处不是starter） 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.16&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 步骤②：修改配置，在数据源配置中有一个type属性，专用于指定数据源类型 1234567spring: datasource: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:3306/ssm_db?serverTimezone=UTC username: root password: root type: com.alibaba.druid.pool.DruidDataSource ​ 这里其实要提出一个问题的，目前的数据源配置格式是一个通用格式，不管你换什么数据源都可以用这种形式进行配置。 ​ 但是新的问题又来了，如果对数据源进行个性化的配置，例如配置数据源对应的连接数量，这个时候就有新的问题了。 ③boot整合专门步骤①：导入对应的starter 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.2.6&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 步骤②：修改配置 1234567spring: datasource: druid: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:3306/ssm_db?serverTimezone=UTC username: root password: root ​ 配置项中，在datasource下面并不是直接配置url这些属性的，而是先配置了一个druid节点，然后再配置的url这些东西。即druid下面含有url这个属性。 除了这4个常规配置外，还有druid专用的其他配置。通过提示功能可以打开druid相关的配置查阅： 4.MybatisPlus的使用1）dao的快速开发①分页MP提供的分页操作API如下 12345678910@Testvoid testGetPage()&#123; IPage page = new Page(2,5);//Ipage是接口 Page是实现类 bookDao.selectPage(page, null);//返回的仍然是Ipage page，实际上就是对从传入的page进行数据添加 System.out.println(page.getCurrent()); System.out.println(page.getSize()); System.out.println(page.getTotal()); System.out.println(page.getPages()); System.out.println(page.getRecords());&#125; ​ 其中selectPage方法需要传入一个封装分页数据的对象，可以通过new的形式创建这个对象，当然这个对象也是MP提供的，别选错包了。创建此对象时就需要指定分页的两个基本数据 当前显示第几页 每页显示几条数据 ​ 可以通过创建Page对象时利用构造方法初始化这两个数据 1IPage page = new Page(2,5); ​ 将该对象传入到查询方法selectPage后，可以得到查询结果，但是我们会发现当前操作查询结果返回值仍然是一个IPage对象 1IPage page = bookDao.selectPage(page, null); ​ 原来这个IPage对象中封装了若干个数据，而查询的结果作为IPage对象封装的一个数据存在的，可以理解为查询结果得到后，又塞到了这个IPage对象中，其实还是为了高度的封装，一个IPage描述了分页所有的信息。下面5个操作就是IPage对象中封装的所有信息了 12345678910@Testvoid testGetPage()&#123; IPage page = new Page(2,5); bookDao.selectPage(page, null); System.out.println(page.getCurrent()); //当前页码值 System.out.println(page.getSize()); //每页显示数 System.out.println(page.getTotal()); //数据总量 System.out.println(page.getPages()); //总页数 System.out.println(page.getRecords()); //详细数据&#125; 但是当你执行这个操作时，这个分页当前是无效的： ​ 对于MySQL的分页操作使用limit关键字进行，而并不是所有的数据库都使用limit关键字实现的，这个时候MP为了制作的兼容性强，将分页操作设置为基础查询操作的升级版，你可以理解为IPhone6与IPhone6S-PLUS的关系。 ​ 基础操作中有查询全部的功能，而在这个基础上只需要升级一下（PLUS）就可以得到分页操作。所以MP将分页操作做成了一个开关，你用分页功能就把开关开启，不用就不需要开启这个开关。而我们现在没有开启这个开关，所以分页操作是没有的。这个开关是通过MP的拦截器的形式存在的，具体设置方式如下 定义MP拦截器并将其设置为Spring管控的bean 123456789@Configurationpublic class MPConfig &#123; @Bean public MybatisPlusInterceptor mybatisPlusInterceptor()&#123; MybatisPlusInterceptor interceptor = new MybatisPlusInterceptor();//制造了一个空的拦截器栈 interceptor.addInnerInterceptor(new PaginationInnerInterceptor());//添加了一个分页拦截器，打开了开关 return interceptor; &#125;&#125; ​ 上述代码第一行是创建MP的拦截器栈，这个时候拦截器栈中没有具体的拦截器，第二行是初始化了分页拦截器，并添加到拦截器栈中。如果后期开发其他功能，需要添加全新的拦截器，按照第二行的格式继续add进去新的拦截器就可以了。 ②条件查询​ 下面的操作就是执行一个模糊匹配对应的操作，由like条件书写变为了like方法的调用 123456@Testvoid testGetBy()&#123; QueryWrapper&lt;Book&gt; qw = new QueryWrapper&lt;&gt;(); qw.like(&quot;name&quot;,&quot;Spring&quot;); bookDao.selectList(qw);&#125; ​ 其中第一句QueryWrapper对象是一个用于封装查询条件的对象，该对象可以动态使用API调用的方法添加条件，最终转化成对应SQL语句。第二句就是一个条件了，需要什么条件，使用QueryWapper对象直接调用对应操作即可。比如做大于小于关系，就可以使用lt或gt方法，等于使用eq方法，等等，此处不做更多的解释了。 ​ 这组API使用还是比较简单的，但是关于属性字段名的书写存在着安全隐患，比如查询字段name，当前是以字符串的形态书写的，万一写错，编译器还没有办法发现，只能将问题抛到运行器通过异常堆栈告诉开发者，不太友好。 ​ MP针对字段检查进行了功能升级，全面支持Lambda表达式，就有了下面这组API。由QueryWrapper对象升级为LambdaQueryWrapper对象，这样在编写的时候就会报错 1234567@Testvoid testGetBy2()&#123; String name = &quot;1&quot;; LambdaQueryWrapper&lt;Book&gt; lqw = new LambdaQueryWrapper&lt;Book&gt;(); lqw.like(Book::getName,name); bookDao.selectList(lqw);&#125; ​ 为了便于开发者动态拼写SQL，防止将null数据作为条件使用，MP还提供了动态拼装SQL的快捷书写方式 12345678@Testvoid testGetBy2()&#123; String name = &quot;1&quot;; LambdaQueryWrapper&lt;Book&gt; lqw = new LambdaQueryWrapper&lt;Book&gt;(); //if(name != null) lqw.like(Book::getName,name); //方式一：JAVA代码控制 lqw.like(name != null,Book::getName,name); //方式二：API接口提供控制开关 bookDao.selectList(lqw);&#125; ​ 该语句在执行的时候会首先判断name!&#x3D;null，结果为真才会执行selectList 2）service快速开发123public interface IBookService extends IService&lt;Book&gt; &#123; //添加非通用操作API接口&#125; ​ 业务层接口实现类快速开发，关注继承的类需要传入两个泛型，前者是数据层接口，后者是实体类 123456@Servicepublic class BookServiceImpl extends ServiceImpl&lt;BookDao, Book&gt; implements IBookService &#123; @Autowired private BookDao bookDao; //添加非通用操作API&#125; ​ 当所提供的方法不满足需求的时候，我们需要自己写方法实现，注意不要override原来的方法，原方法使用remove，我们就使用delete","categories":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"http://example.com/categories/Spring-Boot/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"http://example.com/tags/Spring-Boot/"}]},{"title":"Spring笔记","slug":"Spring笔记","date":"2022-03-10T10:31:31.000Z","updated":"2022-08-14T05:42:10.233Z","comments":true,"path":"2022/03/10/Spring笔记/","link":"","permalink":"http://example.com/2022/03/10/Spring%E7%AC%94%E8%AE%B0/","excerpt":"xml配置、依赖注入、bean的生存周期、注解配置、spring整合第三方、AOP、spring事务","text":"xml配置、依赖注入、bean的生存周期、注解配置、spring整合第三方、AOP、spring事务 一.xml配置相关1. bean标签中的scope spring bean 线程安全问题： https://cloud.tencent.com/developer/article/1743283 https://blog.csdn.net/u012843361/article/details/84023869 2.bean的三种实例方式1）无参构造方法步骤1：准备一个BookDao和BookDaoImpl类 12345678910public interface BookDao &#123; public void save();&#125;public class BookDaoImpl implements BookDao &#123; public void save() &#123; System.out.println(&quot;book dao save ...&quot;); &#125;&#125; 步骤2：将类配置到Spring容器 12345678&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;bean id=&quot;bookDao&quot; class=&quot;com.itheima.dao.impl.BookDaoImpl&quot;/&gt;&lt;/beans&gt; 以上已经可以成功运行了 步骤3：将构造函数改成private测试 123456789public class BookDaoImpl implements BookDao &#123; private BookDaoImpl() &#123; System.out.println(&quot;book dao constructor is running ....&quot;); &#125; public void save() &#123; System.out.println(&quot;book dao save ...&quot;); &#125;&#125; 运行程序，能执行成功,说明内部走的依然是构造函数,能访问到类中的私有构造方法,显而易见Spring底层用的是反射 步骤4：构造函数中添加一个参数测试 123456789public class BookDaoImpl implements BookDao &#123; private BookDaoImpl(int i) &#123; System.out.println(&quot;book dao constructor is running ....&quot;); &#125; public void save() &#123; System.out.println(&quot;book dao save ...&quot;); &#125;&#125; 运行程序，程序会报错，说明Spring底层使用的是类的无参构造方法。 ​ 这样就是通过无参的构造来完成的，并且方法如果是private也能够实例化，说明底层用的是反射（暴力反射） 反射及暴力反射：https://segmentfault.com/a/1190000037432381 2）静态工厂实例化①静态工厂创建bean在讲这种方式之前，我们需要先回顾一个知识点是使用工厂来创建对象的方式: 准备一个OrderDao和OrderDaoImpl类 123456789public interface OrderDao &#123; public void save();&#125;public class OrderDaoImpl implements OrderDao &#123; public void save() &#123; System.out.println(&quot;order dao save ...&quot;); &#125;&#125; 创建一个工厂类OrderDaoFactory并提供一个&#x3D;&#x3D;静态方法&#x3D;&#x3D; 123456//静态工厂创建对象public class OrderDaoFactory &#123; public static OrderDao getOrderDao()&#123; return new OrderDaoImpl(); &#125;&#125; 编写AppForInstanceOrder运行类，在类中通过工厂获取对象 1234567public class AppForInstanceOrder &#123; public static void main(String[] args) &#123; //通过静态工厂创建对象 OrderDao orderDao = OrderDaoFactory.getOrderDao(); orderDao.save(); &#125;&#125; 运行后，可以查看到结果 ②静态工厂实例化这就要用到Spring中的静态工厂实例化的知识了，具体实现步骤为: 在spring的配置文件application.properties中添加以下内容: 1&lt;bean id=&quot;orderDao&quot; class=&quot;com.itheima.factory.OrderDaoFactory&quot; factory-method=&quot;getOrderDao&quot;/&gt; class:工厂类的类全名 factory-mehod:具体工厂类中创建对象的方法名 对应关系如下图: 在AppForInstanceOrder运行类，使用从IOC容器中获取bean的方法进行运行测试 1234567public class AppForInstanceOrder &#123; public static void main(String[] args) &#123; ApplicationContext ctx = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); OrderDao orderDao = (OrderDao) ctx.getBean(&quot;orderDao&quot;); orderDao.save(); &#125;&#125; 运行后，可以查看到结果 ③问题问题：这种方式在工厂类中也是直接new对象的，和自己直接new没什么太大的区别，而且静态工厂的方式反而更复杂，这种方式的意义是什么? 主要的原因是: 在工厂的静态方法中，我们除了new对象还可以做其他的一些业务操作，这些操作必不可少,如: 123456public class OrderDaoFactory &#123; public static OrderDao getOrderDao()&#123; System.out.println(&quot;factory setup....&quot;);//模拟必要的业务操作 return new OrderDaoImpl(); &#125;&#125; 之前new对象的方式就无法添加其他的业务内容，重新运行，查看结果: ​ spring bean 线程安全问题： https://cloud.tencent.com/developer/article/1743283 https://blog.csdn.net/u012843361/article/details/84023869 3）实例工厂实例化①实例工厂创建bean 准备一个UserDao和UserDaoImpl类 12345678910public interface UserDao &#123; public void save();&#125;public class UserDaoImpl implements UserDao &#123; public void save() &#123; System.out.println(&quot;user dao save ...&quot;); &#125;&#125; 创建一个工厂类OrderDaoFactory并提供一个普通方法，注意此处和静态工厂的工厂类不一样的地方是方法不是静态方法 12345public class UserDaoFactory &#123; public UserDao getUserDao()&#123; return new UserDaoImpl(); &#125;&#125; 编写AppForInstanceUser运行类，在类中通过工厂获取对象 12345678public class AppForInstanceUser &#123; public static void main(String[] args) &#123; //创建实例工厂对象 UserDaoFactory userDaoFactory = new UserDaoFactory(); //通过实例工厂对象创建对象 UserDao userDao = userDaoFactory.getUserDao(); userDao.save();&#125; 运行后，可以查看到结果 ②实例工厂实例化 在spring的配置文件中添加以下内容: 12&lt;bean id=&quot;userFactory&quot; class=&quot;com.itheima.factory.UserDaoFactory&quot;/&gt;&lt;bean id=&quot;userDao&quot; factory-method=&quot;getUserDao&quot; factory-bean=&quot;userFactory&quot;/&gt; 实例化工厂运行的顺序是: 创建实例化工厂对象,对应的是第一行配置 调用对象中的方法来创建bean，对应的是第二行配置 factory-bean:工厂的实例对象 factory-method:工厂对象中的具体创建对象的方法名,对应关系如下： factory-mehod:具体工厂类中创建对象的方法名 因为获得bean的方法不再是静态的了，所以要先创建factoryBean，然后再调用factoryBean的方法 在AppForInstanceUser运行类，使用从IOC容器中获取bean的方法进行运行测试 12345678public class AppForInstanceUser &#123; public static void main(String[] args) &#123; ApplicationContext ctx = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); UserDao userDao = (UserDao) ctx.getBean(&quot;userDao&quot;); userDao.save(); &#125;&#125; 运行后，可以查看到结果 以上配置的过程还是比较复杂，所以Spring为了简化这种配置方式就提供了一种叫FactoryBean的方式来简化开发。 ③FactoryBean的使用具体的使用步骤为: 创建一个UserDaoFactoryBean的类，实现FactoryBean接口，重写接口的方法 12345678910public class UserDaoFactoryBean implements FactoryBean&lt;UserDao&gt; &#123; //代替原始实例工厂中创建对象的方法 public UserDao getObject() throws Exception &#123; return new UserDaoImpl(); &#125; //返回所创建类的Class对象 public Class&lt;?&gt; getObjectType() &#123; return UserDao.class; &#125;&#125; 在Spring的配置文件中进行配置 1&lt;bean id=&quot;userDao&quot; class=&quot;com.itheima.factory.UserDaoFactoryBean&quot;/&gt; AppForInstanceUser运行类不用做任何修改，直接运行 这种方式在Spring去整合其他框架的时候会被用到： 查看源码会发现，FactoryBean接口其实会有三个方法，分别是: 1234567T getObject() throws Exception;Class&lt;?&gt; getObjectType();default boolean isSingleton() &#123; return true;&#125; 方法一：getObject()，被重写后，在方法中进行对象的创建并返回 方法二：getObjectType(),被重写后，主要返回的是被创建类的Class对象 方法三：没有被重写，因为它已经给了默认值，从方法名中可以看出其作用是设置对象是否为单例，默认true（单例） 以下是单例验证： 思路很简单，就是从容器中获取该对象的多个值，打印到控制台，查看是否为同一个对象。 12345678910public class AppForInstanceUser &#123; public static void main(String[] args) &#123; ApplicationContext ctx = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); UserDao userDao1 = (UserDao) ctx.getBean(&quot;userDao&quot;); UserDao userDao2 = (UserDao) ctx.getBean(&quot;userDao&quot;); System.out.println(userDao1); System.out.println(userDao2); &#125;&#125; 打印结果，如下： 通过验证，会发现默认是单例。 那如果想改成多例，只需要将isSingleton()方法进行重写，修改返回为false，即可 123456789101112131415//FactoryBean创建对象public class UserDaoFactoryBean implements FactoryBean&lt;UserDao&gt; &#123; //代替原始实例工厂中创建对象的方法 public UserDao getObject() throws Exception &#123; return new UserDaoImpl(); &#125; public Class&lt;?&gt; getObjectType() &#123; return UserDao.class; &#125; public boolean isSingleton() &#123; return false; &#125;&#125; 重新运行AppForInstanceUser，查看结果 从结果中可以看出现在已经是非单例了，但是一般情况下我们都会采用单例，也就是采用默认即可。所以isSingleton()方法一般不需要进行重写。 3.bean的生存周期1）标签中配置init-method和destroy-method①基本步骤具体的控制有两个阶段： bean创建之后，想要添加内容，比如用来初始化需要用到资源 bean销毁之前，想要添加内容，比如用来释放用到的资源 步骤1：添加初始化和销毁方法 针对这两个阶段，我们在BooDaoImpl类中分别添加两个方法，方法名任意 12345678910111213public class BookDaoImpl implements BookDao &#123; public void save() &#123; System.out.println(&quot;book dao save ...&quot;); &#125; //表示bean初始化对应的操作 public void init()&#123; System.out.println(&quot;init...&quot;); &#125; //表示bean销毁前对应的操作 public void destory()&#123; System.out.println(&quot;destory...&quot;); &#125;&#125; 步骤2：配置生命周期 在配置文件添加配置，如下: 1&lt;bean id=&quot;bookDao&quot; class=&quot;com.itheima.dao.impl.BookDaoImpl&quot; init-method=&quot;init&quot; destroy-method=&quot;destory&quot;/&gt; 步骤3：运行程序 运行AppForLifeCycle打印结果为: 从结果中可以看出，init方法执行了，但是destroy方法却未执行，这是为什么呢? Spring的IOC容器是运行在JVM中 运行main方法后,JVM启动,Spring加载配置文件生成IOC容器,从容器获取bean对象，然后调方法执行 main方法执行完后，JVM退出，这个时候IOC容器中的bean还没有来得及销毁就已经结束了 所以没有调用对应的destroy方法 ②close方式关闭容器 pplicationContext中没有close方法 需要将ApplicationContext更换成ClassPathXmlApplicationContext 12ClassPathXmlApplicationContext ctx = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); 调用ctx的close()方法 1ctx.close(); 运行程序，就能执行destroy方法的内容 ③钩子函数方式关闭容 在容器未关闭之前，提前设置好回调函数，让JVM在退出之前回调此函数来关闭容器 调用ctx的registerShutdownHook()方法 1ctx.registerShutdownHook(); **注意:**registerShutdownHook在ApplicationContext中也没有 运行后，查询打印结果 ④两种关闭方式对比 相同点：这两种都能用来关闭容器 不同点：close()是在调用的时候关闭，registerShutdownHook()是在JVM退出前调用关闭。 2）接口控制修改BookServiceImpl类，添加两个接口InitializingBean， DisposableBean并实现接口中的两个方法afterPropertiesSet和destroy 12345678910111213141516public class BookServiceImpl implements BookService, InitializingBean, DisposableBean &#123; private BookDao bookDao; public void setBookDao(BookDao bookDao) &#123; this.bookDao = bookDao; &#125; public void save() &#123; System.out.println(&quot;book service save ...&quot;); bookDao.save(); &#125; public void destroy() throws Exception &#123; System.out.println(&quot;service destroy&quot;); &#125; public void afterPropertiesSet() throws Exception &#123; System.out.println(&quot;service init&quot;); &#125;&#125; 重新运行AppForLifeCycle类， 4.依赖注入(set property)1）setter注入①引用类型步骤1:声明属性并提供setter方法在BookServiceImpl中声明userDao属性，并提供setter方法 1234567891011121314151617public class BookServiceImpl implements BookService&#123; private BookDao bookDao; private UserDao userDao; public void setUserDao(UserDao userDao) &#123; this.userDao = userDao; &#125; public void setBookDao(BookDao bookDao) &#123; this.bookDao = bookDao; &#125; public void save() &#123; System.out.println(&quot;book service save ...&quot;); bookDao.save(); userDao.save(); &#125;&#125; 步骤2:配置文件中进行注入配置在applicationContext.xml配置文件中使用property标签注入 123456789101112&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;bean id=&quot;bookDao&quot; class=&quot;com.itheima.dao.impl.BookDaoImpl&quot;/&gt; &lt;bean id=&quot;userDao&quot; class=&quot;com.itheima.dao.impl.UserDaoImpl&quot;/&gt; &lt;bean id=&quot;bookService&quot; class=&quot;com.itheima.service.impl.BookServiceImpl&quot;&gt; &lt;property name=&quot;bookDao&quot; ref=&quot;bookDao&quot;/&gt; &lt;property name=&quot;userDao&quot; ref=&quot;userDao&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; 步骤3:运行程序运行AppForDISet类，查看结果，说明userDao已经成功注入。 ②注入简单数据类型步骤1:声明属性并提供setter方法在BookDaoImpl类中声明对应的简单数据类型的属性,并提供对应的setter方法 1234567891011121314151617public class BookDaoImpl implements BookDao &#123; private String databaseName; private int connectionNum; public void setConnectionNum(int connectionNum) &#123; this.connectionNum = connectionNum; &#125; public void setDatabaseName(String databaseName) &#123; this.databaseName = databaseName; &#125; public void save() &#123; System.out.println(&quot;book dao save ...&quot;+databaseName+&quot;,&quot;+connectionNum); &#125;&#125; 步骤2:配置文件中进行注入配置在applicationContext.xml配置文件中使用property标签注入 123456789101112131415&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;bean id=&quot;bookDao&quot; class=&quot;com.itheima.dao.impl.BookDaoImpl&quot;&gt; &lt;property name=&quot;databaseName&quot; value=&quot;mysql&quot;/&gt; &lt;property name=&quot;connectionNum&quot; value=&quot;10&quot;/&gt; &lt;/bean&gt; &lt;bean id=&quot;userDao&quot; class=&quot;com.itheima.dao.impl.UserDaoImpl&quot;/&gt; &lt;bean id=&quot;bookService&quot; class=&quot;com.itheima.service.impl.BookServiceImpl&quot;&gt; &lt;property name=&quot;bookDao&quot; ref=&quot;bookDao&quot;/&gt; &lt;property name=&quot;userDao&quot; ref=&quot;userDao&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; 说明: value:后面跟的是简单数据类型，对于参数类型，Spring在注入的时候会自动转换，但是不能写成 1&lt;property name=&quot;connectionNum&quot; value=&quot;abc&quot;/&gt; 这样的话，spring在将abc转换成int类型的时候就会报错。 步骤3:运行程序运行AppForDISet类，查看结果，说明userDao已经成功注入。 **注意:**两个property注入标签的顺序可以任意。 对于setter注入方式的基本使用就已经介绍完了， 对于引用数据类型使用的是&lt;property name=&quot;&quot; ref=&quot;&quot;/&gt; 对于简单数据类型使用的是&lt;property name=&quot;&quot; value=&quot;&quot;/&gt; 2）构造器注入在applicationContext.xml中配置注入 123456789101112&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;bean id=&quot;bookDao&quot; class=&quot;com.itheima.dao.impl.BookDaoImpl&quot;/&gt; &lt;bean id=&quot;userDao&quot; class=&quot;com.itheima.dao.impl.UserDaoImpl&quot;/&gt; &lt;bean id=&quot;bookService&quot; class=&quot;com.itheima.service.impl.BookServiceImpl&quot;&gt; &lt;constructor-arg name=&quot;bookDao&quot; ref=&quot;bookDao&quot;/&gt; &lt;constructor-arg name=&quot;userDao&quot; ref=&quot;userDao&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; **说明:**这两个&lt;contructor-arg&gt;的配置顺序可以任意 3）自动注入首先来实现按照类型注入的配置 12345678910&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;bean class=&quot;com.itheima.dao.impl.BookDaoImpl&quot;/&gt; &lt;!--autowire属性：开启自动装配，通常使用按类型装配--&gt; &lt;bean id=&quot;bookService&quot; class=&quot;com.itheima.service.impl.BookServiceImpl&quot; autowire=&quot;byType&quot;/&gt;&lt;/beans&gt; 需要注入属性的类中对应属性的setter方法不能省略 被注入的对象必须要被Spring的IOC容器管理 按照类型在Spring的IOC容器中如果找到多个对象，会报NoUniqueBeanDefinitionException 一个类型在IOC中有多个对象，还想要注入成功，这个时候就需要按照名称注入，配置方式为: 12345678910&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;bean class=&quot;com.itheima.dao.impl.BookDaoImpl&quot;/&gt; &lt;!--autowire属性：开启自动装配，通常使用按类型装配--&gt; &lt;bean id=&quot;bookService&quot; class=&quot;com.itheima.service.impl.BookServiceImpl&quot; autowire=&quot;byName&quot;/&gt;&lt;/beans&gt; 4）集合注入注入数组类型数据 1234567&lt;property name=&quot;array&quot;&gt; &lt;array&gt; &lt;value&gt;100&lt;/value&gt; &lt;value&gt;200&lt;/value&gt; &lt;value&gt;300&lt;/value&gt; &lt;/array&gt;&lt;/property&gt; 注入List类型数据 12345678&lt;property name=&quot;list&quot;&gt; &lt;list&gt; &lt;value&gt;itcast&lt;/value&gt; &lt;value&gt;itheima&lt;/value&gt; &lt;value&gt;boxuegu&lt;/value&gt; &lt;value&gt;chuanzhihui&lt;/value&gt; &lt;/list&gt;&lt;/property&gt; 注入Set类型数据 12345678&lt;property name=&quot;set&quot;&gt; &lt;set&gt; &lt;value&gt;itcast&lt;/value&gt; &lt;value&gt;itheima&lt;/value&gt; &lt;value&gt;boxuegu&lt;/value&gt; &lt;value&gt;boxuegu&lt;/value&gt; &lt;/set&gt;&lt;/property&gt; 注入Map类型数据 1234567&lt;property name=&quot;map&quot;&gt; &lt;map&gt; &lt;entry key=&quot;country&quot; value=&quot;china&quot;/&gt; &lt;entry key=&quot;province&quot; value=&quot;henan&quot;/&gt; &lt;entry key=&quot;city&quot; value=&quot;kaifeng&quot;/&gt; &lt;/map&gt;&lt;/property&gt; 注入Properties类型数据 1234567&lt;property name=&quot;properties&quot;&gt; &lt;props&gt; &lt;prop key=&quot;country&quot;&gt;china&lt;/prop&gt; &lt;prop key=&quot;province&quot;&gt;henan&lt;/prop&gt; &lt;prop key=&quot;city&quot;&gt;kaifeng&lt;/prop&gt; &lt;/props&gt;&lt;/property&gt; 说明： property标签表示setter方式注入，构造方式注入constructor-arg标签内部也可以写&lt;array&gt;、&lt;list&gt;、&lt;set&gt;、&lt;map&gt;、&lt;props&gt;标签 List的底层也是通过数组实现的，所以&lt;list&gt;和&lt;array&gt;标签是可以混用 集合中要添加引用类型，只需要把&lt;value&gt;标签改成&lt;ref&gt;标签，这种方式用的比较少 5.spring加载properties1）优先加载环境变量问题步骤1:在项目中添对应的类 BookDao和BookDaoImpl类，并在BookDaoImpl类中添加name属性与setter方法 123456789101112131415public interface BookDao &#123; public void save();&#125;public class BookDaoImpl implements BookDao &#123; private String name; public void setName(String name) &#123; this.name = name; &#125; public void save() &#123; System.out.println(&quot;book dao save ...&quot; + name); &#125;&#125; 步骤2:完成配置文件的读取与注入 在applicationContext.xml添加配置，bean的配置管理、读取外部properties、依赖注入: 12345678910111213141516&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;context:property-placeholder location=&quot;jdbc.properties&quot;/&gt; &lt;bean id=&quot;bookDao&quot; class=&quot;com.itheima.dao.impl.BookDaoImpl&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;$&#123;jdbc.driver&#125;&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; 步骤3:运行程序 在App类中，从IOC容器中获取bookDao对象，调用方法，查看值是否已经被获取到并打印控制台 12345678public class App &#123; public static void main(String[] args) throws Exception&#123; ApplicationContext ctx = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); BookDao bookDao = (BookDao) ctx.getBean(&quot;bookDao&quot;); bookDao.save(); &#125;&#125; 有2个小问题： 问题一:键值对的key为username引发的问题 1.在properties中配置键值对的时候，如果key设置为username 1username=root666 2.在applicationContext.xml注入该属性 12345678910111213141516&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;context:property-placeholder location=&quot;jdbc.properties&quot;/&gt; &lt;bean id=&quot;bookDao&quot; class=&quot;com.itheima.dao.impl.BookDaoImpl&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;$&#123;username&#125;&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; 3.运行后，在控制台打印的却不是root666，而是自己电脑的用户名 4.出现问题的原因是&lt;context:property-placeholder/&gt;标签会加载系统的环境变量，而且环境变量的值会被优先加载，如何查看系统的环境变量? 1234public static void main(String[] args) throws Exception&#123; Map&lt;String, String&gt; env = System.getenv(); System.out.println(env);&#125; 5.解决方案 123456789101112&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;context:property-placeholder location=&quot;jdbc.properties&quot; system-properties-mode=&quot;NEVER&quot;/&gt;&lt;/beans&gt; system-properties-mode:设置为NEVER,表示不加载系统属性，就可以解决上述问题。 还有一个解决方案就是避免使用username作为属性的key。 问题二:当有多个properties配置文件需要被加载，该如何配置? 1.调整下配置文件的内容，在resources下添加jdbc.properties,jdbc2.properties,内容如下: jdbc.properties 1234jdbc.driver=com.mysql.jdbc.Driverjdbc.url=jdbc:mysql://127.0.0.1:3306/spring_dbjdbc.username=rootjdbc.password=root jdbc2.properties 1username=root666 2.修改applicationContext.xml 123456789101112131415161718&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!--方式一 --&gt; &lt;context:property-placeholder location=&quot;jdbc.properties,jdbc2.properties&quot; system-properties-mode=&quot;NEVER&quot;/&gt; &lt;!--方式二--&gt; &lt;context:property-placeholder location=&quot;*.properties&quot; system-properties-mode=&quot;NEVER&quot;/&gt; &lt;!--方式三 --&gt; &lt;context:property-placeholder location=&quot;classpath:*.properties&quot; system-properties-mode=&quot;NEVER&quot;/&gt; &lt;!--方式四--&gt; &lt;context:property-placeholder location=&quot;classpath*:*.properties&quot; system-properties-mode=&quot;NEVER&quot;/&gt;&lt;/beans&gt; 说明: 方式一:可以实现，如果配置文件多的话，每个都需要配置 方式二:*.properties代表所有以properties结尾的文件都会被加载，可以解决方式一的问题，但是不标准 方式三:标准的写法，classpath:代表的是从根路径下开始查找，但是只能查询当前项目的根路径 方式四:不仅可以加载当前项目还可以加载当前项目所依赖的所有项目的根路径下的properties配置文件 classpath 是加载当前项目的根路径 classpath* 还会加载当前项目所依赖的所有项目的根路径 6.容器相关 ApplicationContext的延迟加载： bean相关标签： 依赖注入相关标签： 二、注解配置相关1.注解配置相关1）@componentScan@Component等 名称 @Component&#x2F;@Controller&#x2F;@Service&#x2F;@Repository 类型 类注解 位置 类定义上方 作用 设置该类为spring管理的bean 属性 value（默认）：定义bean的id 对于@Component注解，还衍生出了其他三个注解@Controller、@Service、@Repository，通过查看源码会发现: 方便我们后期在编写类的时候能很好的区分出这个类是属于表现层、业务层还是数据层的类。 2）@Configuration@Configuration 名称 @Configuration 类型 类注解 位置 类定义上方 作用 设置该类为spring配置类 属性 value（默认）：定义bean的id applicationContext.xml中&lt;context:component-san/&gt;的作用是指定扫描包路径，注解为@ComponentScan @Configuration标识该类为配置类，使用类替换applicationContext.xml文件 ClassPathXmlApplicationContext是加载XML配置文件 AnnotationConfigApplicationContext是加载配置类 3）@scope和@PostConstruct、@PreDestroy@Scope 名称 @Scope 类型 类注解 位置 类定义上方 作用 设置该类创建对象的作用范围可用于设置创建出的bean是否为单例对象 属性 value（默认）：定义bean作用范围，&#x3D;&#x3D;默认值singleton（单例），可选值prototype（非单例）&#x3D;&#x3D; @PostConstruct 名称 @PostConstruct 类型 方法注解 位置 方法上 作用 设置该方法为初始化方法 属性 无 @PreDestroy 名称 @PreDestroy 类型 方法注解 位置 方法上 作用 设置该方法为销毁方法 属性 无 只需要在对应的方法上添加@PostConstruct和@PreDestroy注解即可 123456789101112131415@Repositorypublic class BookDaoImpl implements BookDao &#123; public void save() &#123; System.out.println(&quot;book dao save ...&quot;); &#125; @PostConstruct //在构造方法之后执行，替换 init-method public void init() &#123; System.out.println(&quot;init ...&quot;); &#125; @PreDestroy //在销毁方法之前执行,替换 destroy-method public void destroy() &#123; System.out.println(&quot;destroy ...&quot;); &#125;&#125; 注意:@PostConstruct和@PreDestroy注解如果找不到，需要导入下面的jar包&#x3D;&#x3D; 12345&lt;dependency&gt; &lt;groupId&gt;javax.annotation&lt;/groupId&gt; &lt;artifactId&gt;javax.annotation-api&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt;&lt;/dependency&gt; 找不到的原因是，从JDK9以后jdk中的javax.annotation包被移除了，这两个注解刚好就在这个包中。 4）@PropertySource和@Value@Value 名称 @Value 类型 属性注解 或 方法注解（了解） 位置 属性定义上方 或 标准set方法上方 或 类set方法上方 作用 为 基本数据类型 或 字符串类型 属性设置值 属性 value（默认）：要注入的属性值 @PropertySource 名称 @PropertySource 类型 类注解 位置 类定义上方 作用 加载properties文件中的属性值 属性 value（默认）：设置加载的properties文件对应的文件名或文件名组成的数组 在配置类上添加@PropertySource注解 123456@Configuration@ComponentScan(&quot;com.itheima&quot;)@PropertySource(&quot;jdbc.properties&quot;)public class SpringConfig &#123;&#125; 使用@Value读取配置文件中的内容 12345678@Repository(&quot;bookDao&quot;)public class BookDaoImpl implements BookDao &#123; @Value(&quot;$&#123;name&#125;&quot;) private String name; public void save() &#123; System.out.println(&quot;book dao save ...&quot; + name); &#125;&#125; PropertySource详解： https://blog.csdn.net/weixin_43849277/article/details/120728182 5）@AutoWired、@Qualifier1234567891011@Servicepublic class BookServiceImpl implements BookService &#123; @Autowired @Qualifier(&quot;bookDao1&quot;) private BookDao bookDao; public void save() &#123; System.out.println(&quot;book service save ...&quot;); bookDao.save(); &#125;&#125; 注意：@Qualifier不能独立使用，必须和@Autowired一起使用 反射及暴力反射：https://segmentfault.com/a/1190000037432381 6）@Bean和@Import①@ComponentScan方式在Spring的配置类上添加包扫描 12345@Configuration@ComponentScan(&quot;com.itheima.config&quot;)public class SpringConfig &#123; &#125; 在JdbcConfig上添加配置注解 JdbcConfig类要放入到com.itheima.config包下，需要被Spring的配置类扫描到即可 123456789101112@Configurationpublic class JdbcConfig &#123; @Bean public DataSource dataSource()&#123; DruidDataSource ds = new DruidDataSource(); ds.setDriverClassName(&quot;com.mysql.jdbc.Driver&quot;); ds.setUrl(&quot;jdbc:mysql://localhost:3306/spring_db&quot;); ds.setUsername(&quot;root&quot;); ds.setPassword(&quot;root&quot;); return ds; &#125;&#125; ②@Import 名称 @Import 类型 类注解 位置 类定义上方 作用 导入配置类 属性 value（默认）：定义导入的配置类类名，当配置类有多个时使用数组格式一次性导入多个配置类 去除JdbcConfig类上的注解 1234567891011public class JdbcConfig &#123; @Bean public DataSource dataSource()&#123; DruidDataSource ds = new DruidDataSource(); ds.setDriverClassName(&quot;com.mysql.jdbc.Driver&quot;); ds.setUrl(&quot;jdbc:mysql://localhost:3306/spring_db&quot;); ds.setUsername(&quot;root&quot;); ds.setPassword(&quot;root&quot;); return ds; &#125;&#125; 在Spring配置类中引入 123456@Configuration//@ComponentScan(&quot;com.itheima.config&quot;)@Import(&#123;JdbcConfig.class&#125;)public class SpringConfig &#123; &#125; 注意: 扫描注解可以移除 @Import参数需要的是一个数组，可以引入多个配置类。 @Import注解在配置类中只能写一次，下面的方式是错误的 12345678@Configuration//@ComponentScan(&quot;com.itheima.config&quot;)@Import(&#123;JdbcConfig.class,Druid.class&#125;) //这样才是正确的@Import(JdbcConfig.class)@Import(Druid.class)public class SpringConfig &#123; &#125; 7）注解方式注入第三方bean①简单数据类型注入对于下面代码关于数据库的四要素不应该写死在代码中，应该是从properties配置文件中读取。 1234567891011public class JdbcConfig &#123; @Bean public DataSource dataSource()&#123; DruidDataSource ds = new DruidDataSource(); ds.setDriverClassName(&quot;com.mysql.jdbc.Driver&quot;); ds.setUrl(&quot;jdbc:mysql://localhost:3306/spring_db&quot;); ds.setUsername(&quot;root&quot;); ds.setPassword(&quot;root&quot;); return ds; &#125;&#125; 步骤1:类中提供四个属性 123456789101112131415public class JdbcConfig &#123; private String driver; private String url; private String userName; private String password; @Bean public DataSource dataSource()&#123; DruidDataSource ds = new DruidDataSource(); ds.setDriverClassName(&quot;com.mysql.jdbc.Driver&quot;); ds.setUrl(&quot;jdbc:mysql://localhost:3306/spring_db&quot;); ds.setUsername(&quot;root&quot;); ds.setPassword(&quot;root&quot;); return ds; &#125;&#125; 步骤2:使用@Value注解引入值 12345678910111213141516171819public class JdbcConfig &#123; @Value(&quot;com.mysql.jdbc.Driver&quot;) private String driver; @Value(&quot;jdbc:mysql://localhost:3306/spring_db&quot;) private String url; @Value(&quot;root&quot;) private String userName; @Value(&quot;password&quot;) private String password; @Bean public DataSource dataSource()&#123; DruidDataSource ds = new DruidDataSource(); ds.setDriverClassName(driver); ds.setUrl(url); ds.setUsername(userName); ds.setPassword(password); return ds; &#125;&#125; ②引用类型注入假设在构建DataSource对象的时候，需要用到BookDao对象，该如何把BookDao对象注入进方法内让其使用呢? 1234567891011public class JdbcConfig &#123; @Bean public DataSource dataSource()&#123; DruidDataSource ds = new DruidDataSource(); ds.setDriverClassName(&quot;com.mysql.jdbc.Driver&quot;); ds.setUrl(&quot;jdbc:mysql://localhost:3306/spring_db&quot;); ds.setUsername(&quot;root&quot;); ds.setPassword(&quot;root&quot;); return ds; &#125;&#125; 步骤1:在SpringConfig中扫描BookDao 扫描的目的是让Spring能管理到BookDao,也就是说要让IOC容器中有一个bookDao对象 12345@Configuration@ComponentScan(&quot;com.itheima.dao&quot;)@Import(&#123;JdbcConfig.class&#125;)public class SpringConfig &#123;&#125; 步骤2:在JdbcConfig类的方法上添加参数 12345678910@Beanpublic DataSource dataSource(BookDao bookDao)&#123; System.out.println(bookDao); DruidDataSource ds = new DruidDataSource(); ds.setDriverClassName(driver); ds.setUrl(url); ds.setUsername(userName); ds.setPassword(password); return ds;&#125; 注意：引用类型注入只需要为bean定义方法设置形参即可，容器会根据类型自动装配对象。 8）XML和注解配置比较 2.spring整合mybatis1）xml配置方式①spring-mybatis.xml ②在xml中写sql 2）注解配置方式①springConfig ②jdbcConfig ③MybatisConfig 3.spring整合junit1）依赖 2）Test类 三、AOP1. 基本概念 Spring的AOP是对一个类的方法在不进行任何修改的前提下实现增强。对于上面的案例中BookServiceImpl中有save,update,delete和select方法,这些方法我们给起了一个名字叫 连接点 在BookServiceImpl的四个方法中，update和delete只有打印没有计算万次执行消耗时间，但是在运行的时候已经有该功能，那也就是说update和delete方法都已经被增强，所以对于需要增强的方法我们给起了一个名字叫 切入点 执行BookServiceImpl的update和delete方法的时候都被添加了一个计算万次执行消耗时间的功能，将这个功能抽取到一个方法中，换句话说就是存放共性功能的方法，我们给起了个名字叫 通知 通知是要增强的内容，会有多个，切入点是需要被增强的方法，也会有多个，那哪个切入点需要添加哪个通知，就需要提前将它们之间的关系描述清楚，那么对于通知和切入点之间的关系描述，我们给起了个名字叫 切面 通知是一个方法，方法不能独立存在需要被写在一个类中，这个类我们也给起了个名字叫 通知类 总结如下： 连接点(JoinPoint)：程序执行过程中的任意位置，粒度为执行方法、抛出异常、设置变量等 在SpringAOP中，理解为方法的执行 切入点(Pointcut):匹配连接点的式子 在SpringAOP中，一个切入点可以描述一个具体方法，也可也匹配多个方法 一个具体的方法:如com.itheima.dao包下的BookDao接口中的无形参无返回值的save方法 匹配多个方法:所有的save方法，所有的get开头的方法，所有以Dao结尾的接口中的任意方法，所有带有一个参数的方法 连接点范围要比切入点范围大，是切入点的方法也一定是连接点，但是是连接点的方法就不一定要被增强，所以可能不是切入点 切入点规定了哪些连接点的方法被增强： pointCut1 可能包含了update方法，没有delete方法 pointCut2 可能包含了delete方法，没有update方法 通知（Advice）:在切入点处执行的操作，也就是共性功能 在SpringAOP中，功能最终以方法的形式呈现 通知1，通知了pointCut1的所有方法，所以update方法会被增强 通知2，通知了pointCut2的所有方法，所以delete方法会被增强 通知类：定义通知的类 切面(Aspect):描述通知与切入点的对应关系 2. eg1）依赖pom.xml 12345&lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;1.9.4&lt;/version&gt;&lt;/dependency&gt; 因为spring-context中已经导入了spring-aop,所以不需要再单独导入spring-aop 导入AspectJ的jar包,AspectJ是AOP思想的一个具体实现，Spring有自己的AOP实现，但是相比于AspectJ来说比较麻烦，所以我们直接采用Spring整合ApsectJ的方式进行AOP开发。 2）通知类BookDaoImpl中有两个方法，分别是save和update，我们要增强的是update方法，该如何定义呢? 1234567891011@Component@Aspectpublic class MyAdvice &#123; @Pointcut(&quot;execution(void com.itheima.dao.BookDao.update())&quot;) private void pt()&#123;&#125; @Before(&quot;pt()&quot;) public void method()&#123; System.out.println(System.currentTimeMillis()); &#125;&#125; 说明: 切入点定义依托一个不具有实际意义的方法进行，即无参数、无返回值、方法体无实际逻辑。 private void pt()&#123;&#125; 3）开启注解格式AOP功能12345@Configuration@ComponentScan(&quot;com.itheima&quot;)@EnableAspectJAutoProxypublic class SpringConfig &#123;&#125; @EnableAspectJAutoProxy开启AOP 3. 注解1）@EnableAspectJAutoProxy 名称 @EnableAspectJAutoProxy 类型 配置类注解 位置 配置类定义上方 作用 开启注解格式AOP功能 2）@Aspect 名称 @Aspect 类型 类注解 位置 切面类定义上方 作用 设置当前类为AOP切面类 3）@Pointcut 名称 @Pointcut 类型 方法注解 位置 切入点方法定义上方 作用 设置切入点方法 属性 value（默认）：切入点表达式 4）@Before 名称 @Before 类型 方法注解 位置 通知方法定义上方 作用 设置当前通知方法与切入点之间的绑定关系，当前通知方法在原始切入点方法前运行 4. AOP原理探究1）执行流程由于AOP是基于Spring容器管理的bean做的增强，所以整个工作过程需要从Spring加载bean说起: ①Spring容器启动 容器启动就需要去加载bean,哪些类需要被加载呢? 需要被增强的类，如:BookServiceImpl 通知类，如:MyAdvice 注意此时bean对象还没有创建成功 ②读取所有切面配置中的切入点 上面这个例子中有两个切入点的配置，但是第一个ptx()并没有被使用，所以不会被读取 ③初始化bean判定bean对应的类中的方法是否匹配到任意切入点 注意第1步在容器启动的时候，bean对象还没有被创建成功。 要被实例化bean对象的类中的方法和切入点进行匹配 匹配失败，创建原始对象，如UserDao 匹配失败说明不需要增强，直接调用原始对象的方法即可。 匹配成功，创建原始对象（目标对象）的**代理对象,**如:BookDao 匹配成功说明需要对其进行增强 对哪个类做增强，这个类对应的对象就叫做目标对象 因为要对目标对象进行功能增强，而采用的技术是动态代理，所以会为其创建一个代理对象 最终运行的是代理对象的方法，在该方法中会对原始方法进行功能增强 ④获取bean执行方法 获取的bean是原始对象时，调用方法并执行，完成操作 获取的bean是代理对象时，根据代理对象的运行模式运行原始方法与增强的内容，完成操作 2）验证容器中是否为代理对象为了验证IOC容器中创建的对象和我们刚才所说的结论是否一致，首先先把结论理出来: 如果目标对象中的方法会被增强，那么容器中将存入的是目标对象的代理对象 如果目标对象中的方法不被增强，那么容器中将存入的是目标对象本身。 验证思路 1.要执行的方法，不被定义的切入点包含，即不要增强，打印当前类的getClass()方法 2.要执行的方法，被定义的切入点包含，即要增强，打印出当前类的getClass()方法 3.观察两次打印的结果 步骤1:修改App类,获取类的类型 12345678public class App &#123; public static void main(String[] args) &#123; ApplicationContext ctx = new AnnotationConfigApplicationContext(SpringConfig.class); BookDao bookDao = ctx.getBean(BookDao.class); System.out.println(bookDao); System.out.println(bookDao.getClass()); &#125;&#125; 步骤2:修改MyAdvice类，不增强 因为定义的切入点中，被修改成update1,所以BookDao中的update方法在执行的时候，就不会被增强， 所以容器中的对象应该是目标对象本身。 1234567891011@Component@Aspectpublic class MyAdvice &#123; @Pointcut(&quot;execution(void com.itheima.dao.BookDao.update1())&quot;)//这里是update1() private void pt()&#123;&#125; @Before(&quot;pt()&quot;) public void method()&#123; System.out.println(System.currentTimeMillis()); &#125;&#125; 这里是update1()，所以bookDao中并没有方法匹配上了 步骤3:运行程序 步骤4:修改MyAdvice类，增强 因为定义的切入点中，被修改成update,所以BookDao中的update方法在执行的时候，就会被增强， 所以容器中的对象应该是目标对象的代理对象 1234567891011@Component@Aspectpublic class MyAdvice &#123; @Pointcut(&quot;execution(void com.itheima.dao.BookDao.update())&quot;) private void pt()&#123;&#125; @Before(&quot;pt()&quot;) public void method()&#123; System.out.println(System.currentTimeMillis()); &#125;&#125; 步骤5:运行程序 至此对于刚才的结论，我们就得到了验证，需要注意的是: 不能直接打印对象，从上面两次结果中可以看出，直接打印对象走的是对象的toString方法，不管是不是代理对象打印的结果都是一样的，原因是内部对toString方法进行了重写，需要打印getClass() System.out.println(bookDao);System.out.println(bookDao.getClass()); 3）AOP核心概念在上面介绍AOP的工作流程中，有两个核心概念，分别是: 目标对象(Target)：原始功能去掉共性功能对应的类产生的对象，这种对象是无法直接完成最终工作的 目标对象就是要增强的类[如:BookServiceImpl类]对应的对象，也叫原始对象，不能说它不能运行，只能说它在运行的过程中对于要增强的内容是缺失的 代理(Proxy)：目标对象无法直接完成工作，需要对其进行功能回填，通过原始对象的代理对象实现 SpringAOP是在不改变原有设计(代码)的前提下对其进行增强的，它的底层采用的是代理模式实现的，所以要对原始对象进行增强，就需要对原始对象创建代理对象，在代理对象中的方法把通知 如:MyAdvice中的method方法 内容加进去，就实现了增强,这就是我们所说的代理(Proxy) 5.切入点表达式1）语法格式首先我们先要明确两个概念： 切入点:要进行增强的方法 切入点表达式:要进行增强的方法的描述方式 对于切入点的描述，我们其实是有两中方式的，先来看下前面的例子 描述方式一：执行com.itheima.dao包下的BookDao接口中的无参数update方法 1execution(void com.itheima.dao.BookDao.update()) 描述方式二：执行com.itheima.dao.impl包下的BookDaoImpl类中的无参数update方法 1execution(void com.itheima.dao.impl.BookDaoImpl.update()) 因为调用接口方法的时候最终运行的还是其实现类的方法，所以上面两种描述方式都是可以的。 对于切入点表达式的语法为: 切入点表达式标准格式：动作关键字(访问修饰符 返回值 包名.类&#x2F;接口名.方法名(参数) 异常名） 对于这个格式，我们不需要硬记，通过一个例子，理解它: 1execution(public User com.itheima.service.UserService.findById(int)) execution：动作关键字，描述切入点的行为动作，例如execution表示执行到指定切入点 public：访问修饰符,还可以是public，private等，可以省略 User：返回值，写返回值类型 com.itheima.service：包名，多级包使用点连接 UserService：类&#x2F;接口名称 findById：方法名 int：参数，直接写参数的类型，多个类型用逗号隔开 异常名：方法定义中抛出指定异常，可以省略 2）通配符我们使用通配符描述切入点，主要的目的就是简化之前的配置： *:单个独立的任意符号，可以独立出现，也可以作为前缀或者后缀的匹配符出现 1execution（public * com.itheima.*.UserService.find*(*)) 匹配com.itheima包下的任意包中的UserService类或接口中所有find开头的带有一个参数的方法 ..：多个连续的任意符号，可以独立出现，常用于简化包名与参数的书写 1execution（public User com..UserService.findById(..)) 匹配com包下的任意包中的UserService类或接口中所有名称为findById的方法 +：专用于匹配子类类型 1execution(* *..*Service+.*(..)) 这个使用率较低，描述子类的，咱们做JavaEE开发，继承机会就一次，使用都很慎重，所以很少用它。*Service+，表示所有以Service结尾的接口的子类。 接下来，我们把案例中使用到的切入点表达式来分析下: 12345678910111213141516171819202122232425262728execution(void com.itheima.dao.BookDao.update())匹配接口，能匹配到execution(void com.itheima.dao.impl.BookDaoImpl.update())匹配实现类，能匹配到execution(* com.itheima.dao.impl.BookDaoImpl.update())返回值任意，能匹配到execution(* com.itheima.dao.impl.BookDaoImpl.update(*))返回值任意，但是update方法必须要有一个参数，无法匹配，要想匹配需要在update接口和实现类添加参数execution(void com.*.*.*.*.update())返回值为void,com包下的任意包三层包下的任意类的update方法，匹配到的是实现类，能匹配execution(void com.*.*.*.update())返回值为void,com包下的任意两层包下的任意类的update方法，匹配到的是接口，能匹配execution(void *..update())返回值为void，方法名是update的任意包下的任意类，能匹配execution(* *..*(..))匹配项目中任意类的任意方法，能匹配，但是不建议使用这种方式，影响范围广execution(* *..u*(..))匹配项目中任意包任意类下只要以u开头的方法，update方法能满足，能匹配execution(* *..*e(..))匹配项目中任意包任意类下只要以e结尾的方法，update和save方法能满足，能匹配execution(void com..*())返回值为void，com包下的任意包任意类任意方法，能匹配，*代表的是方法//..............以下是常用写法.......................execution(* com.itheima.*.*Service.find*(..))将项目中所有业务层方法的以find开头的方法匹配execution(* com.itheima.*.*Service.save*(..))将项目中所有业务层方法的以save开头的方法匹配 后面两种更符合我们平常切入点表达式的编写规则 3）书写技巧对于切入点表达式的编写其实是很灵活的，那么在编写的时候，有没有什么好的技巧让我们用用: 所有代码按照标准规范开发，否则以下技巧全部失效 描述切入点通常描述接口，而不描述实现类,如果描述到实现类，就出现紧耦合了 访问控制修饰符针对接口开发均采用public描述（可省略访问控制修饰符描述） 返回值类型对于增删改类使用精准类型加速匹配，对于查询类使用*通配快速描述 包名书写尽量不使用..匹配，效率过低，常用*做单个包描述匹配，或精准匹配 接口名&#x2F;类名书写名称与模块相关的采用*匹配，例如UserService书写成*Service，绑定业务层接口名 方法名书写以动词进行精准匹配，名词采用*匹配， 例如getById书写成getBy*，selectAll书写成selectAll 参数规则较为复杂，根据业务方法灵活调整 通常不使用异常作为匹配规则 6.通知类型1）前置通知修改MyAdvice,在before方法上添加@Before注解 123456789101112@Component@Aspectpublic class MyAdvice &#123; @Pointcut(&quot;execution(void com.itheima.dao.BookDao.update())&quot;) private void pt()&#123;&#125; @Before(&quot;pt()&quot;) //此处也可以写成 @Before(&quot;MyAdvice.pt()&quot;),不建议 public void before() &#123; System.out.println(&quot;before advice ...&quot;); &#125;&#125; 2）后置通知123456789101112131415@Component@Aspectpublic class MyAdvice &#123; @Pointcut(&quot;execution(void com.itheima.dao.BookDao.update())&quot;) private void pt()&#123;&#125; @Before(&quot;pt()&quot;) public void before() &#123; System.out.println(&quot;before advice ...&quot;); &#125; @After(&quot;pt()&quot;) public void after() &#123; System.out.println(&quot;after advice ...&quot;); &#125;&#125; 后置通知就类似于finally中的代码 3）返回后通知1234567891011121314@Component@Aspectpublic class MyAdvice &#123; @Pointcut(&quot;execution(void com.itheima.dao.BookDao.update())&quot;) private void pt()&#123;&#125; @Pointcut(&quot;execution(int com.itheima.dao.BookDao.select())&quot;) private void pt2()&#123;&#125; @AfterReturning(&quot;pt2()&quot;) public void afterReturning() &#123; System.out.println(&quot;afterReturning advice ...&quot;); &#125;&#125; 注意：返回后通知是需要在原始方法select正常执行后才会被执行，如果select()方法执行的过程中出现了异常，那么返回后通知是不会被执行。后置通知是不管原始方法有没有抛出异常都会被执行。 4）异常后通知1234567891011121314@Component@Aspectpublic class MyAdvice &#123; @Pointcut(&quot;execution(void com.itheima.dao.BookDao.update())&quot;) private void pt()&#123;&#125; @Pointcut(&quot;execution(int com.itheima.dao.BookDao.select())&quot;) private void pt2()&#123;&#125; @AfterReturning(&quot;pt2()&quot;) public void afterThrowing() &#123; System.out.println(&quot;afterThrowing advice ...&quot;); &#125;&#125; 注意：异常后通知是需要原始方法抛出异常，可以在select()方法中添加一行代码int i = 1/0即可。如果没有抛异常，异常后通知将不会被执行。 5）环绕通知①基本使用 1234567891011121314@Component@Aspectpublic class MyAdvice &#123; @Pointcut(&quot;execution(void com.itheima.dao.BookDao.update())&quot;) private void pt()&#123;&#125; @Around(&quot;pt()&quot;) public void around(ProceedingJoinPoint pjp) throws Throwable&#123; System.out.println(&quot;around before advice ...&quot;);//前置 //表示对原始操作的调用 pjp.proceed(); System.out.println(&quot;around after advice ...&quot;);//返回后 &#125;&#125; 说明：proceed()可能会抛出异常 原因很简单，看下源码就知道了 这也很好理解，因为pjp.proceed();表示对原始操作的调用，原方法就可能会抛出异常 ②一个问题原始方法有返回值的处理 修改MyAdvice,对BookDao中的select方法添加环绕通知，select的返回值是int，update没有返回值 1234567891011121314151617@Component@Aspectpublic class MyAdvice &#123; @Pointcut(&quot;execution(void com.itheima.dao.BookDao.update())&quot;) private void pt()&#123;&#125; @Pointcut(&quot;execution(int com.itheima.dao.BookDao.select())&quot;) private void pt2()&#123;&#125; @Around(&quot;pt2()&quot;) public void aroundSelect(ProceedingJoinPoint pjp) throws Throwable &#123; System.out.println(&quot;around before advice ...&quot;); //表示对原始操作的调用 pjp.proceed(); System.out.println(&quot;around after advice ...&quot;); &#125;&#125; 修改App类，调用select方法 12345678public class App &#123; public static void main(String[] args) &#123; ApplicationContext ctx = new AnnotationConfigApplicationContext(SpringConfig.class); BookDao bookDao = ctx.getBean(BookDao.class); int num = bookDao.select(); System.out.println(num); &#125;&#125; 运行后会报错，错误内容为: 1234Exception in thread &quot;main&quot; org.springframework.aop.AopInvocationException: ==Null return value from advice does not match primitive return type for: public abstract int com.itheima.dao.BookDao.select()== at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:226) at com.sun.proxy.$Proxy19.select(Unknown Source) at com.itheima.App.main(App.java:12) 错误大概的意思是：空的返回不匹配原始方法的int返回 因为目标方法select是有返回值int的，而我们around方法现在的返回值是void 所以如果我们使用环绕通知的话，要根据原始方法的返回值来设置环绕通知的返回值，具体解决方案为： 123456789101112131415161718@Component@Aspectpublic class MyAdvice &#123; @Pointcut(&quot;execution(void com.itheima.dao.BookDao.update())&quot;) private void pt()&#123;&#125; @Pointcut(&quot;execution(int com.itheima.dao.BookDao.select())&quot;) private void pt2()&#123;&#125; @Around(&quot;pt2()&quot;) public Object aroundSelect(ProceedingJoinPoint pjp) throws Throwable &#123; System.out.println(&quot;around before advice ...&quot;); //表示对原始操作的调用 Object ret = pjp.proceed(); System.out.println(&quot;around after advice ...&quot;); return ret; &#125;&#125; 为什么返回的是Object而不是int的主要原因是Object类型更通用 在环绕通知中是可以对原始方法返回值就行修改的 7. AOP通知获取数据1）获取参数①非环绕通知获取方式使用JoinPoint的方式获取参数适用于前置、后置、返回后、抛出异常后通知 在方法上添加JoinPoint,通过JoinPoint来获取参数 1234567891011121314@Component@Aspectpublic class MyAdvice &#123; @Pointcut(&quot;execution(* com.itheima.dao.BookDao.findName(..))&quot;) private void pt()&#123;&#125; @Before(&quot;pt()&quot;) public void before(JoinPoint jp) Object[] args = jp.getArgs(); System.out.println(Arrays.toString(args)); System.out.println(&quot;before advice ...&quot; ); &#125; //...其他的略&#125; 运行App类，可以获取如下内容，说明参数100已经被获取 思考：方法的参数只有一个，为什么获取的是一个数组? 因为参数的个数是不固定的，所以使用数组更通配些。 如果将参数改成两个会是什么效果呢? 修改BookDao接口和BookDaoImpl实现类 1234567891011public interface BookDao &#123; public String findName(int id,String password);&#125;@Repositorypublic class BookDaoImpl implements BookDao &#123; public String findName(int id,String password) &#123; System.out.println(&quot;id:&quot;+id); return &quot;itcast&quot;; &#125;&#125; 修改App类，调用方法传入多个参数 12345678public class App &#123; public static void main(String[] args) &#123; ApplicationContext ctx = new AnnotationConfigApplicationContext(SpringConfig.class); BookDao bookDao = ctx.getBean(BookDao.class); String name = bookDao.findName(100,&quot;itheima&quot;); System.out.println(name); &#125;&#125; 运行App，查看结果,说明两个参数都已经被获取到 ②环绕通知获取方式环绕通知使用的是ProceedingJoinPoint，因为ProceedingJoinPoint是JoinPoint类的子类，所以对于ProceedingJoinPoint类中应该也会有对应的getArgs()方法: 123456789101112131415@Component@Aspectpublic class MyAdvice &#123; @Pointcut(&quot;execution(* com.itheima.dao.BookDao.findName(..))&quot;) private void pt()&#123;&#125; @Around(&quot;pt()&quot;) public Object around(ProceedingJoinPoint pjp)throws Throwable &#123; Object[] args = pjp.getArgs(); System.out.println(Arrays.toString(args)); Object ret = pjp.proceed(); return ret; &#125; //其他的略&#125; 运行App后查看运行结果，说明ProceedingJoinPoint也是可以通过getArgs()获取参数 注意: pjp.proceed()方法是有两个构造方法，分别是: 调用无参数的proceed，当原始方法有参数，会在调用的过程中自动传入参数 所以调用这两个方法的任意一个都可以完成功能 但是当需要修改原始方法的参数时，就只能采用带有参数的方法,如下: 12345678910111213141516@Component@Aspectpublic class MyAdvice &#123; @Pointcut(&quot;execution(* com.itheima.dao.BookDao.findName(..))&quot;) private void pt()&#123;&#125; @Around(&quot;pt()&quot;) public Object around(ProceedingJoinPoint pjp) throws Throwable&#123; Object[] args = pjp.getArgs(); System.out.println(Arrays.toString(args)); args[0] = 666; Object ret = pjp.proceed(args); return ret; &#125; //其他的略&#125; 首先获得原始参数args 然后修改args内容 执行的时候传入修改过的args 有了这个特性后，我们就可以在环绕通知中对原始方法的参数进行拦截过滤，避免由于参数的问题导致程序无法正确运行，保证代码的健壮性。 2）获取返回值①返回后通知获取返回值1234567891011@Component@Aspectpublic class MyAdvice &#123; @Pointcut(&quot;execution(* com.itheima.dao.BookDao.findName(..))&quot;) private void pt()&#123;&#125; @AfterReturning(value = &quot;pt()&quot;,returning = &quot;ret&quot;) public void afterReturning(Object ret) &#123; System.out.println(&quot;afterReturning advice ...&quot;+ret); &#125;&#125; 注意： 参数名的问题 afterReturning方法参数类型的问题 参数类型可以写成String，但是为了能匹配更多的参数类型，建议写成Object类型 afterReturning方法参数的顺序问题 运行App后查看运行结果，说明返回值已经被获取到 ②环绕通知获取方式12345678910111213141516@Component@Aspectpublic class MyAdvice &#123; @Pointcut(&quot;execution(* com.itheima.dao.BookDao.findName(..))&quot;) private void pt()&#123;&#125; @Around(&quot;pt()&quot;) public Object around(ProceedingJoinPoint pjp) throws Throwable&#123; Object[] args = pjp.getArgs(); System.out.println(Arrays.toString(args)); args[0] = 666; Object ret = pjp.proceed(args); //..可以对ret进行修改 return ret; &#125;&#125; 上述代码中，ret就是方法的返回值，我们是可以直接获取，不但可以获取，如果需要还可以进行修改。 3）获取异常①抛出异常后通知获取异常123456789101112@Component@Aspectpublic class MyAdvice &#123; @Pointcut(&quot;execution(* com.itheima.dao.BookDao.findName(..))&quot;) private void pt()&#123;&#125; @AfterThrowing(value = &quot;pt()&quot;,throwing = &quot;t&quot;) public void afterThrowing(Throwable t) &#123; System.out.println(&quot;afterThrowing advice ...&quot;+t); &#125; //其他的略&#125; 注意： 运行App后，查看控制台，就能看的异常信息被打印到控制台 ②环绕通知获取异常只需要将异常捕获，就可以获取到原始方法的异常信息了 123456789101112131415161718192021@Component@Aspectpublic class MyAdvice &#123; @Pointcut(&quot;execution(* com.itheima.dao.BookDao.findName(..))&quot;) private void pt()&#123;&#125; @Around(&quot;pt()&quot;) public Object around(ProceedingJoinPoint pjp)&#123; Object[] args = pjp.getArgs(); System.out.println(Arrays.toString(args)); args[0] = 666; Object ret = null; try&#123; ret = pjp.proceed(args); &#125;catch(Throwable throwable)&#123; t.printStackTrace();//捕获异常 &#125; return ret; &#125; //其他的略&#125; 在catch方法中就可以获取到异常，然后对异常进行处理 四、事务管理1. 基本知识 事务作用：在数据层保障一系列的数据库操作同成功同失败 Spring事务作用：在数据层或业务层保障一系列的数据库操作同成功同失败 数据层有事务我们可以理解，为什么业务层也需要处理事务呢? 举个简单的例子： 转账业务会有两次数据层的调用，一次是加钱一次是减钱 在业务层转账就会调用加钱和减钱的数据层方法，它们分别是两个事务，但是在service中并不是一个业务 没办法保证加钱和减钱同时成功或者同时失败 这个时候就需要将事务放在业务层进行处理 Spring为了管理事务，提供了一个平台事务管理器PlatformTransactionManager commit是用来提交事务，rollback是用来回滚事务。 PlatformTransactionManager只是一个接口，Spring还为其提供了一个具体的实现: 从名称上可以看出，我们只需要给它一个DataSource对象，它就可以帮你去在业务层管理事务。其内部采用的是JDBC的事务。所以说如果你持久层采用的是JDBC相关的技术，就可以采用这个事务管理器来管理你的事务。而Mybatis内部采用的就是JDBC的事务，所以后期我们Spring整合Mybatis就采用的这个DataSourceTransactionManager事务管理器。 注意DataSourceTransactionManager内部采用的是JDBC事务，并且mybatis数据层也是采用JDBC，所以我们可以使用 事务管理器要根据使用技术进行选择 eg： 123456789101112@servicepublic class Accountservicermp1 implements Accountservice &#123; @Autowired private AccountDao accountDao; public void transfer(string out,string in ,boub1e money) &#123; //以下操作并不能保证原子执行 accountDao . outMoney(out , money); int i = 1/0; accountDao .inMoney(in , money); &#125;&#125; 如图，①执行完，1&#x2F;0报错，②就不会执行，这样transfer事务就失去了原子性 2. 相关注解@EnableTransactionManagement 名称 @EnableTransactionManagement 类型 配置类注解 位置 配置类定义上方 作用 设置当前Spring环境中开启注解式事务支持 @Transactional 名称 @Transactional 类型 接口注解 类注解 方法注解 位置 业务层接口上方 业务层实现类上方 业务方法上方 作用 为当前业务层方法添加事务（如果设置在类或接口上方则类或接口中所有方法均添加事务） 3. 实现example1）添加@Transactional@Transactional指明具体在什么位置需要开启事务 12345678910111213141516171819202122232425public interface AccountService &#123; /** * 转账操作 * @param out 传出方 * @param in 转入方 * @param money 金额 */ //配置当前接口方法具有事务 public void transfer(String out,String in ,Double money) ;&#125;@Servicepublic class AccountServiceImpl implements AccountService &#123; @Autowired private AccountDao accountDao; @Transactional//添加事务 public void transfer(String out,String in ,Double money) &#123; accountDao.outMoney(out,money); int i = 1/0; accountDao.inMoney(in,money); &#125;&#125; 注意： @Transactional可以写在接口类上、接口方法上、实现类上和实现类方法上 写在接口类上，该接口的所有实现类的所有方法都会有事务 写在接口方法上，该接口的所有实现类的该方法都会有事务 写在实现类上，该类中的所有方法都会有事务 写在实现类方法上，该方法上有事务 建议写在实现类或实现类的方法上 2）配置事务管理器12345678910111213141516171819202122232425262728public class JdbcConfig &#123; @Value(&quot;$&#123;jdbc.driver&#125;&quot;) private String driver; @Value(&quot;$&#123;jdbc.url&#125;&quot;) private String url; @Value(&quot;$&#123;jdbc.username&#125;&quot;) private String userName; @Value(&quot;$&#123;jdbc.password&#125;&quot;) private String password; @Bean public DataSource dataSource()&#123; DruidDataSource ds = new DruidDataSource(); ds.setDriverClassName(driver); ds.setUrl(url); ds.setUsername(userName); ds.setPassword(password); return ds; &#125; //配置事务管理器，mybatis使用的是jdbc事务 @Bean public PlatformTransactionManager transactionManager(DataSource dataSource)&#123; DataSourceTransactionManager transactionManager = new DataSourceTransactionManager(); transactionManager.setDataSource(dataSource); return transactionManager; &#125;&#125; 注意： 事务管理器要根据使用技术进行选择，Mybatis框架使用的是JDBC事务，可以直接使用DataSourceTransactionManager xml配置事务，都是要在config中配置一个transactionManager的bean： 3）开启声明式事务@EnableTransactionManagement可以开启spring的声明式事务 12345678@Configuration@ComponentScan(&quot;com.itheima&quot;)@PropertySource(&quot;classpath:jdbc.properties&quot;)@Import(&#123;JdbcConfig.class,MybatisConfig.class//开启注解式事务驱动@EnableTransactionManagementpublic class SpringConfig &#123;&#125; xml方式： 4. 原理开启事务前： 开启事务后： transfer上添加了@Transactional注解，在该方法上就会有一个事务T AccountDao的outMoney方法的事务T1加入到transfer的事务T中 AccountDao的inMoney方法的事务T2加入到transfer的事务T中 这样就保证他们在同一个事务中，当业务层中出现异常，整个事务就会回滚，保证数据的准确性。 spring事务管理的数据源和mybatis的数据源是同一个数据源！！！ 事务管理员和事务协调员： 事务管理员：发起事务方，在Spring中通常指代业务层开启事务的方法 事务协调员：加入事务方，在Spring中通常指代数据层方法，也可以是业务层方法 5. 事务属性 上面这些属性都可以在@Transactional注解的参数上进行设置。 readOnly：true只读事务，false读写事务，增删改要设为false,查询设为true。 timeout：设置超时时间单位秒，在多长时间之内事务没有提交成功就自动回滚，-1表示不设置超时时间。 rollbackFor：当出现指定异常进行事务回滚 noRollbackFor：当出现指定异常不进行事务回滚 思考：出现异常事务会自动回滚，这个是我们之前就已经知道的 noRollbackFor是设定对于指定的异常不回滚，这个好理解 rollbackFor是指定回滚异常，对于异常事务不应该都回滚么，为什么还要指定? 这是因为并不是所有的异常都会回滚事务，比如下面的代码就不会回滚 123456789101112131415161718192021222324252627public interface AccountService &#123; /** * 转账操作 * @param out 传出方 * @param in 转入方 * @param money 金额 */ //配置当前接口方法具有事务 public void transfer(String out,String in ,Double money) throws IOException;&#125;@Servicepublic class AccountServiceImpl implements AccountService &#123; @Autowired private AccountDao accountDao; @Transactional public void transfer(String out,String in ,Double money) throws IOException&#123; accountDao.outMoney(out,money); //int i = 1/0; //这个异常事务会回滚 if(true)&#123; throw new IOException(); //这个异常事务就不会回滚 &#125; accountDao.inMoney(in,money); &#125;&#125; 出现这个问题的原因是，Spring的事务只会对Error异常和RuntimeException异常及其子类进行事务回滚，其他的异常类型是不会回滚的，对应IOException不符合上述条件所以不回滚 此时就可以使用rollbackFor属性来设置出现IOException异常不回滚 12345678910111213141516@Servicepublic class AccountServiceImpl implements AccountService &#123; @Autowired private AccountDao accountDao; @Transactional(rollbackFor = &#123;IOException.class&#125;) public void transfer(String out,String in ,Double money) throws IOException&#123; accountDao.outMoney(out,money); //int i = 1/0; //这个异常事务会回滚 if(true)&#123; throw new IOException(); //这个异常事务就不会回滚 &#125; accountDao.inMoney(in,money); &#125;&#125; rollbackForClassName等同于rollbackFor,只不过属性为异常的类全名字符串 noRollbackForClassName等同于noRollbackFor，只不过属性为异常的类全名字符串 isolation设置事务的隔离级别 DEFAULT :默认隔离级别, 会采用数据库的隔离级别 READ_UNCOMMITTED : 读未提交 READ_COMMITTED : 读已提交 REPEATABLE_READ : 重复读取 SERIALIZABLE: 串行化 注意： ​ spring并不会对所有异常都进行回滚，只会对RuntimeException和Error及其子类回滚，其它异常类型不会回滚 6. 事务的传播行为1）example步骤1:mysql创建日志表 12345create table tbl_log( id int primary key auto_increment, info varchar(255), createDate datetime) 步骤2:添加LogDao接口 12345public interface LogDao &#123; @Insert(&quot;insert into tbl_log (info,createDate) values(#&#123;info&#125;,now())&quot;) void log(String info);&#125; 步骤3:添加LogService接口与实现类 12345678910111213public interface LogService &#123; void log(String out, String in, Double money);&#125;@Servicepublic class LogServiceImpl implements LogService &#123; @Autowired private LogDao logDao; @Transactional public void log(String out,String in,Double money ) &#123; logDao.log(&quot;转账操作由&quot;+out+&quot;到&quot;+in+&quot;,金额：&quot;+money); &#125;&#125; 步骤4:在转账的业务中添加记录日志 12345678910111213141516171819202122232425262728public interface AccountService &#123; /** * 转账操作 * @param out 传出方 * @param in 转入方 * @param money 金额 */ //配置当前接口方法具有事务 public void transfer(String out,String in ,Double money)throws IOException ;&#125;@Servicepublic class AccountServiceImpl implements AccountService &#123; @Autowired private AccountDao accountDao; @Autowired private LogService logService; @Transactional public void transfer(String out,String in ,Double money) &#123; try&#123; accountDao.outMoney(out,money); accountDao.inMoney(in,money); &#125;finally &#123; logService.log(out,in,money); &#125; &#125;&#125; 步骤5:运行程序 当程序正常运行，tbl_account表中转账成功，tbl_log表中日志记录成功 当转账业务之间出现异常(int i &#x3D;1&#x2F;0),转账失败，tbl_account成功回滚，但是tbl_log表未添加数据 这并不是我们希望的，无论是否成功，我们都要添加日志 失败原因:日志的记录与转账操作隶属同一个事务，同成功同失败 最终效果:无论转账操作是否成功，日志必须保留 2）原因 对于上述案例的分析: log方法、inMoney方法和outMoney方法都属于增删改，分别有事务T1,T2,T3 transfer因为加了@Transactional注解，开启了事务T Spring事务会把T1,T2,T3都加入到事务T中X 所以当转账失败后，所有的事务都回滚，导致日志没有记录下来 这和我们的需求不符，这个时候我们就需要让log方法单独是一个事务 这里就绪需要用到事务传播行为，所谓的事务传播行为指的是: 事务传播行为：事务协调员对事务管理员所携带事务的处理态度 也就是加入事务T的T1，T2，T3对待T事务的态度 涉及到propagation属性。 解决：修改logService改变事务的传播行为 1234567891011@Servicepublic class LogServiceImpl implements LogService &#123; @Autowired private LogDao logDao; //propagation设置事务属性：传播行为设置为当前操作需要新事务 @Transactional(propagation = Propagation.REQUIRES_NEW) public void log(String out,String in,Double money ) &#123; logDao.log(&quot;转账操作由&quot;+out+&quot;到&quot;+in+&quot;,金额：&quot;+money); &#125;&#125; ​ 本例中logService.log作为事务协调员有一个自己的事务T1，transfer作为事务管理员有一个自己的事务T，但是在配置事务传播属性propagation&#x3D;Propagation.REQUIRES_NEW后，T1不会加入T事务，T是否失败不会影响到T2。 3）事务传播属性 REQUIRED： 当事务管理员开启一个事务T时，事务协调员不管是否有事务都加入T 事务管理员没有事务T，事务协调员就会开启一个自己的事务T2 REQUIRES_NEW： 当事务管理员开启一个事务T时，事务协调员不会加入T，而是开启一个自己的事务T2 事务管理员没有事务T，事务协调员就会开启一个自己的事务T2","categories":[{"name":"SSM框架","slug":"SSM框架","permalink":"http://example.com/categories/SSM%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://example.com/tags/Spring/"}]},{"title":"操作系统-概述","slug":"1.操作系统-概述","date":"2022-03-06T11:17:48.000Z","updated":"2022-09-12T14:40:39.861Z","comments":true,"path":"2022/03/06/1.操作系统-概述/","link":"","permalink":"http://example.com/2022/03/06/1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-%E6%A6%82%E8%BF%B0/","excerpt":"四大特征、发展过程、内核态和用户态、中断、系统调用","text":"四大特征、发展过程、内核态和用户态、中断、系统调用 1.操作系统的特征 并发： ​ 宏观上是同时发生的，实际微观上是交替进行的 共享： ​ 互斥共享：摄像头资源一段时间只能供一个进程访问 ​ 同时共享：硬盘资源一段时间可供多个进程访问，实际微观上也是交替访问 虚拟： ​ 空分复用技术：运行的程序共需8G内存，实际上只有4G ​ 时分复用技术：单核CPU在一段时间内运行多段程序，看起来是由多个CPU在完成工作 异步： ​ 由于多个程序并发执行，程序间就有可能因获取资源而被堵塞，以不可预知的速度进行。 2.操作系统的发展 3.内核态和用户态 1）内核程序负责系统资源的管理，只有内核程序可以使用特权指令 2）应用程序只能使用非特权指令 3）内核由很多内核程序组成 4）内核态-&gt;用户态，内核程序执行一条修改PSW的特权指令 5）用户态-&gt;内核态，中断引起，硬件自动完成 3.中断1）中断分类 内中断：和CPU内部执行的指令有关 ​ 陷入：用户程序主动执行，是一条特殊的非特权指令，用去请求调用系统服务 ​ 故障：故障可能被修复，被修复后会继续执行 ​ 终止：如正数&#x2F;0，用户程序非法使用特权指令，终止后不会再执行该程序 外中断：与CPU内部无关，中断信号来自外部 ​ 时钟中断：时钟定时中断用户程序1，CPU处理中断信号让用户程序2执行 ​ I&#x2F;0中断：输入输出设备发出的中断信号 2）中断原理 CPU会根据中断信号查询 中断向量表 ，通过指针找到响应的中断处理程序（内核程序） 内中断：在每条指令执行的时候检查是否有异常发生 外中断：在每个指令周期末尾，CPU都会检查外中断 4.系统调用 调用创建文件的库函数后，需要使用 系统调用 ，这时候会传入一个参数（通过传入的参数找到对应的系统调用），执行trap函数进入内核态，执行系统调用（会执行特权指令），执行完以后返回 系统调用实际上就是由操作系统执行的底层的操作：如创建文件、删除文件等 5.操作系统体系结构 大内核和微内核其实的区别就是是否将 进程管理、存储管理、设备管理 纳入内核","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://example.com/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"http://example.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}]},{"title":"操作系统笔记-内存","slug":"3.操作系统-内存","date":"2022-03-06T11:17:48.000Z","updated":"2022-06-23T10:29:55.956Z","comments":true,"path":"2022/03/06/3.操作系统-内存/","link":"","permalink":"http://example.com/2022/03/06/3.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-%E5%86%85%E5%AD%98/","excerpt":"操作系统! * !","text":"操作系统! * ! 三、内存1.内存基本知识 2.内存管理1）内存空间的分配与回收 2）内存空间的扩展 3）地址转换 4）内存保护上限寄存器： 重定位寄存器+界地址寄存器： 重定位寄存器记录了该段数据的起始地址，起始地址+逻辑地址&#x3D;物理地址 3.内存的覆盖和交换（扩存）1）覆盖 固定区内存是一直都在的，覆盖区内存可能被覆盖 2）交换 交换其实就是中级调度，调出部分进程到外存的挂起队列，缓解内存的压力 外存分为对换区和文件区，调出的进程会在对换区，对换区IO速度&gt;文件区 4.连续分配管理方式1）单一连续分配 所有的用户区只让一个进程使用，无外部碎片，有内部碎片 2）固定分区分配 分区大小相等和分区大小不等两种，无外部碎片，有内部碎片，若某进程超过最大分区需要覆盖 3）动态分区分配 数据结构： 动态分区分配算法 回收：相邻的空闲分区进行合并 5.基本内存分页管理（离散） 1）分页存储 ​ 分页存储实际就是将内存分成一个个的大小相等的 页框，然后再将进程根据页框大小分成一个个的 页面，最后将这些一个个的页面离散地存放在一个个的页框中。进程的最后一个页面可能并没有页框那么大，这样就有可能产生内部碎片。 ​ 内部碎片：进程在使用内存中分配的空间剩余的空间，如该进程6kb，分配了8kb内存，2kb就是内部碎片 ​ 外部碎片：内存中两个划分区域间难以利用的小空间 2）页表 页表记录了该进程各个页面与页框的映射信息，页表存储在PCB中：页号是隐含的分页顺序，块号是该页在内存中的页框号。 由于一个 页框大小&#x3D;页面大小&#x3D;4KB，所以4GB内存可以有2^32&#x2F;2^12&#x3D;2^20个内存块， 那么至少需要20位来表达页框的编号，由于计算机是B来存，那么就需要3B &#x3D; 24b来表示，2B &#x3D; 16b&lt;20 3）页号、页内偏移量 ​ 该进程逻辑大小200B，页面大小为50B，逻辑地址110B&#x2F;50B&#x3D;2，该逻辑地址在2号页表中，页内偏移量&#x3D;110%50&#x3D;10，说明该地址在从该页起始地址偏移了10B。 一个页面大小为4KB，那么一页的数据量就是2^12B 那么一个逻辑地址前 20位 表示页号，后12位表示这个逻辑地址相较于该页号对应的地址块首地址偏移了多少B 通过该逻辑地址可以推出 页号 和 偏移量 ，查询页表的页号可以找到块号，通过块号可以得到该块起始地址，起始地址+偏移量&#x3D;内存真实地址 4）页表寄存器 5）逻辑地址-&gt;物理地址过程 ​ 首先逻辑地址可以分割出 页号P 和 业内偏移量W，通过页号P 和 页表寄存器的页表长度M（存了共有多少页表项）对比看地址是否越界，越界则会发生内中断。 没越界的话，就会通过页号找到对应的内存块号b，内存块号b*页框大小 + 页内偏移量W &#x3D; 物理地址E eg： 6）页表项储存 页表项是存储在内存中的，一个页面大小&#x3D;页框大小&#x3D;2^12B&#x3D;4096B，页表中一个 页表项大小&#x3D;块号大小&#x3D;3B， 这样内存中一个页框存储页表项的时候会存4096&#x2F;3&#x3D;1365个，最后一个的index是1364，该页框还剩1B内存， 但是下一个index为1365页表项则需从下一个页框中存，则其内存地址为 X+3*1365+1&#x3D;X+4096 所以为了方便，会调整页表项大小（块号大小）使其能被一个页框大小整除，这比如说扩大到4，则一个页框刚好可存2^12&#x2F;2^2&#x3D;2^10个 7）快表页表（慢表）是存储在内存中的，快表（页表的副本）是存储在cache中的： 首先会查快表再查慢表；或者快慢表一起查 8）两级页表单级页表存在两个问题： 问题1：页表很大的时候需要连续占用多个页框 问题2：进程在运行的时候可能并不需要页表所有内容，只需要访问几个特定页面 问题1解决： 为此我们为 页表 设计一个 页目录表 来实现页表在页框的分散存储 ​ 页表大小为4KB&#x3D;2^12B，所以32位的后12位存业内偏移量，32位的前12位存 页号，则每个页框能存4K&#x2F;4B&#x3D;1K&#x3D;2^10个页表项， 存完所有页表项需要2^10个页框 若可以我们可以将整张页表分为1024个分组，每个分组有1024个页表项，最后制定一个 页目录表 记录叶分组和块号的对应关系 ​ 这样页表就可以分散存储了，这样的话本来需要1024个连续的页框（每个空间有1024个页表项），只需要多申请一个页框，就可以这张页面储存这1024个页框的位置，共1025个页框 问题2解决： 一级页表设置一个属性，说明该页面是否在内存中，如果不在就发生内中断从外存中调入 6.基本内存分段管理（离散）1）分段存储 逻辑地址&#x3D;段号+段内偏移量 过程： 段表寄存器中段表长度M（存了段表项的总数量），若段号&gt;&#x3D;段表长度则越界了 2）分段和分页的对比分段的每一段都是一个共同的逻辑，这样每段的功能相对独立，这样便于用户的管理 分段资源利于共享，因为某段的功能相对独立，而分页的话可能某页框中的一段能访问另一端不能访问 7.段页式管理 段页式实际上就是先进行分段，然后再将每一段进行分页 ​ 首先查段表，根据段号查到对应也变存放的块号，找到页表后，根据页号找到对应 页面数据 存放的块号， 块号*页框大小+页内偏移量&#x3D;物理地址 8.虚拟内存1）局部性原理 时间局部性：while指令被执行了可能还会继续执行，a变量在while指令中，所以短时间会多次访问a地址 空间局限性：指令执行后其前后的指令也有可能被执行，因为内存中大部分数据都是连续存放的 2）虚拟内存特点 实现虚拟内存的两个方向： 作业运行不需要一次性装入，可以将先要用到的先装入，然后多次调入内存 作业运行的时候不需要一直常驻内存，短时间不需要的先调入外存，用到的时候再换进来 虚拟内存在逻辑上扩充了内存的大小 3）实现 传统的非连续分配是将进程所有数据调入，虚拟内存的请求方式会灵活调出部分数据到外存，并在需要时从外存调入 9.请求分页管理相较于基本分页管理，请求分页最大的不同就是在快表没命中，慢表没有的时候，将相应页面调入内存，同时页表项修改对应数据 10.页面置换算法1）最佳置换算法 注意： 因为该算法需要知道后序将要访问到的页，所以无法实现 发生了缺页中断并不一定会发生页面置换，只有当内存满了以后才会发生调换，当页面有空闲的时候，缺页会直接调入空闲区域 2）先进先出 注意： 效率低 发生Belady异常：分配的物理块变多，效率反而越低 3）最近最久未使用 注意： 效率最接近最佳置换算法，但是需要硬件成本 4）时钟置换算法 为每个页表项添加一个访问位，当某页被访问置1，两轮扫描： 第一轮：扫描寻找访问位为0，并把1置0 第二轮：扫描寻找访问位为0（必有） 5）改进的时钟置换算法 每一个页表项设置&lt;访问位，修改位&gt;，1代表被访问或修改了，四轮扫描： 第一轮：寻找&lt;0,0&gt; 第二轮：寻找&lt;0,1&gt;，并把&lt;x,y&gt;中的x置0 第三轮：寻找&lt;0,0&gt; 第四轮：寻找&lt;0,1&gt;（必有） 11.页面(框)分配策略由于请求分页管理不是将进程所有页面一次性装入，而是装入部分后再根据实际情况调入，所以我们需要为其分配一定页框 1）分配策略、置换策略 驻留集：系统为进程分配的页框集合 固定分配：开始分配一定数量后大小不变 可变分配：开始分配一定数量，根据情况改变，驻留集大小可能改变 局部置换：置换只能是自己进程的页面 全局置换：除了自己进程，也可以置换其它进程 2）何时、何处调入页面 3）抖动和颠簸 驻留集分配不够，进程页面频繁调入调出 可以根据实际工作集大小调整驻留集","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://example.com/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"http://example.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}]},{"title":"操作系统笔记-IO管理","slug":"5.操作系统-IO设备","date":"2022-03-06T11:17:48.000Z","updated":"2022-06-23T10:30:14.039Z","comments":true,"path":"2022/03/06/5.操作系统-IO设备/","link":"","permalink":"http://example.com/2022/03/06/5.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-IO%E8%AE%BE%E5%A4%87/","excerpt":"操作系统+ . +","text":"操作系统+ . + 1.I&#x2F;O控制器1）I&#x2F;O控制器各部分 ①CPU通过控制线发出相关命令，通过地址线来指明需要操作的IO设备， ②数据寄存器： ​ 当CPU可以在此处放入或接收一些数据的时候，IO逻辑可从这里取得数据取得或放入数据（如CPU需要IO输出一些数据时） ③控制寄存器： ​ CPU发出的IO指令会有一些参数，这些参数会放在这里，IO逻辑可从此处读入 ④状态寄存器： ​ IO设备可以将各个IO设备的状态放在此处，CPU可读入 ⑤数据：IO逻辑可以将要输入的数据在放在这里让IO设备输出，也可以从此处获得IO输入的数据 ​ 状态：IO逻辑可以从此处获得该IO设备的状态 ​ 控制：IO根据CPU发出的指令和对应参数，在这里指明IO设备执行对应操作 注意： 一个IO控制器可能对应多个IO设备 数据寄存器、控制寄存器、状态寄存器有多个 2）寄存器的编址 ①寄存器与内存同一编址： ​ 简化了指令，控制寄存器可以直接用内存相关指令 ②寄存器独立编址： 需要设置专门的指令来控制寄存器 2.IO控制方式 1）程序直接控制 ​ CPU在发出读指令后，设置状态寄存器为1（代表IO设备还未就绪），然后while访问状态寄存器直到其值为0（IO设备已经就绪）。IO设备在准备好后，会将数据和自身状态传给IO控制器，IO逻辑修改响应寄存器，CPU收到后再进行处理。 CPU需要等待IO设备的准备，并且每一次procedure只能读一个字的数据，效率太低 2）中断驱动方式 ​ 程序直接控制CPU在循环检查等待IO就绪会浪费大量时间，中断驱动会在CPU发出指令后将IO进程阻塞执行其它进程，当IO准备好并发送相关信息给IO控制器后，IO控制器会发出一个中断信号来提醒CPU，CPU会根据该中断信号进行处理。 该方式虽然能够让CPU和IO设备并行工作，但是中断处理恢复现场会浪费时间，并且每一次procedure还是只能读一个字的数据 3）DMA方式 ​ DMA也是一种IO控制器，当CPU发出指令（读入多少数据，数据存在内存什么位置，数据存在外存什么位置），DMA会完成这一些操作，在全部处理完以后才会发出中断信号让CPU来处理。 ​ 读写的块必须是连续的，同时读入内存后这些块也是连续的 ​ 注意：DMA一次完整过程的数据传输是以块为单位的，但是内部在读入的时候还是以一个字为单位 DMA结构： 4）通道控制方式 “通道”可以理解为一种更低级的CPU，只能执行一系列通道指令，并且通道可以对应多个IO控制器 CPU会将通道程序（一系列任务、任务清单），通道读入该通道程序后执行该任务清单，在执行完任务户在发出中断让CPU来处理 DMA和通道的区别： 1.DMA对数据的读取还是由CPU下达命令的，而通道执行的是CPU给出的通道程序 2.DMA可读写单块或连续的多块数据，通道能读写一组数据 3.IO软件层次结构 4.IO核心子系统实现功能 1）IO调度 IO作为一种资源，其调度算法等和进程、磁盘的调度类似 2）设备保护 ​ 设备可以看作是一种文件，其保护机制类似于文件保护的机制，当用户要访问某个IO设备的时候，系统会根据其FCB中记录的信息来判断该用户是否有相应的访问权限。 3）假脱机技术（SPOOLing技术） ①脱机技术和SPOOLing技术的对比 脱机技术： ​ CPU读入读出数据都是通过磁带这种高速的媒介，而用户真正提供或获得的输入输出还是依靠纸带机，但这时候输入输出已经脱离了CPU的控制，CPU在这时可以处理其它更多的请求。 假脱机技术： ​ 输入设备（纸带机）输入的数据会先慢速写入输入缓冲区，输入进程（外围控制机）将输入缓冲区数据写入输入井（输入磁带），输入井中的数据会被高速读入CPU ​ CPU会将输出的数据高速写入到输出井（输出磁带），输出井会通过输出进程（外围控制机）将数据写入输出缓冲区，输出设备（纸带机）会从输出缓冲区慢速读入数据 ②共享打印机例子 ​ 进程在打印数据时会先将数据写入输出井（输出磁带）中，这样在主机层面该进程就好像完成了打印任务，实际上打印的顺序还是按照输出进程中的假脱机文件队列，但这些任务已经不受CPU控制了，这样看起来就是多个文件共享了打印机。 4）设备的分配与回收 ①考虑因数​ 固有属性：要考虑该设备是独占（打印机）还是共享设备（磁盘） ​ 分配算法：资源分配的算法和进程的调度算法类似 ​ 安全性： ​ 安全分配：IO的时候进行该进程进行阻塞 ​ 不安全分配：IO的时候进程继续运行，并且还可以请求其它IO ​ ​ ②静态分配和动态分配 ​ 静态分配：一次性分配完所有资源，否则不分配；运行结束后归还所有资源 ​ 动态分配：在运行过程中需要了再申请，可能发生死锁，可用银行家算法避免 ③设备分配管理的数据结构 一个通道控制表对应多个IO控制器表，一个IO控制器表（HP打印机）对应多个设备控制表（多台相同的打印机） 每张表都有一个等待队列指针，缺少对应资源的进程会将PCB挂在该队列上 ④设备分配的步骤 若让用户使用“物理设备名”扩展性较低，应使用逻辑设备名，每个用户可享有独立的 逻辑设备名和物理设备名映射表 5）缓冲区管理 注意缓冲区最重要的特性： ​ 缓冲区满的时候，才能读取缓冲区的数据 ​ 缓冲区空的时候，才能往缓冲区写数据 ①单缓冲 ②双缓冲 ​ ​ 单缓冲和双缓冲在通信时的区别： ​ 单缓冲： ​ 双缓冲： ​ 管道实际上就是一种缓冲区，要想实现双向通信，必须实施两个管道 ③循环缓冲 ④缓冲池","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://example.com/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"http://example.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}]},{"title":"操作系统笔记-进程与锁","slug":"2.操作系统-进程和锁","date":"2022-03-06T11:17:48.000Z","updated":"2022-09-12T15:03:58.009Z","comments":true,"path":"2022/03/06/2.操作系统-进程和锁/","link":"","permalink":"http://example.com/2022/03/06/2.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-%E8%BF%9B%E7%A8%8B%E5%92%8C%E9%94%81/","excerpt":"进程通信、进程的调度、进程的同步和互斥、信号量、死锁相关问题","text":"进程通信、进程的调度、进程的同步和互斥、信号量、死锁相关问题 1.进程的概念、组成、特征 进程是动态的，是一次程序运行的过程，同一程序多次启动会对应多个进程。 2.进程的状态及进程状态的转换 进程首先会进入创建态（完成新建PCB等准备工作），然后进入就绪态等待CPU的调度，当CPU运行该进程后进程进入运行态。当运行时出现异常，如整数&#x2F;0就会终止掉该进程，进程进入终止态。若进程在运行过程中需要请求资源但资源还未空闲，如IO设备，那么进程就会进入阻塞态等待资源空闲。当资源空闲，进程有了需要的资源，那么进程将进入就绪态等待CPU调动，CPU在运行该进程后又会重新进入运行态。同时运行态也有可能因为时钟中断进入就绪态。 对于同一状态的进程，操作系统会将它们的PCB组织起来：队列和索引两种方式 3.进程控制1）原语 ​ 原语是程序段，实现了原子性。主要由两条特权指令实现：关中断指令执行后，中断信号将不会影响该任务执行，等到开中断指令执行后，改程序段才能被中断。 ​ 关中断指令和开中断指令必须是特权指令，若是非特权指令，那某一程序就能够一直霸占CPU了。 2）进程控制原语 注意： ​ PCB中存储了上次进程执行的相关信息，在被唤醒后，进程可以根据这些信息还原上次的运行状态。 4. 进程通信1）共享内存 ​ 共享内存就是开辟一片空间供通信进程使用： ​ 基于数据结构—只能根据数据结构进行存储，较低级； ​ 基于存储区—划出一片共享存储区，存储的形式和位置等都由通信进程控制。 2）消息传递 直接通信： ​ 发送进程P直接将msg发送到接收进程Q，并将msg挂到接收进程Q的消息队列中，接收进程Q通过接收原语取得消息 间接通信： ​ 可以有多个发送方向信箱A发送信息，也可以有多个接收方从信箱A接收信息 3）管道通信 管道对于数据的接收相当于于一个单向队列，FIFO。 管道实际上就是一个缓冲区，只有写满数据才能取，只有取完数据管道空了才能写。 要想实现双向通信，必须要要两个管道。 5.线程的实现方式 1）用户级线程 如while（true）循环判断一个值就是一个傻瓜用户级线程，若其中一个线程堵塞了，整个进程都会被堵塞 2）内核级线程 内核级线程的管理由操作系统内核完成，一个用户进程会占用多个内核级线程，线程切换需要系统内完成，切换内核态是有成本的 3）多线程一对一： ​ 其实就是内核级线程 多对一： 一个内核级线程管理多个用户级线程，用户级线程切换不需要经过内核态，但是一个用户级线程阻塞后，其他也会被阻塞 多对多： 一个进程包含多1，2，3个用户级线程，1可由内核级线程1执行，2，3可由内核级线程2并发执行。即一个进程可能对应多个内核级线程。当一个进程对应的所有内核级线程都被阻塞后，才称该进程被阻塞。 6.调度1）高级调度 高级调度是面向队列的，启动一个程序并创建相关进程。外存的作业后备队列-&gt;调入内存，并创建响应PCB，当调出的时候才会撤销。 2）中级调度 ​ 当内存空间不足的时候，会将某些数据调出外存，这些进程的PCB会被组织成挂起队列，中级调度会根据某种策略决定将某个处于挂起状态的进程重新调入内存。 3）低级调度 按照某种策略，从就绪队列中挑选一个进程并将处理机分配给它。 4）七状态模型 挂起的进程是在外存的挂起队列中，阻塞的进程是在内存的阻塞队列中的。 ​ 当内存空间不足，会将某些进程从就绪队列调入外存的就绪挂起队列中，默写阻塞态进程夜游可能会被调入阻塞挂起队列，当需要的事件出现以后该进程就有可能从阻塞挂起变为就绪挂起。运行态的进程也有可能进入就绪挂起。 5）三种调度对比 7.进程调度的时机 切换和过程方式1）切换时机 中断处理不能调度 操作系统内核程序临界区： ​ 临界资源是互斥访问资源，一个进程访问就会上锁，临界区就是这段资源的代码 ​ 如就绪队列就是一种内核程序临界区，访问的时候会上锁，但需要快速释放，如果进行进程调度会浪费时间 ​ 而打印机设备就是普通临界区，访问的时候也会上锁，但是使用这些资源的时候，CPU处于空闲状态，可以进行调度 原语：原子性不能被中断 2）切换过程 从就绪队列中选择一个进程，然后切换该进程： 保存原来运行进程的各种数据：保存现场 恢复选择的进程的现场：通过保存的数据恢复 8.批处理调度算法1）FCFS 先来先服务 https://www.scimall.org.cn/article/detail?id=4813361 2）SJF 短作业优先非抢占式： 抢占式： 注意： 非抢占式中每个进程一旦开始运行会运行到结束，期间如果有新的进程就绪也不会被剥夺处理机。而抢占式则是每当有一个新的进程进入等待队列就会动态计算剩余时间，会产生抢占现象 3）HRRN 高响应比优先 4）对比 9.交互式处理调度算法1）时间片轮转 时间片太大： ​ 会退化成先来先服务FCFS 时间片太小： ​ 导致频繁的进程切换，浪费系统资源 2）优先级调度算法抢占式和非抢占式的区别：前者在执行完一个进程后做判断（主动失去），后者在有新的进程进入就绪队列后就会做判断（剥夺） 抢占式： 非抢占式： 3）多级反馈队列 饥饿情况：持续有高优先级的短进程进入第1级队列，那么低级队列的进程就可能会饥饿。 4）对比 10.进程的同步和互斥1）进程同步​ 制约进程的执行顺序，无论整体顺序如何，吃饭一定要在洗澡的前面 2）进程互斥 临界资源： 进入区：给该资源上锁，保证其它进程无法访问到该资源 临界区：访问临界资源的代码 退出区：解锁 11.进程互斥的软件实现1）单标志法 ​ 缺点：但是若此时turn&#x3D;0，允许进程P0访问，但P0一直不访问，那么P1就无法访问，违反“空闲让进” 2）双标志先检查法 ​ 先检查他人意愿，再表达自己意愿flag ​ 缺点：违反忙则等待，两个进程都可能会进入临界区，因为检查 和 表达意愿 操作不具原子性，违反忙则等待 3）双标志后检查法 ​ 先表达自己意愿，再检查别人意愿 ​ 缺点：可能会导致双方都有意愿，双方都堵塞住进不了，违反空闲让进和有限等待 4）Peterson算法（结合了1 3） 先表达意愿，再谦让，然后判断对方是否有意愿且自己已经谦让了：该算法后谦让的会丧失执行权（改变了之前turn的值） 12.进程互斥的硬件实现 中断屏蔽： ​ 其实就是加入了开中断和关中断（类似原语）：若有多个CPU，那么中断屏蔽只能在一个CPU1上实现互斥，若在CPU2上有对该临界资源的访问，CPU1的约束无法生效；且关中断和开中断属于特权指令，只能运行在内核态，不能让普通程序使用，所以中断屏蔽的方法只适合操作系统的内核进程 TestAndSet和Swap指令： 实际逻辑都是检查是否上锁，若没上锁，自己再上锁：类似双标志先检查法，但硬件实现具有原子性，不用考虑并发 13.信号量机制信号量代表了某一资源的值，如int src代表了打印机数量，那么int src&#x3D;2，说明打印机有两台。 1）整型信号量 ​ ​ P、V操作使用了原语，保证了原子性，但仍然不满足让权等待，会发生忙等问题。 2）记录型信号量 忙等： ​ 当某进程访问资源，但是资源不够的时候，会进入while死循环，这时候无法访问资源但是仍然在占用CPU，所以会浪费CPU资源， 进入忙等状态。 为什么记录型信号量不会发生盲等？ ​ 定义的信号量数据结构中带有等待队列的指针，可以操作相应进程挂在等待队列上等待唤醒 ​ 因为在P（wait）操作的时，当信号量&lt;0，即资源数量不够后，该进程会主动进入阻塞态，让出CPU资源。 ​ 而当V（signal）操作的时候，判断信号量++后是否&lt;&#x3D;0，如果&lt;&#x3D;0，说明仍然有进程在等待资源，那么V操作会唤醒该进程。 14.信号量实现进程互斥和同步 前驱关系1）实现进程互斥 ​ 初始化互斥信号量&#x3D;1（说明该资源只有一个，只允许一个进程使用完后再还回去），进程A在进入区前P操作（申请一个资源并让资源-1，那么唯一的一个资源已经被占用）；进程B要想P申请一个，发现mutex&lt;0，资源没了，进入阻塞状态；进程A用完该资源后会通过V操作mutex++（还回该资源），并唤醒进程B。 2）实现进程同步 ​ 保证12 4相对顺序 初始化mutex&#x3D;0（相当于没有该资源），代码1和代码2的结果相当与生成了一个该资源，那么V(S)使mutex++，代表有了一个该资源，那么P(S)就可以获得该资源并执行接下来的代码。 ​ 若向先执行4，那么P(S)操作没有获得相应的资源，所以会执行block原语进入阻塞态无法执行，只有当12执行产生了对应资源S后才能继续执行，这样就保证了同步的相对 15.各类问题1）生产者和消费者问题 为什么要互斥：因为虽然容量为5，但是多个生产者进程可能会并发修改同一个地址的数据，这样就会导致数据覆盖。 full-&gt;目前产品数量 ，一开始数量要为0，生产者v生产并放入缓冲区+1，消费者才能从缓冲区p消耗取出一个。 empty-&gt;容器剩余容量，一开始数量要为5，生产者p消耗一个容量生产，消费者才能v获得一个产品。 ​ 生产者：先消耗一个容量P(empty)，生产一个产品并放入缓冲区，增加一个目前产品数量V(full) ​ 消费者：消费者先消耗一个目前产品P(full)，从缓冲区拿出一个产平，增加一个缓冲区容量，使用产品 ​ 实现互斥的P操作一定要在实现同步的P操作之后！！！ 2）多类生产者和多类消费者问题 本体可以不用互斥信号量，因为容量总共为1，若为2就不行 3）吸烟者问题 finish：这里也可以理解为桌子上剩下的容量，当吸烟者拿走组合后并吸烟后，容量就会+1，这时候生产者就能生成组合并放桌上 4）读者写者问题 写进程1和其它所有写进程和读进程都互斥，写进程之前需要所有进程的工作都结束 rw：保证读进程和写进程之间的互斥，count：记录有多少个读进程 ②：第一个进程获得文件资源后上rw锁，并使count+1，最后一个读进程读完后count–，并释放rw锁，唤醒写进程 ①：当两个读进程同时执行，这时候两个count都&#x3D;&#x3D;0，而此时读进程A P(rw)上锁，那么读进程2就会被rw锁阻挡，这是不对的 ​ 导致这个问题的原因是这段代码不是一气呵成的，所以可以加一个mutex锁，保证各个读进程在访问该段代码的时候是互斥的 注：以上实现有一个缺点，如果有无限读进程进入，那么写进程就会因为得不到rw锁而被饿死 再增加一个w变量，该变量来保证写进程不会被饿死 ​ 当读进程1 P(w)上锁，此时后来读进程和写进程都会因为w锁没被释放而被阻塞，按照先后顺序排列在一个阻塞队列中 （读进程2-&gt;写进程1），当读进程1在读文件前V(w)释放锁，这时候队首的写进程1就会获得该锁并会堵在rw锁…… w锁保证了 读进程和写进程是按照来的顺序 来获得操作文件的机会 rw：保证读进程和所有写进程之间的互斥 w：保证读进程和写进程按照来的顺序获得机会，不会因为大量读进程而被阻塞 mutex：保证rw上锁的过程一气呵成，不会导致多个读进程的堵塞，保证对count变量操作的互斥 5）哲学家进餐问题 三种解决方案： 16.管程 管程实际上就相当于一个类，成员变量就是需要加锁的对象，一组过程就是定义的函数 每次仅允许一个进程在管程内执行某个函数，编译器负责各进程互斥进入管程 17.死锁相关1）死锁、饥饿、死循环的区别 死锁：死锁是多个进程循环等待对方手里的资源而发生的，并且死锁的进程一定处于阻塞态，等待对方释放资源将自己唤醒 饥饿：可能只有一个进程发生饥饿，如短进程优先算法，如果一直有短进程进入，那么长进程就会发生饥饿 ​ 发生饥饿的进程可能是阻塞态，长期得不到I&#x2F;O设备 ​ 可能是就绪态，如长期得不到处理机 死循环：程序员编码导致 2）死锁发生的4个条件 ​ 互斥条件：对互斥资源的争抢才会导致死锁 ​ 不剥夺条件：各进程持有的资源不能被剥夺，只能主动释放 ​ 请求和保持条件：进程在吃持有别的进程想要的资源时，又在请求其他资源 ​ 循环等待条件：存在一种进程资源的循环等待链 注意： 发生了循环等待不一定发生死锁，如有第6个哲学家持有3号能用的筷子，虽然3在循环等待，但是6若释放则不会死锁：当有其它可替代资源的时候就未必发生死锁 3）死锁的处理策略 18.预防避免死锁1）破坏发生死锁的4个条件 2）银行家算法 安全序列：一个能满足所有进程资源需求的分配序列 不安全状态：当分配了某一些资源后，如果各进程不主动归还一些资源，那么系统有可能不能再满足接下来的资源分配而导致死锁 银行家算法： 银行家算法其实就是在分配资源的时候进行检查，看手上的资源能否满足剩下进程的最大需求，通过回收进程能够得到一条安全序列 19.死锁的检测和解除1）死锁的检测 实际上就是找到了一条安全序列 死锁检测算法：依次消除不阻塞进程相连的边，直到无边可消 2）死锁的解除 剥夺某进程资源（将进程挂起），将资源给其它进程 强制终止某些死锁进程，这样对运行快结束的进程可能代价很高 回退到某一步，可以避免死锁","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://example.com/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"http://example.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}]},{"title":"操作系统笔记-文件和磁盘","slug":"4.操作系统-文件和磁盘","date":"2022-03-06T11:17:48.000Z","updated":"2022-06-23T10:30:08.666Z","comments":true,"path":"2022/03/06/4.操作系统-文件和磁盘/","link":"","permalink":"http://example.com/2022/03/06/4.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-%E6%96%87%E4%BB%B6%E5%92%8C%E7%A3%81%E7%9B%98/","excerpt":"操作系统+ . +","text":"操作系统+ . + 1.文件逻辑结构 1）无结构文件 无结构不文件中的数据就是一系列 二进制bit流 和 字符流，txt文件就是一系列 字符流 2）有结构文件 有结构文件中存储的是一条条数据，每条数据的各列可能是定长的，也有可能是不定长的 ①顺序文件 可变长记录： ​ 顺序存储无法随机存取，因为不像数组每一个[]的大小都是相等的 定长记录： ​ 串结构——对每一条记录都随机存储 ​ 顺序结构——按关键字存记录，添加删除都需要再重新调整 ②索引文件 由于不定长文件不能直接查找第i个记录，可以建立一张索引表来记录每条记录的地址 ③索引顺序文件 索引顺序文件是为了解决索引文件可能过大的问题： ​ 索引文件的每条记录记录的不再是单个记录地址，而是顺序的一组数据，如存储逻辑文件的前50个数据 多级顺序文件 我们可以建立多级索引进一步提高效率，类似于mysql中的B+树 2.文件目录（文件的逻辑组织方式） 1）FCB文件数据块 一个文件目录就是各文件FCB的集合，一个文件对应了一个目录项，也就是一个FCB，可以说一个目录中存储的就是FCB的集合 2）单级目录 整个操作系统只有一张目录表，存了所有FCB，所以文件不能重名 3）两级目录 两级目录中的主文件目录记录了各个用户文件目录，不同用户目录的文件可以重名 4）多级（树形）目录 多及目录中不同目录文件名可以重名 5）无环图目录 实际上就是允许了不同目录下的不同文件可以指向同一个地址，方便了文件的共享， 共享计数器记录了文件被几个用户共享了，只有当共享计数器&#x3D;0，每个用户都删除不需要后才会完全删除该文件 6）索引结点（FCB改进） FCB只存储文件名和指向其它结点的指针，其它所有信息都存储在索引结点中 3.文件保护 1）口令保护 相当于设置了一个密码，密码可能在FCB或索引结点中，打开时输入正确指令才能访问 2）加密保护 加密保护就是对原始数据按照一定方式加密转换成，要访问该文件的时候需要使用正确的方式解密，才能获得正确的文件数据。 3）访问控制 针对每个文件建立访问控制表，规定了各个用户的具体权限，也可以分组管理 4.文件共享1）基于索引结点的共享（硬链接） ​ 两个文件的索引结点指针指向同一个索引结点，当user1要删除该文件的时候，索引结点的count–，说明user1不再需要，但是user2还需要。只有当count&#x3D;0，没有人需要的时候才会真正删除该文件 2）基于符号链的共享（软连接） 对应的文件是link文件，link文件指向了如aaa文件，通过aaa来访问文件1，删除link文件不会导致 索引结点1的count– 其实就是windows中的快捷方式 5.文件的物理结构（非空闲管理） 该节主要是针对已为文件分配了磁盘块后，这些磁盘块的管理（组织方式），属于非空闲块（已分配块）的管理。 1）文件块（磁盘块） 类似于内存，磁盘也被划分成了一块一块的（一页一页） 2）连续分配 由于是连续分配，方便查找，但是不方便增删等拓展，顺序访问速度最快，但容易产生碎片 3）链接分配①隐式链接分配 FCB记录了起始块号和结束块号，每个磁盘块有指向下一个磁盘块的指针，拓展简单，不会产生碎片，查找的效率低 ②显式链接分配 不同隐式链接分配，显式链接中各个磁盘块没有指向下一个磁盘块的指针，所有的下一块信息存储在一张文件分配表中FAT， 每一个磁盘都有一张FAT表，开机时会读入内存并常驻内存 4）索引分配 逻辑块就是页面，磁盘块就是页框 索引表就类似于内存分配中的页表，一个进程对应一张或多张页表，一个文件对应一张或多张页表 ①链接索引 ​ 当一个文件的索引表较大，一个磁盘块存不了的时候，就可以存在多个磁盘块上，各个磁盘块用指指针相连，但是这样若需要访问最后一张索引表上记录的地址，那么就需要遍历其之前所有的索引表，效率较低 ②多层索引 ​ 多层索引方案就是分页存储中的多级页表方案，通过一级索引表来记录各个二级页表的地址，在通过二级索引表来查询逻辑地址对应的磁盘块地址 ​ 但这样有一个缺点，一个文件总共只有1KB，采用这种方式会查询三次 ③混合索引 顶级索引就是FCB对应的索引节点，如某一大文件A含几个小文件，A-1小文件只有8个磁盘块，只需要通过直接地址，2次查询就够了（访存2次），某个大文件A-3需要256*256个磁盘块，那么就可以建立二级间接索引，查询3次（访存4次）。 6.逻辑结构和物理结构回顾 ​ 用户存储的数组Stu[]，我们可以通过Stu[1]、Stu[2]这种方式来访问1、2号学生，在我们看来他们的地址是连续的，实际上他们的逻辑地址是连续的。在文件中他们的逻辑地址是数组连续存储的，但是我们需要将将这些文件分块存储在不同的磁盘块，把它们分配在不同的磁盘块，这是操作系统来解决的，这样他们的物理地址是不连续的，但对我们来说，我们可以通过其连续的逻辑地址来访问，而逻辑地址转化成物理地址的过程我们并不关心，这是操作系统需要解决的。 1——逻辑地址 5——物理地址 7.存储空间管理（空闲管理）文件的分配是指对一个文件，我们采用什么样的方式将这个文件的数据组织起来 文件的管理是指对磁盘中的空闲磁盘块，我们应该采用怎样的方式将其组织起来，方便管理 1）文件卷、目录区、文件区 文件卷：C、D、E盘就是一个个文件卷（逻辑卷） 目录区：每一个文件卷中包含了目录区，主要存储了FCB及超级块等用于磁盘存储空间管理的信息 文件区：存储数据的区域 2）空闲表法 空闲表法很类似内存管理中 连续分配管理方式中 的 动态分区分配 类似 空闲表记录了第一个空闲块的块号，并且记录了从该块开始的空闲块数量，如0、1号空闲，记录就是0 2：块号从0开始，有两块空闲 对于磁盘的回收主和动态分区分配类似： 3）空闲链表法①空闲盘块链 ​ 该方式就是将所有空闲的磁盘块连接成一个链表，操作系统记录了链头、链尾指针。 ​ 若某文件申请k个磁盘块，只需要卸掉k个node就行，并且修改操作系统所记录的联投、链尾指针。 ​ 回收的时候只需要将回收的磁盘块接在链尾即可，并且修改链尾指针。 ②空闲盘区链 空闲盘区链和空闲盘块链最大的区别就是，node不再是一个磁盘块而可能是好几块磁盘块组成的一个磁盘区。 同样，操作系统也记录了链头、链尾。 这个和空闲表法很相似，所以分配的方法也很相似。 回收的时候若周围有链表中存储的空闲区了，需要和该区合并在一起，如果没有则作为一个单独的空闲区挂到链尾 4）位示图法（常考） 位示图的每一个格子就代表了一个盘号，本例中1代表已分配，0代表空闲 每一行有0-15格子，16个空闲块0。第0行就是0-15地址块，第一行就是（1 * 16+0）- （1 *16+15）地址块 如31号地址块对应的位示图位置，31&#x2F;16&#x3D;1，31%16&#x3D;15，即第1行15号位置 如何分配：顺序扫描位示图，找到k个相邻或不相邻的0空闲块，算出盘块号并分配，填1 如何回收：计算出回收的字号、位号，在表中位置填0 5）成组链表法 文件卷（如C盘）的目录区记录了超级块，超级块在系统启动的时候需要读入内存并保证内存中的超级块与外存中的超级块数据一致 超级块实际上类似于一个链头，该链表的每个结点结构如下： ①代表了该组的第一个磁盘块，该磁盘块不用作数据存储，而是存储了下一组的磁盘块数量，并记录了下一组所有磁盘块的指针 ②出第一个磁盘块外的所有磁盘块都用作数据存储 eg：100个磁盘块，1个用于存储下一个组的信息，99个用于存储分配的信息 而超级块就是①，存储了第一个组的所有信息 如何分配： ​ 分配的时候若某一组的无剩余，那么需要将该组记录的下一组的信息，修改超级组的数据，因为分配是从前往后分配，而且每一组所挂磁盘块数是定好的 如何回收： 若每一组的限制是100块，当某一块挂的数据块不够99个的时候，直接挂在该组，若满了则作为一个分组并修改超级块的数据 8.文件的基本操作 1）创建文件 使用了“create系统调用” 需要的参数： ​ 申请的大小（盘块数量）、存放路径、文件名（有默认值） steps： ​ 1.在外存中分配所需空间（空闲管理） ​ 2.通过地址找到该目录的目录文件，根据信息创建对一个目录项FCB 2）删除文件 需要的参数： ​ 文件的存放路径、文件名 steps： ​ 1.通过文件的存放路径找到对应的目录文件，并通过文件名找到对应的目录项（FCB） ​ 2.通过FCB记录的索引结点找到对应的磁盘块，回收磁盘块 ​ 3.删除该FCB 3）打开文件 打开一个文件就是将目录文件中该文件的目录项FCB，复制到系统的的打开文件表中。 需要参数： ​ 文件的存放路径、文件名、对文件的操作类型(r、rw、rwx) steps： 1.通过文件存放路径找到对应目录文件，通过文件名找到对应FCB，检查该用户的权限 ​ 2.将目录项复制到内存的打开文件表中，并返回其在该表编号，用户打开该文件需要编号 打开文件表： 每个进程都有一张属于自己的打开文件表，系统中有一张打开文件总表，每个进程打开文件表会由指正指向，系统打开文件表的对应表项 ①读写指针记录了该进程对文件读写进行到的位置 ②访问权限限制了进程对文件的操作 ③每一个进程对应系统打开文件表的一个表项就会使打开计数器+1，只有当计数器为0才会删除该表项 ​ 打开计数器作用： ​ 如要删除一个文件，这时候该文件还在被打开，说明打开计数器!&#x3D;0，就会提醒该文件正在被使用，不能删除 4）关闭文件 某一个进程关闭该文件就会使系统的打开文件表中的打开计数器-1，当打开计数器的值&#x3D;0的时候删除该表项 5）读文件和写文件 ​ 各进程读写文件都可以从本进程的打开文件表中的该文件项的系统索引号找到其在系统打开文件表中的位置，并分别调用 read和write系统调用。 9.文件系统的层次结构 ①8.文件基本操作的知识：系统调用 ②2.文件目录的知识：根据提供的地址找到对应FCB ③3.文件保护的知识：验证访问权限 ④1.逻辑结构的知识：逻辑地址的处理 ⑤5.物理结构的知识：逻辑地址转化成物理地址 ⑥7.存储空间的知识：文件存储空间的分配和回收 ⑦：硬件设备的管理，磁盘的调度等 10.磁盘的基本知识1）磁盘、磁道、扇区 每个扇区的数据量是相等的，磁道就是一圈一圈半径不同的圆 2）读写数据 读写数据需要磁头定位到对应的磁道，磁盘spin的时候磁头滑过对应扇区就能完成读写操作 3）定位磁盘块 柱面号：对应多个盘面的半径相同的磁道 盘面号：某一盘面的编号 扇区号：找到对应盘面的某一扇区 4）一次磁盘读&#x2F;写操作所需时间 1.寻找时间： ①启动磁头臂的时间：物理因数决定 ​ ②移动磁头(寻道时间)：磁头臂移动到对应磁道的时间，可以设计算法改进 2.延迟时间 ​ 旋转磁盘到达对应扇区的时间 3.传输时间 ​ 对应磁盘块的数据读入写入的时间 11.磁盘调度算法（寻道时间） 该部分算法是用来解决寻道时间的 1）先来先服务（FCFS） ​ 按照请求到达的顺序进行移动，若磁道很分散，效率就会很低 2）最短寻找时间优先（SSTF） ​ 优先移动至与当前磁道最近的磁道（贪心，未必全局最优），若有不段近距离请求加入可能造成远距离磁道的”饥饿” 3）扫描算法（Scan） ​ 移动到最右侧才能向左侧移动，移动到最左侧才能向最右侧移动。到达最右侧慢慢想最左侧移动 4）Look调度算法 ​ 和扫描算法相似，最大的不同就是不用移动到最右侧才想左搜索，在移动到184后观察到最左边已经没有请求了，那么直接想左移动 5）循环扫描算法（C-SCAN） ​ 循环扫描算法和扫描算法最大的不同就是到达最右侧以后，不再是一步一步从右侧往左侧扫描，而是立马到达最左侧，中途不处理任何请求，从最左侧往右扫描 ​ 6）C-Look调度算法 ​ C-Lool算法对循环扫描算法的最大改进就是往右扫描的时候不需要到达最右边，到达184后观察到左边已经没有请求了，那么就直接到最左边的一个请求18，而不是最左边的0号磁道 12.延迟时间 若2、3、4逻辑上是连续区域，物理上也设计为连续区域的话是不合适的。 ​ 因为当磁头读取完一个扇区中磁盘块的内容以后，需要一段时间处理，而盘片在不断旋转，那么读取完2后，3就无法马上读取，而是要等待再spin一圈后才能读取 1）交替编号 交替编号可以解决等待磁头臂处理时间的问题，这里还有一个问题 ①为什么磁盘物理地址是 （柱面号，盘面号，扇区号）？（柱面号，盘面号，扇区号）：（000，00，000 ）~（000，01，111） ​ 这样两个相邻的地址，他们的扇区号虽然不同，但是他们的柱面号都是00，说明他们的都是00，01盘面的00号磁道，磁头臂不用移动 ​ 即这两个相邻地址的柱面号都是相同的，不用移动磁头臂（不消耗物理时间） （盘面号，柱面号，扇区号）：（000，00，000 ）~（000，01，111） ​ 这样两个地址都是在同一盘面上，但是柱面号不相同，所以需要移动磁头臂 2）错位命名 ​ 假设：两盘的对应扇区都是相对的0-0,1-1（vertically） ​ 当第一个盘面读取完第0盘0号磁道第1扇区数据后，刚好走到需要读的第1盘的0号磁道第2扇区开头，但是因为磁头臂需要一定时间处理数据，那么第1盘的0号磁道第2扇区就需要第二次spin到的时候才能读取，那么这种方式就是不对的，我们需要扇区编号在vertically 上错开 13.磁盘的管理 1）磁盘的初始化 step1： ​ 划分扇区，扇区可分为头、数据区域、尾，尾部存有FAT ​ FAT：每个磁盘独有的，FAT记录了该磁盘中磁盘块的 物理块号 和 下一块地址 2）引导块 ​ 计算机开始要执行一系列初始化工作（执行自举程序），ROM中存储的数据是不能改变的，开机先启动ROM中的自举装入程序，执行该程序后会找到磁盘启动块中存储的完整的自举程序（可以修改）完成初始化。 3）坏块的管理 坏块的处理： ①FAT上表明，对操作系统不透明（操作系统知道哪些是坏块） ​ ②保留一些备用扇区，这些扇区来替换坏块，对操作系统透明（操作系统并不知道哪些是坏块）","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://example.com/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"http://example.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}]}],"categories":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://example.com/categories/Spring-Cloud/"},{"name":"Redis","slug":"Redis","permalink":"http://example.com/categories/Redis/"},{"name":"JVM","slug":"JVM","permalink":"http://example.com/categories/JVM/"},{"name":"并发","slug":"并发","permalink":"http://example.com/categories/%E5%B9%B6%E5%8F%91/"},{"name":"mysql","slug":"mysql","permalink":"http://example.com/categories/mysql/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"http://example.com/categories/Spring-Boot/"},{"name":"MQ","slug":"MQ","permalink":"http://example.com/categories/MQ/"},{"name":"Spring MVC","slug":"Spring-MVC","permalink":"http://example.com/categories/Spring-MVC/"},{"name":"计算机网络","slug":"计算机网络","permalink":"http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"git","slug":"git","permalink":"http://example.com/categories/git/"},{"name":"SSM框架","slug":"SSM框架","permalink":"http://example.com/categories/SSM%E6%A1%86%E6%9E%B6/"},{"name":"操作系统","slug":"操作系统","permalink":"http://example.com/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://example.com/tags/Spring-Cloud/"},{"name":"redis","slug":"redis","permalink":"http://example.com/tags/redis/"},{"name":"JVM","slug":"JVM","permalink":"http://example.com/tags/JVM/"},{"name":"JUC并发","slug":"JUC并发","permalink":"http://example.com/tags/JUC%E5%B9%B6%E5%8F%91/"},{"name":"mysql","slug":"mysql","permalink":"http://example.com/tags/mysql/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"http://example.com/tags/Spring-Boot/"},{"name":"rabbitMQ","slug":"rabbitMQ","permalink":"http://example.com/tags/rabbitMQ/"},{"name":"Spring MVC","slug":"Spring-MVC","permalink":"http://example.com/tags/Spring-MVC/"},{"name":"计算机网络","slug":"计算机网络","permalink":"http://example.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"git","slug":"git","permalink":"http://example.com/tags/git/"},{"name":"Spring","slug":"Spring","permalink":"http://example.com/tags/Spring/"},{"name":"操作系统","slug":"操作系统","permalink":"http://example.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}]}